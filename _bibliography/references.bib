
@article{ablart2017,
  title = {The How and Why behind a Multisensory Art Display},
  author = {Ablart, Damien and Velasco, Carlos and Vi, Chi Thanh and Gatti, Elia and Obrist, Marianna},
  year = {2017},
  month = oct,
  journal = {interactions},
  volume = {24},
  number = {6},
  pages = {38--43},
  issn = {10725520},
  doi = {10.1145/3137091},
  language = {en},
  annotation = {ZSCC: 0000003},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Ablart et al., 2017 - The how and why behind a multisensory art display.pdf}
}

@inproceedings{ablart2019,
  title = {Using {{Ultrasonic Mid}}-Air {{Haptic Patterns}} in {{Multi}}-{{Modal User Experiences}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Haptic}}, {{Audio}} and {{Visual Environments}} and {{Games}} ({{HAVE}})},
  author = {Ablart, Damien and Frier, William and Limerick, Hannah and Georgiou, Orestis and Obrist, Marianna},
  year = {2019},
  month = oct,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Subang Jaya, Malaysia}},
  doi = {10.1109/HAVE.2019.8920969},
  abstract = {Ultrasonic mid-air tactile displays offer a unique combination of high spatial and temporal resolution and can stimulate a wide range of tactile frequencies. Leveraging those features, a new modulation technique producing spatially distributed tactile sensations has recently been introduced. This new approach, referred to as Spatiotemporal Modulation (STM), draws lines, curves and shapes on users' palm by moving a midair tactile point rapidly and repeatedly along the path. STM parameters and their impact on tactile perception are yet to be studied systematically. In this work, we first study how varying the draw frequency and the size of a simple shape affects the participants perception of texture and their emotional responses. In the second part of our study, we used the most salient tactile patterns of the first study to extend the results within a multimodal context. We found that tactile patterns' perception was consistent within both studies. We also found instances when the tactile patterns could alter the perception of the audio and visual stimuli. Finally, we discuss the benefits of our findings and conclude with implications for future work.},
  isbn = {978-1-72812-355-4},
  language = {en},
  annotation = {ZSCC: 0000002},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Ablart et al., 2019 - Using Ultrasonic Mid-air Haptic Patterns in Multi-Modal User Experiences.pdf}
}

@article{aceti2013,
  title = {Not {{Here Not There}}},
  author = {Aceti, Lanfranco},
  year = {2013},
  journal = {Leonardo},
  volume = {19},
  number = {2},
  annotation = {ZSCC: 0000010},
  file = {Human Computer Interaction\\Augmented Reality\\Aceti, 2013 - Not Here Not There.pdf}
}

@misc{aftershokz2020,
  title = {Aftershockz {{Aeropex}}},
  author = {Aftershokz},
  year = {2020},
  journal = {AfterShokz},
  urldate = {2020-05-25},
  abstract = {Listen to music through bone vibration with these contemporary Aftershokz Aeropex headphones. The open ear design and bone conduction technology lets you hear external sounds, such as traffic, during use, and the Enhanced Audio feature ensures deeper bass and less vibration. These Aftershokz Aeropex headphones are lightweight for comfortable all-day wearing and can be used during workouts and runs in the rain with an IP67 waterproof rating.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\7DNSJP4C\\aeropex.html},
  url = {https://aftershokz.co.uk/products/aeropex}
}

@inproceedings{altosaar2019,
  title = {Physically {{Colliding}} with {{Music}}: {{Full}}-Body {{Interactions}} with an {{Audio}}-Only {{Virtual Reality Interface}}},
  shorttitle = {Physically {{Colliding}} with {{Music}}},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Tangible}}, {{Embedded}}, and {{Embodied Interaction}}  - {{TEI}} '19},
  author = {Altosaar, Raul and Tindale, Adam and Doyle, Judith},
  year = {2019},
  pages = {553--557},
  publisher = {{ACM Press}},
  address = {{Tempe, Arizona, USA}},
  doi = {10.1145/3294109.3301256},
  abstract = {A Very Real Looper (AVRL) is an audio-only virtual reality (VR) interface inside of which a performer triggers and controls music through full-body movement. Contrary to how musical interfaces in VR are normally used, a performer using AVRL is not disconnected from their surrounding environment through immersion, nor is their body restrained by a head-mounted display. Rather, AVRL utilizes two VR sensors and the Unity game engine to map virtual musical sounds onto physical objects in the real world. These objects help the performer locate the sounds. Using two handheld VR controllers, these sounds can be triggered, looped, acoustically affected, or repositioned in space. AVRL thus combines the affordances of the physical world and a VR system with the reconfigurability of a game engine. This integration results in an expansive and augmented performance environment that facilitates full-body musical interactions.},
  isbn = {978-1-4503-6196-5},
  language = {en},
  annotation = {ZSCC: 0000004},
  file = {Human Computer Interaction\\Virtual Reality\\Altosaar et al., 2019 - Physically Colliding with Music.pdf}
}

@misc{apple2020,
  title = {{{ARKit}}},
  author = {Apple},
  year = {2020},
  journal = {ARKit},
  urldate = {2020-05-25},
  abstract = {Take advantage of the latest advances in ARKit to create incredible augmented reality experiences for Apple platforms.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\RN5UG5LG\\arkit.html},
  url = {https://developer.apple.com/augmented-reality/arkit/}
}

@misc{apple2020a,
  title = {App {{Clips}}},
  author = {{Apple}},
  year = {2020},
  journal = {Apple Developer},
  urldate = {2020-10-02},
  abstract = {App Clips are a great way for users to quickly access and experience what your app has to offer.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\37WZ3946\\app-clips.html},
  url = {https://developer.apple.com/app-clips/}
}

@phdthesis{armstrong2006,
  title = {An {{Enactive Approach}} to {{Digital Musical Instrument Design}}},
  author = {Armstrong, Newton},
  year = {2006},
  language = {en},
  school = {Princeton University},
  annotation = {ZSCC: 0000079},
  file = {Arts & Humanities\\Computational Art\\Armstrong, 2006 - An Enactive Approach to Digital Musical Instrument Design.pdf}
}

@article{azuma1997,
  title = {A {{Survey}} of {{Augmented Reality}}},
  author = {Azuma, Ronald T},
  year = {1997},
  journal = {Presence: Teleoperators and Virtual Environments},
  volume = {6},
  number = {4},
  pages = {355--385},
  abstract = {This paper surveys the field of Augmented Reality, in which 3-D virtual objects are integrated into a 3-D real environment in real time. It describes the medical, manufacturing, visualization, path planning, entertainment and military applications that have been explored. This paper describes the characteristics of Augmented Reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective Augmented Reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using Augmented Reality.},
  language = {en},
  annotation = {ZSCC: 0010558},
  file = {Human Computer Interaction\\Augmented Reality\\Azuma, 1997 - A Survey of Augmented Reality.pdf}
}

@article{azuma2001,
  title = {Recent Advances in Augmented Reality},
  author = {Azuma, R. and Baillot, Y. and Behringer, R. and Feiner, S. and Julier, S. and MacIntyre, B.},
  year = {Nov.-Dec./2001},
  journal = {IEEE Computer Graphics and Applications},
  volume = {21},
  number = {6},
  pages = {34--47},
  issn = {02721716},
  doi = {10.1109/38.963459},
  language = {en},
  annotation = {ZSCC: 0003748},
  file = {Human Computer Interaction\\Augmented Reality\\Azuma et al., 2001 - Recent advances in augmented reality.pdf}
}

@article{baldassi2018,
  title = {Challenges and {{New Directions}} in {{Augmented Reality}}, {{Computer Security}}, and {{Neuroscience}} -- {{Part}} 1: {{Risks}} to {{Sensation}} and {{Perception}}},
  shorttitle = {Challenges and {{New Directions}} in {{Augmented Reality}}, {{Computer Security}}, and {{Neuroscience}} -- {{Part}} 1},
  author = {Baldassi, Stefano and Kohno, Tadayoshi and Roesner, Franziska and Tian, Moqian},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.10557 [cs]},
  eprint = {1806.10557},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1806.10557},
  urldate = {2020-05-25},
  abstract = {Rapidly advancing AR technologies are in a unique position to directly mediate between the human brain and the physical world. Though this tight coupling presents tremendous opportunities for human augmentation, it also presents new risks due to potential adversaries, including AR applications or devices themselves, as well as bugs or accidents. In this paper, we begin exploring potential risks to the human brain from augmented reality. Our initial focus is on sensory and perceptual risks (e.g., accidentally or maliciously induced visual adaptations, motion-induced blindness, and photosensitive epilepsy), but similar risks may span both lower- and higher-level human brain functions, including cognition, memory, and decision-making. Though they have not yet manifested in practice in early-generation AR technologies, we believe that such risks are uniquely dangerous in AR due to the richness and depth with which it interacts with a user's experience of the physical world. We propose a framework, based in computer security threat modeling, to conceptually and experimentally evaluate such risks. The ultimate goal of our work is to aid AR technology developers, researchers, and neuroscientists to consider these issues before AR technologies are widely deployed and become targets for real adversaries. By considering and addressing these issues now, we can help ensure that future AR technologies can meet their full, positive potential.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {Human Computer Interaction\\Augmented Reality\\Baldassi et al., 2018 - Challenges and New Directions in Augmented Reality, Computer Security, and.pdf;..\\..\\Zotero\\storage\\773269IN\\1806.html}
}

@article{barde2016,
  ids = {bardeAttentionRedirectionUsing2016a},
  title = {Attention {{Redirection Using Binaurally Spatialised Cues Delivered Over}} a {{Bone Conduction Headset}}},
  author = {Barde, Amit and Ward, Matt and Helton, William S. and Billinghurst, Mark and Lee, Gun},
  year = {2016},
  month = sep,
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume = {60},
  number = {1},
  pages = {1534--1538},
  issn = {1541-9312},
  doi = {10.1177/1541931213601352},
  language = {en},
  annotation = {ZSCC: 0000009},
  file = {Human Computer Interaction\\Augmented Reality\\Barde et al., 2016 - Attention Redirection Using Binaurally Spatialised Cues Delivered Over a Bone.pdf}
}

@phdthesis{barde2018,
  title = {Design {{Considerations}} for a {{Wearable}}, {{Bi}}-{{Modal Interface}}},
  author = {Barde, Amit},
  year = {2018},
  school = {University of Canterbury},
  annotation = {ZSCC: 0000000},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Barde, 2018 - Design Considerations for a Wearable, Bi-Modal Interface.pdf}
}

@inproceedings{barde2020,
  title = {The Use of Spatialised Auditory and Visual Cues for Target Acqusition in a Search Task},
  booktitle = {Audio Engineering Society Conference: 2020 {{AES}} International Conference on Audio for Virtual and Augmented Reality},
  author = {Barde, Amit and Ward, Matt and Lindeman, Robert and Billinghurst, Mark},
  year = {2020},
  month = aug,
  url = {http://www.aes.org/e-lib/browse.cfm?elib=20875}
}

@inproceedings{barrett2020,
  title = {Deepening Presence: {{Probing}} the Hidden Artefacts of Everyday Soundscapes},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Barrett, Natasha},
  year = {2020},
  series = {{{AM}} '20},
  pages = {77--84},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411120},
  abstract = {Sound penetrates our outdoor spaces. Much of it we ignore amidst our fast passage from place to place, its qualities may be too quiet or fleeting to pay heed to above the bustle of our own thoughts, or we may experience the sounds as an annoyance. Manoeuvring our listening to be excited by its features is not so easy.This paper presents new artistic research that probes the hidden artefacts of everyday soundscapes - the sounds and details which we ignore or fail to engage - and draws them into a new audible reality. The work focuses on the affordances of spatial information in a novel combination of art and technology: site-specific composition and the ways of listening established by Schaeffer and his successors are combined with the technology of beam-forming from high resolution (Eigenmike) Ambisonics recordings, Ambisonics sound-field synthesis and the deployment of a new prototype loudspeaker. Underlying the artistic and scientific research is the hypothesis that spatially distributed information offers new opportunities to explore, isolate and musically develop features of interest, and that composition should address the same degree of spatiality as the real landscape. The work is part of the 'Reconfiguring the Landscape' project investigating how 3-D electroacoustic composition and sound-art can incite a new awareness of outdoor sound environments.},
  isbn = {978-1-4503-7563-4},
  keywords = {acoustic ecology,acoustics,composition,feature extraction,higher-order ambisonics,loudspeaker technology,sonification,soundscapes},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Computational Art\\Barrett, 2020 - Deepening presence.pdf}
}

@incollection{bassett2015,
  title = {Not {{Now}}? {{Feminism}}, {{Technology}}, {{Postdigital}}},
  booktitle = {Postdigital {{Aesthetics}}},
  author = {Bassett, Caroline},
  editor = {Berry, David M. and Dieter, Michael},
  year = {2015},
  pages = {136--150},
  publisher = {{Palgrave Macmillan UK}},
  address = {{London}},
  isbn = {978-1-349-49378-4 978-1-137-43720-4},
  language = {en},
  annotation = {ZSCC: 0000021[s0]},
  file = {Arts & Humanities\\Media Studies\\Bassett, 2015 - Not Now.pdf}
}

@article{battaglia2003,
  title = {Bayesian Integration of Visual and Auditory Signals for Spatial Localization},
  author = {Battaglia, Peter W. and Jacobs, Robert A. and Aslin, Richard N.},
  year = {2003},
  month = jul,
  journal = {Journal of the Optical Society of America A},
  volume = {20},
  number = {7},
  pages = {1391},
  issn = {1084-7529, 1520-8532},
  doi = {10.1364/JOSAA.20.001391},
  language = {en},
  annotation = {ZSCC: 0000471},
  file = {Cognitive Science\\Multisensory Integration\\Battaglia et al., 2003 - Bayesian integration of visual and auditory signals for spatial localization.pdf}
}

@article{bayle2007,
  title = {Space, and More},
  author = {Bayle, Fran{\c c}ois},
  year = {2007},
  month = dec,
  journal = {Organised Sound},
  volume = {12},
  number = {3},
  pages = {241--249},
  issn = {1469-8153, 1355-7718},
  doi = {10.1017/S1355771807001872},
  abstract = {Among the questions regularly put to me, those that recur most often touch upon three aspects, three things to consider that are, truth to tell, unending within the domain of organised sounds. Those questions that take the lead are usually concerned with space \textendash{} the representation of space as well as the production of space. Then come those that deal with listening, active or passive. Last are those relating to tools for making and for listening. Together, these approaches run through the stages that are very natural in position problematics: where, when, how, for whom, why? I would like, in formulating these repeated questions once more, to make myself re-orientate them in such a way to show that by bringing to bear forty years of experience, I have tried to reply to them as much through practice as through theory, putting forward the idea of a family of operational concepts linked together through the perceptual radical `acous': acousmatic, acousmonium, acousmographe, acousmath\`eque.},
  language = {en},
  annotation = {ZSCC: 0000026},
  file = {Arts & Humanities\\Computational Art\\Bayle, 2007 - Space, and more.pdf}
}

@inproceedings{bederson1995,
  title = {Audio Augmented Reality: A Prototype Automated Tour Guide},
  shorttitle = {Audio Augmented Reality},
  booktitle = {Conference Companion on {{Human}} Factors in Computing Systems  - {{CHI}} '95},
  author = {Bederson, Benjamin B.},
  year = {1995},
  pages = {210--211},
  publisher = {{ACM Press}},
  address = {{Denver, Colorado, United States}},
  doi = {10.1145/223355.223526},
  abstract = {Augmented reality (or computer augmented environments as it is sometimes called) uses computers to enhance the richness of the real world. It differs from virtual reality in that it doesn't attempt to replace the real world. Our prototype automated tour guide superimposes audio on the world based on where a user is located. We propose this technique for use as an automated tour guide in museums and expect it will enhance the social aspects of museum visits, compared to taped tour guides.},
  isbn = {978-0-89791-755-1},
  language = {en},
  annotation = {ZSCC: 0000269},
  file = {Human Computer Interaction\\Augmented Reality\\Bederson, 1995 - Audio augmented reality.pdf}
}

@article{bermejo2017,
  title = {A Survey on Haptic Technologies for Mobile Augmented Reality},
  author = {Bermejo, Carlos and Hui, Pan},
  year = {2017},
  month = sep,
  journal = {arXiv:1709.00698 [cs]},
  eprint = {1709.00698},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1709.00698},
  urldate = {2020-01-10},
  abstract = {Augmented Reality (AR) and Mobile Augmented Reality (MAR) applications have gained much research and industry attention these days. The mobile nature of MAR applications limits users' interaction capabilities such as inputs, and haptic feedbacks. This survey reviews current research issues in the area of human computer interaction for MAR and haptic devices. The survey first presents human sensing capabilities and their applicability in AR applications. We classify haptic devices into two groups according to the triggered sense: cutaneous/tactile: touch, active surfaces, and mid-air; kinesthetic: manipulandum, grasp, and exoskeleton. Due to the mobile capabilities of MAR applications, we mainly focus our study on wearable haptic devices for each category and their AR possibilities. To conclude, we discuss the future paths that haptic feedbacks should follow for MAR applications and their challenges.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Human-Computer Interaction},
  annotation = {ZSCC: 0000020},
  file = {Human Computer Interaction\\Augmented Reality\\Bermejo and Hui, 2017 - A survey on haptic technologies for mobile augmented reality.pdf}
}

@book{berry2011,
  title = {{The philosophy of software: code and mediation in the digital age}},
  shorttitle = {{The philosophy of software}},
  author = {Berry, David M},
  year = {2011},
  publisher = {{Palgrave Macmillan}},
  address = {{Houndmills, Basingstoke, Hampshire; New York}},
  url = {http://site.ebrary.com/id/10462131},
  urldate = {2020-01-10},
  abstract = {This book is a critical introduction to code and software that develops an understanding of its social and philosophical implications in the digital age. Written specifically for people interested in the subject from a non-technical background, the book provides a lively and interesting analysis of these new media forms.},
  isbn = {9780230306479 9781283067690 9786613067692},
  language = {English.},
  annotation = {ZSCC: NoCitationData[s2]  OCLC: 716239756},
  file = {Arts & Humanities\\Media Studies\\Berry, 2011 - The philosophy of software.pdf;Arts & Humanities\\Media Studies\\Berry, 2011 - The philosophy of software2.pdf}
}

@incollection{berry2015,
  title = {Thinking {{Postdigital Aesthetics}}: {{Art}}, {{Computation}} and {{Design}}},
  shorttitle = {Thinking {{Postdigital Aesthetics}}},
  booktitle = {Postdigital {{Aesthetics}}},
  author = {Berry, David M. and Dieter, Michael},
  editor = {Berry, David M. and Dieter, Michael},
  year = {2015},
  pages = {1--11},
  publisher = {{Palgrave Macmillan UK}},
  address = {{London}},
  doi = {10.1057/9781137437204_1},
  isbn = {978-1-349-49378-4 978-1-137-43720-4},
  language = {en},
  annotation = {ZSCC: 0000142},
  file = {Arts & Humanities\\Media Studies\\Berry and Dieter, 2015 - Thinking Postdigital Aesthetics.pdf}
}

@inproceedings{berthaut2016,
  title = {{{ControllAR}}: {{Appropriation}} of {{Visual Feedback}} on {{Control Surfaces}}},
  shorttitle = {{{ControllAR}}},
  booktitle = {Proceedings of the 2016 {{ACM}} on {{Interactive Surfaces}} and {{Spaces}} - {{ISS}} '16},
  author = {Berthaut, Florent and Jones, Alex},
  year = {2016},
  pages = {271--277},
  publisher = {{ACM Press}},
  address = {{Niagara Falls, Ontario, Canada}},
  doi = {10.1145/2992154.2992170},
  abstract = {Despite the development of touchscreens, many expert systems for working with digital multimedia content, such as in music composition and performance, video editing or visual performance, still rely on control surfaces. This can be due to the accuracy and appropriateness of their sensors, the haptic feedback that they offer, and most importantly the way they can be adapted to the specific subset of gestures and tasks that users need to perform. On the other hand, visual feedback on controllers remains limited and/or fixed, preventing similar personalizing. In this paper, we propose ControllAR, a novel system that facilitates the appropriation of rich visual feedback on control surfaces through remixing of graphical user interfaces and augmented reality display. We then use our system to study current and potential appropriation of visual feedback in the case of digital musical instruments and derive guidelines for designers and developers.},
  isbn = {978-1-4503-4248-3},
  language = {en},
  annotation = {ZSCC: 0000002},
  file = {Human Computer Interaction\\Augmented Reality\\Berthaut and Jones, 2016 - ControllAR.pdf}
}

@misc{berweck2012,
  title = {It {{Worked Yesterday}}},
  author = {Berweck, Sebastian},
  year = {2012},
  urldate = {2018-04-10},
  annotation = {ZSCC: 0000027},
  file = {Arts & Humanities\\Media Studies\\Berweck, 2012 - It Worked Yesterday.pdf},
  url = {http://eprints.hud.ac.uk/id/eprint/17540/1/sberweckfinalthesis.pdf}
}

@misc{bilbow2020,
  title = {Area\textasciitilde{} 360\textdegree{} Video / Ambisonics Documentation},
  author = {Bilbow, Sam},
  year = {2020},
  month = jul,
  address = {{Brighton, United Kingdom}},
  urldate = {2020-07-16},
  abstract = {Please use headphones in order to hear the spatial audio effect of both my real and virtual (constructed) environment.     This is a VR ready 360 video / ambisonics documentation of my system "area\textasciitilde " - a gestural sound sampler that uses hand and head tracking to place and manipulate virtual audio nodes in the user's environment, heard through bone conduction headphones.      Through the development of the area\textasciitilde{} system, I call to attention the ability of non-visual augmented reality (AR) displays to provide new aesthetic experiences of real and virtual environments, namely through playful interaction methods such as head and hand gesture, and the use of bone conduction headphones as an audio AR display technology},
  url = {https://www.youtube.com/watch?v=SPd-f2EXuIQ}
}

@inproceedings{bilbow2021,
  title = {The {{Value}} of {{Sound}} within a {{Multisensory Approach}} to {{AR}} in the {{Arts}}},
  booktitle = {Workshop: {{Multisensory Augmented Reality}}},
  author = {Bilbow, Sam and Kiefer, Chris and Chevalier, Cecile},
  year = {2021},
  pages = {8},
  address = {{Italy}},
  abstract = {We explore the potential of sound within broader multisensory augmented reality, and its value in creating coherent, immersive and embodied experiences in computational art. We demonstrate this practically through accounts of the authors experiences in creating two pieces. Looking at the wider place of AR in the arts, we argue that DIY approaches to augmented reality are essential for creative work, and we speculate on how art can contribute to future theory, technologies and practice in the field.},
  language = {en},
  file = {Human Computer Interaction\\Augmented Reality\\Bilbow et al., 2021 - The Value of Sound within a Multisensory Approach to AR in the Arts.pdf}
}

@article{bilbow2021a,
  title = {The Area\textasciitilde{} System: {{Exploring}} Real and Virtual Environments through Gestural Ambisonics and Audio Augmented Reality},
  author = {Bilbow, Sam},
  year = {2021},
  month = feb,
  journal = {Sonic Scope: New Approaches to Audiovisual Culture},
  volume = {2},
  doi = {10.21428/66f840a4.b74711a8},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Bilbow, 2021 - The area~ system.pdf}
}

@inproceedings{bilbow2021b,
  title = {Developing {{Multisensory Augmented Reality As A Medium For Computational Artists}}},
  booktitle = {Proceedings of the {{Fifteenth International Conference}} on {{Tangible}}, {{Embedded}}, and {{Embodied Interaction}}},
  author = {Bilbow, Sam},
  year = {2021},
  month = feb,
  pages = {1--7},
  publisher = {{ACM}},
  address = {{Salzburg Austria}},
  doi = {10.1145/3430524.3443690},
  abstract = {This paper resituates multisensory augmented reality (MSAR) as an artistic medium for the creation of interactive and expressive works by computational artists. If an AR system can be thought of as one that combines real and virtual processes, is interactive in real-time, and is registered in three dimensions; why do we witness the majority of AR applications utilising primarily visual displays of information? In this paper, I propose a practice-led compositional approach for developing `MSAR Experiences', arguing that, as an medium that combines real and virtual multisensory processes, it must be explored with a multisensory approach. The paper further outlines the study methods that I will use to evaluate the developed experiences. The outcome of this project is the practice-led method as well as MSAR hardware, software and experiences that are developed and evaluated.},
  isbn = {978-1-4503-8213-7},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {..\\..\\Zotero\\storage\\ZAHHSULR\\Bilbow - 2021 - Developing Multisensory Augmented Reality As A Med.pdf}
}

@article{billinghurst2006,
  title = {Research {{Directions}} in {{Handheld AR}}},
  author = {Billinghurst, Mark N. and Henrysson, Anders},
  year = {2006},
  month = jan,
  journal = {International Journal of Virtual Reality},
  volume = {5},
  number = {2},
  pages = {51--58},
  issn = {1081-1451},
  doi = {10.20870/IJVR.2006.5.2.2690},
  abstract = {Handheld mobile devices are an exciting new platform for Augmented Reality (AR). Mobile phones and PDAs have the potential to provide AR experiences to hundreds of millions of consumers. However, before widespread use can occur there are some obstacles that must be overcome. In particular, developers must consider the hardware and software capabilities of mobile devices and how these can be used to provide an effective AR experience. They must also develop AR interaction metaphors suitable for handheld AR. In this paper we review current and previous research in the field, provide design guidelines and outline future research directions.},
  language = {en},
  annotation = {ZSCC: 0000060},
  file = {Human Computer Interaction\\Augmented Reality\\Billinghurst and Henrysson, 2006 - Research Directions in Handheld AR.pdf}
}

@book{billinghurst2015,
  title = {A Survey of Augmented Reality},
  author = {Billinghurst, Mark and Clark, Adrian and Lee, Gun},
  year = {2015},
  series = {Foundations and Trends in Human-Computer Interaction},
  number = {8:2-3},
  publisher = {{Now}},
  address = {{Boston Delft}},
  isbn = {978-1-60198-920-8},
  language = {eng},
  annotation = {ZSCC: 0000642  OCLC: 939905792},
  file = {Human Computer Interaction\\Augmented Reality\\Billinghurst et al., 2015 - A survey of augmented reality.pdf}
}

@book{bimber2005,
  title = {Spatial {{Augmented Reality}}: {{Merging Real}} and {{Virtual Worlds}}},
  shorttitle = {Spatial {{Augmented Reality}}},
  author = {Bimber, Oliver and Raskar, Ramesh},
  year = {2005},
  month = aug,
  edition = {Zeroth},
  publisher = {{A K Peters/CRC Press}},
  doi = {10.1201/b10624},
  isbn = {978-0-429-10850-1},
  language = {en},
  annotation = {ZSCC: 0001282},
  file = {Human Computer Interaction\\Augmented Reality\\Bimber and Raskar, 2005 - Spatial Augmented Reality.pdf}
}

@inproceedings{bisig2020,
  title = {Sounding Feet},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Bisig, Daniel and Palacio, Pablo},
  year = {2020},
  series = {{{AM}} '20},
  pages = {222--228},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411112},
  abstract = {The project emphSounding Feet explores the creative possibilities of interactively controlling sound synthesis through pressure sensitive shoe inlays that can monitor minute body movements. The project is motivated by the authors' own experience of working with interactive technologies in the context of dance. This experience has led to the desire to more closely relate the sensing capabilities of an interactive system to a dancer's own body awareness which prominently involve aspects of inner perception. The outcome of this project demonstrates that such an approach can help to establish interactive musical scenarios for dance that are not only more intuitive to work with for dancers but that also offer new possibilities for composers to tap into aspects of the dancers' expressivity that are normally hidden for an audience.},
  isbn = {978-1-4503-7563-4},
  keywords = {body awareness,dance and technology,movement sonification,wearable interface},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Computational Art\\Bisig and Palacio, 2020 - Sounding feet.pdf}
}

@incollection{blake2009,
  title = {Recording Practices and the Role of the Producer},
  booktitle = {The {{Cambridge Companion}} to {{Recorded Music}}},
  author = {Blake, Andrew},
  editor = {Cook, Nicholas and Clarke, Eric and {Leech-Wilkinson}, Daniel and Rink, John},
  year = {2009},
  pages = {36--53},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CCOL9780521865821.008},
  isbn = {978-1-139-00268-4},
  annotation = {ZSCC: NoCitationData[s0]}
}

@incollection{blum2012,
  title = {What's around {{Me}}? {{Spatialized Audio Augmented Reality}} for {{Blind Users}} with a {{Smartphone}}},
  shorttitle = {What's around {{Me}}?},
  booktitle = {Mobile and {{Ubiquitous Systems}}: {{Computing}}, {{Networking}}, and {{Services}}},
  author = {Blum, Jeffrey R. and Bouchard, Mathieu and Cooperstock, Jeremy R.},
  editor = {Puiatti, Alessandro and Gu, Tao},
  year = {2012},
  volume = {104},
  pages = {49--62},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-30973-1_5},
  abstract = {Numerous projects have investigated assistive navigation technologies for the blind community, tackling challenges ranging from interface design to sensory substitution. However, none of these have successfully integrated what we consider to be the three factors necessary for a widely deployable system that delivers a rich experience of one's environment: implementation on a commodity device, use of a preexisting worldwide point of interest (POI) database, and a means of rendering the environment that is superior to a naive playback of spoken text. Our ``In Situ Audio Services'' (ISAS) application responds to these needs, allowing users to explore an urban area without necessarily having a particular destination in mind. We describe the technical aspects of its implementation, user requirements, interface design, safety concerns, POI data source issues, and further requirements to make the system practical on a wider basis. Initial qualitative feedback from blind users is also discussed.},
  isbn = {978-3-642-30972-4 978-3-642-30973-1},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Blum et al., 2012 - What’s around Me.pdf}
}

@article{bohil2011,
  title = {Virtual Reality in Neuroscience Research and Therapy},
  author = {Bohil, Corey J. and Alicea, Bradly and Biocca, Frank A.},
  year = {2011},
  month = dec,
  journal = {Nature Reviews Neuroscience},
  volume = {12},
  number = {12},
  pages = {752--762},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3122},
  abstract = {Virtual reality (VR) environments are increasingly being used by neuroscientists to simulate natural events and social interactions. VR creates interactive, multimodal sensory stimuli that offer unique advantages over other approaches to neuroscientific research and applications. VR's compatibility with imaging technologies such as functional MRI allows researchers to present multimodal stimuli with a high degree of ecological validity and control while recording changes in brain activity. Therapists, too, stand to gain from progress in VR technology, which provides a high degree of control over the therapeutic experience. Here we review the latest advances in VR technology and its applications in neuroscience research.},
  language = {en},
  file = {Human Computer Interaction\\Virtual Reality\\Bohil et al., 2011 - Virtual reality in neuroscience research and therapy.pdf}
}

@article{bolter2013,
  title = {Media Studies, Mobile Augmented Reality, and Interaction Design},
  author = {Bolter, Jay David and Engberg, Maria and MacIntyre, Blair},
  year = {2013},
  pages = {10},
  language = {en},
  annotation = {ZSCC: 0000063},
  file = {Human Computer Interaction\\Augmented Reality\\Bolter et al., 2013 - Media studies, mobile augmented reality, and interaction design.pdf}
}

@book{bourriaud2009,
  title = {Relational Aesthetics},
  author = {Bourriaud, Nicolas},
  year = {2009},
  series = {Documents Sur l'art},
  edition = {Nachdr.},
  publisher = {{Presses du r\'eel}},
  address = {{Dijon}},
  isbn = {978-2-84066-060-6},
  language = {eng},
  annotation = {ZSCC: 0000002  OCLC: 767748638},
  file = {Arts & Humanities\\Aesthetics\\Bourriaud, 2009 - Relational aesthetics.pdf}
}

@article{brooks1996,
  title = {The {{Computer Scientist}} as {{Toolsmith II}}},
  author = {Brooks, Fred},
  year = {1996},
  journal = {Communications of the ACM},
  volume = {39},
  number = {3},
  annotation = {ZSCC: 0000469},
  file = {Human Computer Interaction\\Miscellaneous\\Brooks, 1996 - The Computer Scientist as Toolsmith II.pdf}
}

@inproceedings{brooks2020,
  title = {Trigeminal-Based {{Temperature Illusions}}},
  booktitle = {{{CHI}} '20: {{Proceedings}} of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Brooks, Jas and Nagels, Steven and Lopes, Pedro},
  year = {2020},
  doi = {10.1145/3313831.3376806},
  abstract = {We explore a temperature illusion that uses low-powered electronics and enables the miniaturization of simple warm and cool sensations. Our illusion relies on the properties of certain scents, such as the coolness of mint or hotness of peppers. These odors trigger not only the olfactory bulb, but also the nose's trigeminal nerve, which has receptors that respond to both temperature and chemicals. To exploit this, we engineered a wearable device based on micropumps and an atomizer that emits up to three custom-made ``thermal'' scents directly to the user's nose. Breathing in these scents causes the user to feel warmer or cooler. We demonstrate how our device renders warmth and cooling sensations in virtual experiences. Participants rated VR experiences with our trigeminal stimulants as significantly warmer or cooler than the baseline conditions. Lastly, we believe this offers an alternative to thermal feedback devices, which unfortunately rely on power-hungry heat-lamps or Peltier-elements.},
  annotation = {ZSCC: 0000000},
  file = {Human Computer Interaction\\Virtual Reality\\Brooks et al., 2020 - Trigeminal-based Temperature Illusions.pdf}
}

@inproceedings{brown2020,
  title = {Was That Me? {{Exploring}} the Effects of Error in Gestural Digital Musical Instruments},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Brown, Dom and Nash, Chris and Mitchell, Thomas J.},
  year = {2020},
  series = {{{AM}} '20},
  pages = {168--174},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411137},
  abstract = {Traditional Western musical instruments have evolved to be robust and predictable, responding consistently to the same player actions with the same musical response. Consequently, errors occurring in a performance scenario are typically attributed to the performer and thus a hallmark of musical accomplishment is a flawless musical rendition. Digital musical instruments often increase the potential for a second type of error as a result of technological failure within one or more components of the instrument. Gestural instruments using machine learning can be particularly susceptible to these types of error as recognition accuracy often falls short of 100\%, making errors a familiar feature of gestural music performances. In this paper we refer to these technology-related errors as system errors, which can be difficult for players and audiences to disambiguate from performer errors. We conduct a pilot study in which participants repeat a note selection task in the presence of simulated system errors. The results suggest that, for the gestural music system under study, controlled increases in system error correspond to an increase in the occurrence and severity of performer error. Furthermore, we find the system errors reduce a performer's sense of control and result in the instrument being perceived as less accurate and less responsive.},
  isbn = {978-1-4503-7563-4},
  keywords = {augmented reality,game audio,musicology,sonic interaction design,sonification,sound art,spatial audio,virtual reality},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Brown et al., 2020 - Was that me.pdf}
}

@article{burdea1996,
  title = {Multimodal {{Virtual Reality}}: {{Input}}-{{Output Devices}}, {{System Integration}}, and {{Human Factors}}},
  author = {Burdea, Grigore and Coiffet, Paul},
  year = {1996},
  url = {http://ti.rutgers.edu/publications/papers/1996_ijhci.pdf},
  urldate = {2020-05-28},
  file = {Human Computer Interaction\\Virtual Reality\\Burdea and Coiffet, 1996 - Multimodal Virtual Reality.pdf}
}

@incollection{burnham1968,
  title = {System {{Esthetics}}},
  booktitle = {Artforum},
  author = {Burnham, Jack},
  year = {1968},
  annotation = {ZSCC: 0000003},
  file = {Arts & Humanities\\Aesthetics\\Burnham, 1968 - System Esthetics.pdf}
}

@article{cadoz2014,
  title = {Tangibility, {{Presence}}, {{Materiality}}, {{Reality}} in {{Artistic Creation}} with {{Digital Technology}}},
  author = {Cadoz, Claude and Luciani, Annie and Villeneuve, Jerome and Kontogeorgakopoulos, Alexandros and Zannos, Iannis},
  year = {2014},
  pages = {9},
  abstract = {The democratization of Computer Arts and Computer Music has, due to dematerialization (virtualization) consequence of digital technologies, considerably widened the boundaries of creativity. As we are now entering a second phase that has been labeled ``post-digital'', we are called to reconcile this openness with notions such as embodiment, presence, enaction and tangibility. These notions are in our view inherently linked to creativity. Here we outline some approaches to this problem under development within the ``European Art-ScienceTechnology Network'' (EASTN1). Several areas of artistic creation are represented (Music, Animation, Multisensory Arts, Architecture, Fine Arts, Graphic communication, etc.). A main objective of this network is to establish common grounds through collaborative reflection and work on the above notions, using the concept of tangibility as a focal point. In this paper we describe several different approaches to the tangibility, in relation to concepts such as reality, materiality, objectivity, presence, concreteness, etc. and their antonyms. Our objective is to open a debate on tangibility, in the belief that it has a strong unifying potential but is also at the same time presents challenging and difficult to define. Here we present some initial thoughts on this topic in a first effort to bring together the approaches that arise from the different practices and projects developed within the partner institutions involved in the EASTN network.},
  language = {en},
  file = {Philosophy\\Materiality\\Cadoz et al., 2014 - Tangibility, Presence, Materiality, Reality in Artistic Creation with Digital.pdf}
}

@phdthesis{callahan1983,
  type = {Master of {{Science}}},
  title = {A 3-{{D}} Display Head-Set for Personalized Computing},
  author = {Callahan, Mark},
  year = {1983},
  address = {{Massachusetts Institute of Technology}},
  url = {http://hdl.handle.net/1721.1/71348},
  school = {Massachusetts Institute of Technology},
  annotation = {ZSCC: 0000020},
  file = {Human Computer Interaction\\Augmented Reality\\Callahan, 1983 - A 3-D display head-set for personalized computing.pdf}
}

@article{candy2018,
  title = {Practice-{{Based Research}} in the {{Creative Arts}}: {{Foundations}} and {{Futures}} from the {{Front Line}}},
  shorttitle = {Practice-{{Based Research}} in the {{Creative Arts}}},
  author = {Candy, Linda and Edmonds, Ernest},
  year = {2018},
  month = feb,
  journal = {Leonardo},
  volume = {51},
  number = {1},
  pages = {63--69},
  issn = {0024-094X, 1530-9282},
  doi = {10.1162/LEON_a_01471},
  language = {en},
  annotation = {ZSCC: 0000061},
  file = {Arts & Humanities\\Computational Art\\Candy and Edmonds, 2018 - Practice-Based Research in the Creative Arts.pdf}
}

@incollection{caramiaux2010,
  title = {Towards a {{Gesture}}-{{Sound Cross}}-{{Modal Analysis}}},
  booktitle = {Gesture in {{Embodied Communication}} and {{Human}}-{{Computer Interaction}}},
  author = {Caramiaux, Baptiste and Bevilacqua, Fr{\'e}d{\'e}ric and Schnell, Norbert},
  year = {2010},
  volume = {5934},
  pages = {158--170},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-12553-9_14},
  abstract = {This article reports on the exploration of a method based on canonical correlation analysis (CCA) for the analysis of the relationship between gesture and sound in the context of music performance and listening. This method is a first step in the design of an analysis tool for gesture-sound relationships. In this exploration we used motion capture data recorded from subjects performing free hand movements while listening to short sound examples. We assume that even though the relationship between gesture and sound might be more complex, at least part of it can be revealed and quantified by linear multivariate regression applied to the motion capture data and audio descriptors extracted from the sound examples. After outlining the theoretical background, the article shows how the method allows for pertinent reasoning about the relationship between gesture and sound by analysing the data sets recorded from multiple and individual subjects.},
  isbn = {978-3-642-12552-2 978-3-642-12553-9},
  language = {en},
  file = {Arts & Humanities\\Computational Art\\Caramiaux et al., 2010 - Towards a Gesture-Sound Cross-Modal Analysis.pdf}
}

@article{cardew1961,
  title = {Notation: {{Interpretation}}, Etc.},
  author = {Cardew, Cornelius},
  year = {1961},
  journal = {Tempo, New Series},
  number = {58},
  pages = {21--33},
  url = {http://www.jstor.org/stable/944250},
  language = {en},
  annotation = {ZSCC: 0000067},
  file = {Arts & Humanities\\Musicology\\Cardew, 1961 - Notation.pdf}
}

@inproceedings{caudell1992,
  title = {Augmented Reality: An Application of Heads-up Display Technology to Manual Manufacturing Processes},
  shorttitle = {Augmented Reality},
  booktitle = {Proceedings of the {{Twenty}}-{{Fifth Hawaii International Conference}} on {{System Sciences}}},
  author = {Caudell, Thomas and Mizell, David},
  year = {1992},
  pages = {659-669 vol.2},
  publisher = {{IEEE}},
  address = {{Kauai, HI, USA}},
  doi = {10.1109/HICSS.1992.183317},
  abstract = {We describe the design and prototyping steps we have taken toward the implementation of a heads-up, see-through, head-mounted display (HUDSET). Combined with head position sensing and a real world registration system, this technology allows a computer-produced diagram to be superimposed and stabilized on a specific position on a real-world object. Successful development of the HUDset technology will enable cost reductions and efficiency improvements in many of the human-involved operations in aircraft manufacturing, by eliminating templates, formboard diagrams, and other masking devices.},
  isbn = {978-0-8186-2420-9},
  language = {en},
  annotation = {ZSCC: 0000003},
  file = {Human Computer Interaction\\Augmented Reality\\Caudell and Mizell, 1992 - Augmented reality.pdf}
}

@inproceedings{cecchinato2017,
  title = {Always {{On}}(Line)?: {{User Experience}} of {{Smartwatches}} and Their {{Role}} within {{Multi}}-{{Device Ecologies}}},
  shorttitle = {Always {{On}}(Line)?},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Cecchinato, Marta E. and Cox, Anna L. and Bird, Jon},
  year = {2017},
  month = may,
  pages = {3557--3568},
  publisher = {{ACM}},
  address = {{Denver Colorado USA}},
  doi = {10.1145/3025453.3025538},
  abstract = {Users have access to a growing ecosystem of devices (desktop, mobile and wearable) that can deliver notifications and help people to stay in contact. Smartwatches are gaining popularity, yet little is known about the user experience and their impact on our increasingly always online culture. We report on a qualitative study with existing users on their everyday use of smartwatches to understand both the added value and the challenges of being constantly connected at the wrist. Our findings show that users see a large benefit in receiving notifications on their wrist, especially in terms of helping manage expectations of availability. Moreover, we find that response rates after viewing a notification on a smartwatch change based on the other devices available: laptops prompt quicker replies than smartphones. Finally, there are still many costs associated with using smartwatches, thus we make a series of design recommendations to improve the user experience of smartwatches.},
  isbn = {978-1-4503-4655-9},
  language = {en},
  annotation = {ZSCC: 0000029},
  file = {Human Computer Interaction\\User Experience Design\\Cecchinato et al., 2017 - Always On(line).pdf}
}

@inproceedings{chatzidimitris2016,
  title = {{{SoundPacman}}: {{Audio}} Augmented Reality in Location-Based Games},
  shorttitle = {{{SoundPacman}}},
  booktitle = {2016 18th {{Mediterranean Electrotechnical Conference}} ({{MELECON}})},
  author = {Chatzidimitris, Thomas and Gavalas, Damianos and Michael, Despina},
  year = {2016},
  month = apr,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Lemesos, Cyprus}},
  doi = {10.1109/MELCON.2016.7495414},
  abstract = {Sound design has received little attention in location-based games research. Typically, existing prototypes heavily rely on visual information with sound only having a marginal role in the game design and development process. This paper investigates the role of sound as primary interface for conveying game information and creating engaging gaming experiences. As a case study, we present SoundPacman, a prototype location-based game, wherein players experience the game space with the use of 3D sounds, which augment the physical environment. Preliminary tests utilizing EEG analysis provide evidence that sound augmentation may significantly contribute towards enhancing the immersion levels of players.},
  isbn = {978-1-5090-0058-6},
  language = {en},
  annotation = {ZSCC: 0000029},
  file = {Human Computer Interaction\\Augmented Reality\\Chatzidimitris et al., 2016 - SoundPacman.pdf}
}

@phdthesis{chevalier2016,
  title = {Remembering to {{Remember}}: {{A Practice}}-Based {{Study}} in {{Digital Re}}-Appropriation and {{Bodily Perception}}},
  author = {Chevalier, C{\'e}cile},
  year = {2016},
  abstract = {Through the evolution of digital media technology, social networks and more recently Web 3.0 (e.g. Cloud-based) technologies, culture and memory is being transformed, both in relation to how memories are represented, and how they may be engaged with or re-accessed.},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Chevalier, 2016 - Remembering to Remember.pdf}
}

@inproceedings{chevalier2018,
  title = {Listening {{Mirrors}}: {{Prototyping}} for a {{Hybrid Audio Augmented Reality Installation}}},
  booktitle = {{{ICLI}} 2018, 4th {{International Conference}} on {{Live Interfaces}}. {{Inspiration}}, {{Performance}}, {{Emancipation}}.},
  author = {Chevalier, C{\'e}cile and Kiefer, Chris},
  editor = {{Jos\'e Alberto Gomes} and {Miguel Carvalhais} and {Rui Penha}},
  year = {2018},
  pages = {241},
  address = {{Porto, Portugal}},
  url = {https://sro.sussex.ac.uk/id/eprint/74980/},
  urldate = {2021-04-30},
  abstract = {We introduce ongoing developments of Listening Mirrors, a sound art instal-lation and live interface for musician and non-musician alike. The piece, in its construction and interaction design, investigates ways in which collective sonic expression can be made possi-ble using Audio Augmented Reality technology (AAR) and acoustic mirrors, whilst asking how such environments promote collective sonic expression.},
  isbn = {978-989-746-170-5},
  file = {Human Computer Interaction\\Augmented Reality\\Chevalier and Kiefer, 2018 - Listening Mirrors.pdf}
}

@article{chevalier2020,
  title = {What {{Does Augmented Reality Mean}} as a {{Medium}} of {{Expression}} for {{Computational Artists}}?},
  author = {Chevalier, C{\'e}cile and Kiefer, Chris},
  year = {2020},
  month = may,
  journal = {Leonardo},
  volume = {53},
  number = {3},
  pages = {263--267},
  issn = {0024-094X, 1530-9282},
  doi = {10.1162/leon_a_01740},
  abstract = {As augmented reality (AR) quickly evolves with new technological practice, there is a growing need to question and reevaluate its potential as a medium for creative expression. The authors discuss AR within computational art, framed within AR as a medium, AR aesthetics and applications. The Forum for Augmented Reality Immersive Instruments (ARImI), a two-day event on AR, highlights both possibilities and fundamental concerns for continuing artworks in this field, including visual bias, sensory modalities, interactivity and performativity. The authors offer a new AR definition as real-time computationally mediated perception.},
  language = {en},
  annotation = {ZSCC: 0000006},
  file = {Human Computer Interaction\\Augmented Reality\\Chevalier and Kiefer, 2020 - What Does Augmented Reality Mean as a Medium of Expression for Computational.pdf}
}

@article{chng2017,
  title = {Shift-{{Life Interactive Art}}: {{Mixed}}-{{Reality Artificial Ecosystem Simulation}}},
  shorttitle = {Shift-{{Life Interactive Art}}},
  author = {Ch'ng, Eugene and Harrison, Dew and Moore, Samantha},
  year = {2017},
  month = may,
  journal = {Presence: Teleoperators and Virtual Environments},
  volume = {26},
  number = {2},
  pages = {157--181},
  issn = {1054-7460, 1531-3263},
  doi = {10.1162/PRES_a_00291},
  abstract = {This article presents a detailed design, development and implementation of a Mixed Reality Art-Science collaboration project which was exhibited during Darwin's bicentenary exhibition at Shrewsbury, England. As an artist-led project the concerns of the artist were paramount, and this article presents Shift-Life as part of an on-going exploration into the parallels between the non-linear human thinking process and computation using semantic association to link items into ideas, and ideas into holistic concepts. Our art explores perceptions and states of mind as we move our attention between the simulated world of the computer and the real-world we inhabit, which means that any viewer engagement is participatory rather than passive. From a Mixed Reality point of view, the lead author intends to explore the convergence of the physical and virtual, therefore the formalization of the Mixed Reality system, focusing on the integration of artificial life, ecology, physical sensors and participant interaction through an interface of physical props. It is common for digital media artists to allow viewers to activate a work either through a computer screen via direct keyboard or mouse manipulation, or through immersive means to activate their work, for ``Shift-Life'' the artist was concerned with a direct ``relational'' approach where viewers would intuitively engage with the installation's everyday objects, and with each other, to fully experience the piece. The Mixed Reality system is mediated via physical environmental sensors, which affect the virtual environment and autonomous agents, which in turn reacts and is expressed as virtual pixels projected onto a physical surface. The tangible hands-on interface proved to be instinctive, attractive and informative on many levels, delivering a good example of collaboration between the Arts and Science.},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Ch’ng et al., 2017 - Shift-Life Interactive Art.pdf}
}

@article{clark1998,
  title = {The {{Extended Mind}}},
  author = {Clark, A. and Chalmers, D.},
  year = {1998},
  month = jan,
  journal = {Analysis},
  volume = {58},
  number = {1},
  pages = {7--19},
  issn = {0003-2638, 1467-8284},
  doi = {10.1093/analys/58.1.7},
  language = {en},
  annotation = {ZSCC: 0005456},
  file = {Philosophy\\Extended Cognition\\Clark and Chalmers, 1998 - The Extended Mind.pdf}
}

@book{clark2001,
  title = {Being There: Putting Brain, Body, and World Together Again},
  shorttitle = {Being There},
  author = {Clark, Andy},
  year = {2001},
  series = {A {{Bradford}} Book},
  edition = {1. paperback ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  isbn = {978-0-262-53156-6 978-0-262-03240-7},
  language = {eng},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: 253803894}
}

@inproceedings{cohen1993,
  title = {Augmented Audio Reality: Telepresence/{{VR}} Hybrid Acoustic Environments},
  shorttitle = {Augmented Audio Reality},
  booktitle = {Proceedings of 1993 2nd {{IEEE International Workshop}} on {{Robot}} and {{Human Communication}}},
  author = {Cohen, M. and Aoki, S. and Koizumi, N.},
  year = {1993},
  month = nov,
  pages = {361--364},
  doi = {10.1109/ROMAN.1993.367692},
  abstract = {Augmented audio reality consists of hybrid presentations in which computer-generated sounds are overlayed on top of more directly acquired audio signals. We are exploring the alignability of binaural signals with artificially spatialized sources, synthesized by convolving monaural signals with left/right pairs of directional transfer functions. We use MAW (multidimensional audio windows), a NeXT-based system, as a binaural directional mixing console. Since the rearrangement of a dynamic map is used to dynamically select transfer functions, a user may specify the virtual location of a sound source, throwing the source into perceptual space, using exocentric graphical control to drive egocentric auditory display. As a concept demonstration, we muted a telephone, and then used MAW to spatialize a ringing signal at its location, putting the sonic image of the phone into the office environment. By juxtaposing and mixing 'real' and 'synthetic' audio transmissions, we are exploring the relationship between acoustic telepresence and VR presentations: telepresence manifests as the actual configuration of sources in a sound field, as perceivable by a dummyhead; VR is the perception yielded by filtering of virtual sources with respect to virtual sinks. We have conducted an experiment testing the usefulness of such a hybrid.{$<>$}},
  keywords = {acoustic telepresence,Acoustic testing,audio-visual systems,augmented audio reality,binaural directional mixing console,binaural signal alignability,computer-generated sounds,convolving monaural signals,directional transfer functions,egocentric auditory display,exocentric graphical control,Humans,Laboratories,Layout,left/right pairs,MAW,multidimensional audio windows,NeXT-based system,Robots,Signal synthesis,Switches,telecontrol,Telephony,transfer function dynamic selection,Transfer functions,virtual location,virtual reality,Virtual reality,VR hybrid acoustic environments},
  annotation = {ZSCC: 0000049},
  file = {Human Computer Interaction\\Augmented Reality\\Cohen et al., 1993 - Augmented audio reality.pdf}
}

@inproceedings{collins2011,
  title = {Making Gamers Cry: Mirror Neurons and Embodied Interaction with Game Sound},
  shorttitle = {Making Gamers Cry},
  booktitle = {Proceedings of the 6th {{Audio Mostly Conference}} on {{A Conference}} on {{Interaction}} with {{Sound}} - {{AM}} '11},
  author = {Collins, Karen},
  year = {2011},
  pages = {39--46},
  publisher = {{ACM Press}},
  address = {{Coimbra, Portugal}},
  doi = {10.1145/2095667.2095673},
  abstract = {In this paper, I draw on an embodied cognition approach to describe how sound mediates our identification with and empathy for video game characters. This identification is discussed in terms of mirror neurons and body schema, drawing on theoretical and empirical research to explore ways in which identity is created from our embodied interaction with sound. I conclude by suggesting ways in which sound designers and composers can use this information to create more empathy and identification between players and their game characters.},
  isbn = {978-1-4503-1081-9},
  language = {en},
  file = {Arts & Humanities\\Computational Art\\Collins, 2011 - Making gamers cry.pdf}
}

@book{collins2013,
  title = {Playing with Sound: A Theory of Interacting with Sound and Music in Video Games},
  shorttitle = {Playing with Sound},
  author = {Collins, Karen},
  year = {2013},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-01867-8},
  language = {en},
  lccn = {QA76.76.I59 C653 2013},
  keywords = {Interactive multimedia,Video games},
  file = {Arts & Humanities\\Media Studies\\Collins, 2013 - Playing with sound.pdf}
}

@misc{combinereality2020,
  title = {Portable {{Project Northstar Rig}}},
  shorttitle = {{{CombineReality}} on {{Twitter}}},
  author = {Combine Reality},
  year = {2020},
  month = apr,
  journal = {Twitter},
  urldate = {2020-05-26},
  language = {en},
  file = {..\\..\\Zotero\\storage\\FMSPZNZD\\1252638433870143488.html},
  url = {https://twitter.com/CombineReality/status/1252638433870143488}
}

@incollection{concannon1990,
  title = {Cut and {{Paste}}: {{Collage}} and the {{Art}} of {{Sound}}},
  booktitle = {Sound by {{Artists}}},
  author = {Concannon, Kevin},
  year = {1990},
  publisher = {{Art Metropole}},
  address = {{Walter Phillips Gallery}},
  annotation = {ZSCC: 0000019},
  file = {Arts & Humanities\\Musicology\\Concannon, 1990 - Cut and Paste.pdf}
}

@misc{constanzo2015,
  title = {Tool: Karma\textasciitilde{} (Sampler/Looper External) | {{Cycling}} '74},
  shorttitle = {Tool},
  author = {Constanzo, Rodrigo},
  year = {2015},
  month = may,
  urldate = {2020-05-26},
  abstract = {karma\textasciitilde{} is a looper/sampler external for Max.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\QI82CXG8\\karma-samplerlooper-external.html},
  url = {https://cycling74.com/tools/karma-samplerlooper-external}
}

@misc{cornford2016,
  title = {Review of {{Wolgang Ernst}}'s {{Sonic Time Machines}}},
  author = {Cornford, Stephen},
  year = {2016},
  month = oct,
  journal = {Theory, Culture \& Society},
  url = {https://www.theoryculturesociety.org/review-wolgang-ernsts-sonic-time-machines-stephen-cornford/},
  urldate = {2018-04-10},
  abstract = {Review of Wolgang Ernst, Sonic Time Machines (Amsterdam University Press, 2016), 184 pages, \texteuro 79.00. ~ Reviewed by Stephen Cornford ~ Book details: http://en.aup.nl/books/9789089649492-sonic-time-machines.html ~ ~ ~ ~ Sonic Time Machines is Wolfgang Ernst's first book to be published directly in},
  language = {en-US}
}

@article{correia2020,
  title = {Affordances and {{Constraints}} in {{Interactive Audio}} / {{Visual Systems}}},
  author = {Correia, Nuno and Masu, Raul},
  year = {2020},
  month = apr,
  journal = {EAI Endorsed Transactions on Creative Technologies},
  volume = {7},
  number = {23},
  pages = {164000},
  issn = {2409-9708},
  doi = {10.4108/eai.23-4-2020.164000},
  language = {en},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Correia and Masu, 2020 - Affordances and Constraints in Interactive Audio - Visual Systems.pdf}
}

@incollection{cramer2015,
  title = {What {{Is}} '{{Post}}-Digital'?},
  booktitle = {Postdigital {{Aesthetics}}},
  author = {Cramer, Florian},
  year = {2015},
  pages = {12--26},
  publisher = {{Palgrave Macmillan UK}},
  address = {{London}},
  url = {http://link.springer.com/10.1057/9781137437204_1},
  isbn = {978-1-349-49378-4 978-1-137-43720-4},
  language = {en},
  annotation = {ZSCC: 0000265},
  file = {Arts & Humanities\\Media Studies\\Cramer, 2015 - What Is 'Post-digital'.pdf}
}

@article{critchley2004,
  title = {Neural Systems Supporting Interoceptive Awareness},
  author = {Critchley, Hugo D and Wiens, Stefan and Rotshtein, Pia and {\"O}hman, Arne and Dolan, Raymond J},
  year = {2004},
  month = feb,
  journal = {Nature Neuroscience},
  volume = {7},
  number = {2},
  pages = {189--195},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1176},
  language = {en},
  annotation = {ZSCC: 0003029},
  file = {Cognitive Science\\Interoception\\Critchley et al., 2004 - Neural systems supporting interoceptive awareness.pdf}
}

@incollection{cutler2000,
  title = {Plunderphonics},
  booktitle = {Music, Electronic Media, and Culture},
  author = {Cutler, Chris},
  editor = {Simon, Emmerson},
  year = {2000},
  publisher = {{Ashgate}},
  address = {{Aldershot ; Burlington, USA}},
  isbn = {978-0-7546-0109-8},
  lccn = {ML1380 .M86 2000},
  keywords = {20th century,Electronic music,History and criticism,Music,Music and technology},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Musicology\\Cutler, 2000 - Plunderphonics.pdf}
}

@misc{cycling742020,
  title = {Max {{MSP}}},
  shorttitle = {What Is {{Max}}?},
  author = {Cycling'74},
  year = {2020},
  urldate = {2020-05-25},
  abstract = {Max is an infinitely flexible place to create interactive media software. With in-depth tools for audio, graphics, interaction, and communication, Max is an environment to explore and develop your own ideas.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\I9NPEJPU\\max.html},
  url = {https://cycling74.com/products/max}
}

@article{das2017,
  title = {Music {{Everywhere}} \textendash{} {{Augmented Reality Piano Improvisation Learning System}}},
  author = {Das, Shantanu and Glickman, Seth and Hsiao, Fu Yen and Lee, Byunghwan},
  year = {2017},
  pages = {2},
  abstract = {This paper describes the design and implementation of an augmented reality (AR) piano learning tool that uses a Microsoft HoloLens and a MIDI-over-Bluetooth-enabled electric piano. The tool presents a unique visual interface\textemdash a ``mirror key overlay'' approach\textemdash fitted for the AR environment, and opens up the possibility of on-instrument learning experiences. The curriculum focuses on teaching improvisation in blues, rock, jazz and classical genres. Users at the piano engage with interactive lessons, watch virtual hand demonstrations, see and hear example improvisations, and play their own solos and accompaniment along with AR-projected virtual musicians. The tool aims to be entertaining yet also effective in teaching core musical concepts.},
  language = {en},
  annotation = {ZSCC: 0000004},
  file = {Human Computer Interaction\\Augmented Reality\\Das et al., 2017 - Music Everywhere – Augmented Reality Piano Improvisation Learning System.pdf}
}

@incollection{davies2004,
  title = {Virtual Space},
  booktitle = {Space: {{In Science}}, {{Art}}, and {{Society}}},
  author = {Davies, Char},
  editor = {Penz, Fran{\c c}ois and Radick, Gregory and Howell, Robert},
  year = {2004},
  pages = {69--104},
  publisher = {{Cambridge University Press}},
  isbn = {978-0-521-82376-0}
}

@book{decerteau2013,
  title = {The Practice of Everyday Life.},
  shorttitle = {The Practice of Everyday Life. 1},
  author = {{de Certeau}, Michel},
  year = {2013},
  edition = {2. print},
  publisher = {{Univ. of California Press}},
  address = {{Berkeley, Calif.}},
  isbn = {978-0-520-27145-6},
  language = {eng},
  annotation = {ZSCC: 0000012  OCLC: 935601693},
  file = {Philosophy\\Space\\de Certeau, 2013 - The practice of everyday life.pdf}
}

@article{deguzman2020,
  title = {Security and {{Privacy Approaches}} in {{Mixed Reality}}: {{A Literature Survey}}},
  shorttitle = {Security and {{Privacy Approaches}} in {{Mixed Reality}}},
  author = {{de Guzman}, Jaybie A. and Thilakarathna, Kanchana and Seneviratne, Aruna},
  year = {2020},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {6},
  eprint = {1802.05797},
  eprinttype = {arxiv},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3359626},
  abstract = {Mixed reality (MR) technology development is now gaining momentum due to advances in computer vision, sensor fusion, and realistic display technologies. With most of the research and development focused on delivering the promise of MR, there is only barely a few working on the privacy and security implications of this technology. This survey paper aims to put in to light these risks, and to look into the latest security and privacy work on MR. Specifically, we list and review the different protection approaches that have been proposed to ensure user and data security and privacy in MR. We extend the scope to include work on related technologies such as augmented reality (AR), virtual reality (VR), and human-computer interaction (HCI) as crucial components, if not the origins, of MR, as well as numerous related work from the larger area of mobile devices, wearables, and Internet-of-Things (IoT). We highlight the lack of investigation, implementation, and evaluation of data protection approaches in MR. Further challenges and directions on MR security and privacy are also discussed.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Computers and Society,Computer Science - Cryptography and Security,Computer Science - Human-Computer Interaction},
  file = {Human Computer Interaction\\Mixed Reality\\de Guzman et al., 2020 - Security and Privacy Approaches in Mixed Reality.pdf}
}

@article{delanda2015,
  title = {The {{New Materiality}}: {{The New Materiality}}},
  shorttitle = {The {{New Materiality}}},
  author = {DeLanda, Manuel},
  year = {2015},
  month = sep,
  journal = {Architectural Design},
  volume = {85},
  number = {5},
  pages = {16--21},
  issn = {00038504},
  doi = {10.1002/ad.1948},
  language = {en},
  annotation = {ZSCC: 0000023},
  file = {Philosophy\\Materiality\\DeLanda, 2015 - The New Materiality.pdf}
}

@book{deleuze1988,
  title = {A Thousand Plateaus: Capitalism and Schizophrenia},
  shorttitle = {A Thousand Plateaus},
  author = {Deleuze, Gilles and Guattari, F{\'e}lix},
  year = {1988},
  publisher = {{Athlone Press}},
  address = {{London}},
  isbn = {978-0-485-12058-5 978-0-485-11335-8},
  lccn = {B77 .D413x 1988},
  keywords = {Philosophy}
}

@article{demetriou2018,
  title = {`{{Imagineering}}' Mixed Reality ({{MR}}) Immersive Experiences in the Postdigital Revolution: Innovation, c},
  author = {Demetriou, Panayiota A},
  year = {2018},
  journal = {nternational Journal of Performance Arts and DigitalMedia,},
  volume = {14},
  number = {2},
  pages = {19},
  abstract = {At the frontiers of our technoculture and experience economy, artist-researchers as catalytic agents become Imagineers and entrepreneurs of themselves. Arts are quantified in expectations of extending forms of communication with people and our environments, by creating humanistic ways of interfacing with machines. Within the experience economy the term `immersion' is overused trending towards VR, which has troubled many researchers and practitioners across disciplines. Drawing on perspectives from performance studies, digital humanities, and human-computer interaction (HCI), this paper reviews the role of XR-enabling technologies, beyond VR, in designing immersive experiences, and their integration into performance practices. It discusses the shift of the artist's role in imagineering new resources and new ways of working to immerse audiences, and it evaluates this in a postdigital context. It discusses how immersion operates, and critiques components necessary to create affective environments in terms of audience engagement, agency, participation, involvement, presence, embodiment and interaction. The article discusses how performance as a lab can act as a method of inquiry by bringing the anthropological, performative and theatrical perspectives; and the ethics of to testing immersive-enabling technologies and/or experiences within the context of live performances.},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Mixed Reality\\Demetriou, 2018 - ‘Imagineering’ mixed reality (MR) immersive experiences in the postdigital.pdf}
}

@inproceedings{denning2014,
  title = {In Situ with Bystanders of Augmented Reality Glasses: Perspectives on Recording and Privacy-Mediating Technologies},
  shorttitle = {In Situ with Bystanders of Augmented Reality Glasses},
  booktitle = {Proceedings of the 32nd Annual {{ACM}} Conference on {{Human}} Factors in Computing Systems - {{CHI}} '14},
  author = {Denning, Tamara and Dehlawi, Zakariya and Kohno, Tadayoshi},
  year = {2014},
  pages = {2377--2386},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/2556288.2557352},
  abstract = {Augmented reality (AR) devices are poised to enter the market. It is unclear how the properties of these devices will affect individuals' privacy. In this study, we investigate the privacy perspectives of individuals when they are bystanders around AR devices. We conducted 12 field sessions in caf\'es and interviewed 31 bystanders regarding their reactions to a co-located AR device. Participants were predominantly split between having indifferent and negative reactions to the device. Participants who expressed that AR devices change the bystander experience attributed this difference to subtleness, ease of recording, and the technology's lack of prevalence. Additionally, participants surfaced a variety of factors that make recording more or less acceptable, including what they are doing when the recording is being taken. Participants expressed interest in being asked permission before being recorded and in recording-blocking devices. We use the interview results to guide an exploration of design directions for privacymediating technologies.},
  isbn = {978-1-4503-2473-1},
  language = {en},
  file = {Human Computer Interaction\\Augmented Reality\\Denning et al., 2014 - In situ with bystanders of augmented reality glasses.pdf}
}

@inproceedings{desjardins2018,
  title = {Revealing {{Tensions}} in {{Autobiographical Design}} in {{HCI}}},
  booktitle = {Proceedings of the 2018 on {{Designing Interactive Systems Conference}} 2018 - {{DIS}} '18},
  author = {Desjardins, Audrey and Ball, Aubree},
  year = {2018},
  pages = {753--764},
  publisher = {{ACM Press}},
  address = {{Hong Kong, China}},
  doi = {10.1145/3196709.3196781},
  abstract = {While self-usage has long been regarded as a questionable approach in human-computer interaction (HCI) research, recent projects have shown the successful use of autobiographical design as a method to investigate long-term and intimate relations between people and technologies in everyday life. In an effort to continue the development of methodological best practices, we need to acknowledge with more nuance the tensions that arise in use. In this paper, we articulate such tensions by examining two first-hand accounts of using autobiographical design and four autobiographical design projects of other HCI researchers. Our findings address: genuine needs, design participation, intimacy, reflexivity, and authorial voice. Our contribution is constituted of critical insights into the complexities of using autobiographical design and recommendations for researchers interested in using this method.},
  isbn = {978-1-4503-5198-0},
  language = {en},
  annotation = {ZSCC: 0000017},
  file = {Research Methods\\Autobiographical Design\\Desjardins and Ball, 2018 - Revealing Tensions in Autobiographical Design in HCI.pdf}
}

@article{dey2018,
  title = {A {{Systematic Review}} of 10 {{Years}} of {{Augmented Reality Usability Studies}}: 2005 to 2014},
  shorttitle = {A {{Systematic Review}} of 10 {{Years}} of {{Augmented Reality Usability Studies}}},
  author = {Dey, Arindam and Billinghurst, Mark and Lindeman, Robert W. and Swan, J. Edward},
  year = {2018},
  month = apr,
  journal = {Frontiers in Robotics and AI},
  volume = {5},
  pages = {37},
  issn = {2296-9144},
  doi = {10.3389/frobt.2018.00037},
  abstract = {Augmented Reality (AR) interfaces have been studied extensively over the last few decades, with a growing number of user-based experiments. In this paper, we systematically review 10 years of the most influential AR user studies, from 2005 to 2014. A total of 291 papers with 369 individual user studies have been reviewed and classified based on their application areas. The primary contribution of the review is to present the broad landscape of user-based AR research, and to provide a high-level view of how that landscape has changed. We summarize the high-level contributions from each category of papers, and present examples of the most influential user studies. We also identify areas where there have been few user studies, and opportunities for future research. Among other things, we find that there is a growing trend toward handheld AR user studies, and that most studies are conducted in laboratory settings and do not involve pilot testing. This research will be useful for AR researchers who want to follow best practices in designing their own AR user studies.},
  language = {en},
  annotation = {ZSCC: 0000112},
  file = {Human Computer Interaction\\Augmented Reality\\Dey et al., 2018 - A Systematic Review of 10 Years of Augmented Reality Usability Studies.pdf}
}

@incollection{dieter2015,
  title = {Dark {{Patterns}}: {{Interface Design}}, {{Augmentation}} and {{Crisis}}},
  shorttitle = {Dark {{Patterns}}},
  booktitle = {Postdigital {{Aesthetics}}},
  author = {Dieter, Michael},
  editor = {Berry, David M. and Dieter, Michael},
  year = {2015},
  pages = {163--178},
  publisher = {{Palgrave Macmillan UK}},
  address = {{London}},
  doi = {10.1057/9781137437204_13},
  isbn = {978-1-349-49378-4 978-1-137-43720-4},
  language = {en},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {Arts & Humanities\\Media Studies\\Dieter, 2015 - Dark Patterns.pdf}
}

@article{discipio2003,
  ids = {discipioSoundInterfaceInteractive2003,discipioSoundInterfaceInteractive2003a},
  title = {'{{Sound}} Is the Interface': From Interactive to Ecosystemic Signal Processing},
  shorttitle = {`{{Sound}} Is the Interface'},
  author = {Di Scipio, Agostino},
  year = {2003},
  month = dec,
  journal = {Organised Sound},
  volume = {8},
  number = {3},
  pages = {269--277},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S1355771803000244},
  abstract = {This paper takes a systemic perspective on interactive signal processing and introduces the author's               Audible Eco-Systemic Interface               (AESI) project. It starts with a discussion of the paradigm of `interaction' in existing computer music and live electronics approaches, and develops following bio-cybernetic principles such as `system/ambience coupling', `noise', and `self-organisation'. Central to the paper is an understanding of `interaction' as a network of interdependencies among system components, and as a means for dynamical behaviour to emerge upon the contact of an autonomous system (e.g. a DSP unit) with the external environment (room or else hosting the performance). The author describes the design philosophy in his current work with the AESI (whose DSP component was implemented as a signal patch in K               YMA               5.2), touching on compositional implications (not only live electronics situations, but also sound installations).},
  language = {en},
  annotation = {ZSCC: 0000178},
  file = {Arts & Humanities\\Computational Art\\Di Scipio, 2003 - 'Sound is the interface'.pdf}
}

@book{dourish2004,
  title = {Where the Action Is: The Foundations of Embodied Interaction},
  shorttitle = {Where the Action Is},
  author = {Dourish, Paul},
  year = {2004},
  series = {A {{Bradford}} Book},
  edition = {1. MIT Press paperback ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  isbn = {978-0-262-54178-7},
  language = {eng},
  annotation = {OCLC: 254468221},
  file = {Philosophy\\Extended Cognition\\Dourish, 2004 - Where the action is.pdf}
}

@article{dupreez2008,
  title = {({{Im}}){{Materiality}} - {{On}} the {{Immateriality}} of {{Art}}},
  author = {Du Preez, Amanda},
  year = {2008},
  journal = {Image \& Text: a Journal for Design},
  volume = {2008},
  number = {14},
  pages = {30--41},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Philosophy\\Materiality\\Du Preez, 2008 - (Im)Materiality - On the Immateriality of Art.pdf}
}

@book{durlach1995,
  title = {Virtual {{Reality}}: {{Scientific}} and {{Technological Challenges}}},
  shorttitle = {Virtual {{Reality}}},
  author = {Durlach, Nathaniel and Mavor, Anne},
  year = {1995},
  month = dec,
  publisher = {{National Academies Press}},
  address = {{Washington, D.C.}},
  doi = {10.17226/4761},
  isbn = {978-0-309-05135-4},
  language = {en},
  annotation = {ZSCC: 0000917},
  file = {Human Computer Interaction\\Virtual Reality\\Durlach and Mavor, 1995 - Virtual Reality.pdf}
}

@book{earnshaw2020,
  title = {Technology, {{Design}} and the {{Arts}} - {{Opportunities}} and {{Challenges}}},
  editor = {Earnshaw, Rae and Liggett, Susan and Excell, Peter and Thalmann, Daniel},
  year = {2020},
  series = {Springer {{Series}} on {{Cultural Computing}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-42097-0},
  isbn = {978-3-030-42096-3 978-3-030-42097-0},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Earnshaw et al., 2020 - Technology, Design and the Arts - Opportunities and Challenges.pdf}
}

@article{edmonds2010,
  title = {The {{Art}} of {{Interaction}}},
  author = {Edmonds, Ernest},
  year = {2010},
  month = dec,
  journal = {Digital Creativity},
  volume = {21},
  number = {4},
  pages = {257--264},
  issn = {1462-6268, 1744-3806},
  doi = {10.1080/14626268.2010.556347},
  abstract = {Interactive art has become much more common as a result of the many ways in which the computer and the Internet have facilitated it. Issues relating to human\textendash{} computer interaction (HCI) are as important to interactive art making as issues relating to the colours of paint are to painting. It is not that HCI and art necessarily share goals. It is just that much of the knowledge of HCI and its methods can contribute to interactive art making. This paper reviews recent work that looks at these issues in the art context. In interactive digital art, the artist is concerned with how the artwork behaves, how the audience interacts with it and, ultimately, in participant experience and their degree of engagement. The paper looks at these issues and brings together a collection of research results and art practice experiences that together help to illuminate this significant new and expanding area. In particular, it is suggested that this work points towards a much needed critical language that can be used to describe, compare and discuss interactive digital art.},
  language = {en},
  annotation = {ZSCC: 0000063},
  file = {Arts & Humanities\\Computational Art\\Edmonds, 2010 - The Art of Interaction.pdf}
}

@article{edmonds2017,
  title = {Systems Theory, Systems Art and the Computer: {{Ernest Edmonds}} Interviewed by {{Francesca Franco}}},
  shorttitle = {Systems Theory, Systems Art and the Computer},
  author = {Edmonds, Ernest and Franco, Francesca},
  year = {2017},
  month = apr,
  journal = {Interdisciplinary Science Reviews},
  volume = {42},
  number = {1-2},
  pages = {169--179},
  issn = {0308-0188, 1743-2790},
  doi = {10.1080/03080188.2017.1297158},
  abstract = {This interview with Edmonds, conducted by Franco in 2016, explores how Systems art, Systems Theory, and his personal relationships with artists such as Malcolm Hughes, Kenneth Martin and Edward Ihnatowicz influenced his art practice.},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Edmonds and Franco, 2017 - Systems theory, systems art and the computer.pdf}
}

@article{edmonds2018,
  title = {Algorithmic {{Art Machines}}},
  author = {Edmonds, Ernest},
  year = {2018},
  month = jan,
  journal = {Arts},
  volume = {7},
  number = {1},
  pages = {3},
  issn = {2076-0752},
  doi = {10.3390/arts7010003},
  abstract = {The article reviews the author's personal development in relation to art made by algorithmic machines and discusses both the nature of such systems and the future implications for art.},
  language = {en},
  annotation = {ZSCC: 0000003},
  file = {Arts & Humanities\\Computational Art\\Edmonds, 2018 - Algorithmic Art Machines.pdf}
}

@inproceedings{elblaus2020,
  title = {Utruchirp: {{An}} Impulse Response Measurement and Auralisation Tool Developed for Artistic Practice},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Elblaus, Ludvig and Eckel, Gerhard},
  year = {2020},
  series = {{{AM}} '20},
  pages = {61--68},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411140},
  abstract = {This paper presents the utruchirp software, a tool for measuring impulse responses and modelling room acoustics in real time through auralisation based on convolution using those responses. utruchirp is the result of concerns and needs emerging from the authors' ongoing artistic practice, exploring room scale acoustic feedback as material for live performance, installations, and fixed media pieces as utrumque.The paper provides the technical and, more importantly, the artistic details of the development of utruchirp and its features, highlighting those that are the direct result of insights from artistic work: Monitoring of all stages of measuring and signal processing, auralisations of the measurements from within the measurement process, and integrated round trip delay estimation. Finally, it points out future directions and features that are to be explored next, with an invitation for collaborative efforts, aiming to bring the sensibilities of musical instruments to our measurement tools.},
  isbn = {978-1-4503-7563-4},
  keywords = {acoustics,composition,convolution,electroacoustic music,feedback,impulse response measurement,room modelling},
  annotation = {ZSCC: 0000001},
  file = {Arts & Humanities\\Computational Art\\Elblaus and Eckel, 2020 - Utruchirp.pdf}
}

@inproceedings{elblaus2020a,
  title = {Acoustic Modelling as a Strategy for Composing Site-Specific Music},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Elblaus, Ludvig and Eckel, Gerhard},
  year = {2020},
  series = {{{AM}} '20},
  pages = {69--76},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411141},
  abstract = {This paper describes two site-specific musical compositions, focusing on how modelling was used in their respective composition processes. Primarily, the acoustics of the sites were modelled to aid in the preparation and composition of the pieces. From this we propose the general use of modelling as a way to work with the concept of site. But the idea of formulating a model is also applicable more widely in the work described and this is discussed with the two pieces as starting points.Both pieces use acoustic room scale feedback as their only source of sound, so the impact of the room, speakers and microphones used is immense. The first piece, Rundg\aa ng, is a commission for the GRM Acousmonium. The second piece, Clockwork, is a public installation that will also be the site of a performance, combining the installation with live interventions. Clockwork will also employ modelling as a component of the piece itself, and include a remote performer and a remote audience. We suggest that there are possibilities to employ compositional strategies to embrace these kinds of hybrid presence situations by composing for many vantage points.},
  isbn = {978-1-4503-7563-4},
  keywords = {acoustics,composition,electroacoustic music,feedback,modelling,presence,room impulse response,site specificity},
  annotation = {ZSCC: 0000001},
  file = {Arts & Humanities\\Computational Art\\Elblaus and Eckel, 2020 - Acoustic modelling as a strategy for composing site-specific music.pdf}
}

@misc{eliasson2020,
  title = {Wunderkammer},
  author = {Eliasson, Olafur},
  year = {2020},
  urldate = {2020-05-25},
  abstract = {Official website of Olafur Eliasson and his studio: Studio Olafur Eliasson},
  file = {..\\..\\Zotero\\storage\\8JBQEWC8\\olafureliasson.net.html},
  url = {https://olafureliasson.net/}
}

@article{emmerson1998,
  ids = {emmersonAuralLandscapeMusical1998a},
  title = {Aural Landscape: Musical Space},
  shorttitle = {Aural Landscape},
  author = {Emmerson, Simon},
  year = {1998},
  month = aug,
  journal = {Organised Sound},
  volume = {3},
  number = {2},
  pages = {135--140},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S1355771898002064},
  abstract = {This paper seeks to examine how sound in general (and electroacoustic music in particular) can evoke a sense of being and place which may be strongly related to our visual experience. The auditory system has evolved to seek the reasons for the soundfield it encounters and this property cannot meaningfully be ignored by composers in this medium. The acousmatic condition stimulates and enhances this response. The science of acoustics cannot any longer alone explain sound phenomena and requires psychological and ecological dimensions. The idea of the `frame' is developed from large-scale to small-scale soundfields: `landscape', `arena' and `stage' are seen to be flexible components of this approach to composition. The paper concludes that a mature relationship of audio and visual art forms requires a greater acknowledgement of these attributes of sound.},
  language = {en},
  annotation = {ZSCC: 0000096},
  file = {Arts & Humanities\\Computational Art\\Emmerson, 1998 - Aural landscape.pdf}
}

@incollection{emmerson2015,
  title = {Local/{{Field}} and {{Beyond The Scale}} of {{Spaces}}},
  booktitle = {Kompositionen F\"ur H\"orbaren {{RaumDie}} Fr\"uhe Elektroakustische {{Musik}} Und Ihre {{Kontexte}}},
  author = {Emmerson, Simon},
  year = {2015},
  edition = {1. Aufl.},
  publisher = {{transcript Verlag}},
  address = {{Bielefeld}},
  doi = {10.14361/9783839430767-001},
  isbn = {978-3-8394-3076-7},
  annotation = {ZSCC: 0000000}
}

@article{engberg2014,
  title = {Cultural Expression in Augmented and Mixed Reality},
  author = {Engberg, Maria and Bolter, Jay David},
  year = {2014},
  month = feb,
  journal = {Convergence: The International Journal of Research into New Media Technologies},
  volume = {20},
  number = {1},
  pages = {3--9},
  issn = {1354-8565, 1748-7382},
  doi = {10.1177/1354856513516250},
  language = {en},
  annotation = {ZSCC: 0000024},
  file = {Human Computer Interaction\\Augmented Reality\\Engberg and Bolter, 2014 - Cultural expression in augmented and mixed reality.pdf}
}

@article{engberg2014a,
  title = {Polyaesthetic Sights and Sounds:},
  author = {Engberg, Maria},
  year = {2014},
  volume = {4},
  number = {1},
  pages = {20},
  language = {en},
  annotation = {ZSCC: 0000008},
  file = {Arts & Humanities\\Aesthetics\\Engberg, 2014 - Polyaesthetic sights and sounds.pdf}
}

@misc{eno2018,
  title = {Bloom: {{Open Space}}},
  author = {Eno, Brian and Chilvers, Peter},
  year = {2018},
  annotation = {ZSCC: NoCitationData[s0]}
}

@book{ernst2016,
  title = {Sonic Time Machines: Explicit Sound, Sirenic Voices, and Implicit Sonicity},
  shorttitle = {Sonic Time Machines},
  author = {Ernst, Wolfgang},
  year = {2016},
  series = {Recursions: Theories of Media, Materiality, and Cultural Techniques},
  publisher = {{Amsterdam University Press}},
  address = {{Amsterdam}},
  isbn = {978-90-8964-949-2},
  keywords = {Music,Philosophy and aesthetics,Sound (Philosophy),Sound in mass media}
}

@misc{espressif2020,
  title = {{{ESP32}}},
  author = {Espressif},
  year = {2020},
  urldate = {2020-05-25},
  file = {..\\..\\Zotero\\storage\\YJAA9S5C\\overview.html},
  url = {https://www.espressif.com/en/products/socs/esp32/overview}
}

@article{essl2006,
  title = {An Enactive Approach to the Design of New Tangible Musical Instruments},
  author = {Essl, Georg and O'Modhrain, Sile},
  year = {2006},
  month = dec,
  journal = {Organised Sound},
  volume = {11},
  number = {3},
  pages = {285--296},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S135577180600152X},
  abstract = {In this paper, we propose a theoretical framework for the design of tangible interfaces for musical expression. The main insight for the proposed approach is the importance and utility of familiar sensorimotor experiences for the creation of engaging and playable new musical instruments. In particular, we suggest exploiting the commonalities between different natural interactions by varying the auditory response or tactile details of the instrument within certain limits. Using this principle, devices for classes of sounds such as coarse grain collision interactions or friction interactions can be designed. The designs we propose retain the familiar tactile aspect of the interaction so that the performer can take advantage of tacit knowledge gained through experiences with such phenomena in the real world.},
  language = {en},
  annotation = {ZSCC: 0000085},
  file = {Arts & Humanities\\Computational Art\\Essl and O'Modhrain, 2006 - An enactive approach to the design of new tangible musical instruments.pdf}
}

@incollection{fazi2016,
  title = {Computational {{Aesthetics}}},
  booktitle = {A {{Companion}} to {{Digital Art}}},
  author = {Fazi, M Beatrice and Fuller, Matthew},
  editor = {Paul, Christiane},
  year = {2016},
  publisher = {{John Wiley \& Sons}},
  url = {https://monoskop.org/images/f/f1/Fazi_M_Beatrice_Fuller_Matthew_2016_Computational_Aesthetics.pdf},
  urldate = {2020-06-11},
  file = {Arts & Humanities\\Aesthetics\\Fazi and Fuller, 2016 - Computational Aesthetics.pdf}
}

@article{feiner1993,
  title = {Knowledge-Based Augmented Reality},
  author = {Feiner, Steven and Macintyre, Blair and Seligmann, Dor{\'e}e},
  year = {1993},
  month = jul,
  journal = {Communications of the ACM},
  volume = {36},
  number = {7},
  pages = {53--62},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/159544.159587},
  language = {en},
  annotation = {ZSCC: 0001460},
  file = {Human Computer Interaction\\Augmented Reality\\Feiner et al., 1993 - Knowledge-based augmented reality.pdf}
}

@article{feiner1997,
  title = {A {{Touring Machine}}: {{Prototyping 3D Mobile Augmented Reality Systems}} for {{Exploring}} the {{Urban Environment}}},
  author = {Feiner, Steven and MacIntyre, Blair and H{\"o}llerer, Tobias and Webster, Anthony},
  year = {1997},
  pages = {8},
  abstract = {We describe a prototype system that combines together the overlaid 3D graphics of augmented reality with the untethered freedom of mobile computing. The goal is to explore how these two technologies might together make possible wearable computer systems that can support users in their everyday interactions with the world. We introduce an application that presents information about our university's campus, using a head-tracked, see-through, headworn, 3D display, and an untracked, opaque, handheld, 2D display with stylus and trackpad. We provide an illustrated explanation of how our prototype is used, and describe our rationale behind designing its software infrastructure and selecting the hardware on which it runs.},
  language = {en},
  annotation = {ZSCC: 0001590},
  file = {Human Computer Interaction\\Augmented Reality\\Feiner et al., 1997 - A Touring Machine.pdf}
}

@article{feingold1995,
  title = {{{OU}}: {{Interactivity}} as {{Divination}} as {{Vending Machine}}},
  shorttitle = {{{OU}}},
  author = {Feingold, Ken},
  year = {1995},
  journal = {Leonardo},
  volume = {28},
  number = {5},
  pages = {399},
  issn = {0024094X},
  doi = {10.2307/1576224},
  language = {en},
  file = {Human Computer Interaction\\Augmented Reality\\Feingold, 1995 - OU.pdf}
}

@book{fetterman2010,
  title = {Ethnography: Step-by-Step},
  shorttitle = {Ethnography},
  author = {Fetterman, David M.},
  year = {2010},
  series = {Applied Social Research Methods Series},
  edition = {3rd ed},
  number = {17},
  publisher = {{SAGE}},
  address = {{Los Angeles}},
  isbn = {978-1-4129-5045-9},
  language = {en},
  lccn = {GN345 .F47 2010},
  keywords = {Ethnology,Methodology},
  annotation = {ZSCC: 0000011  OCLC: ocn398506695},
  file = {Research Methods\\Autoethnography\\Fetterman, 2010 - Ethnography.pdf}
}

@techreport{franziskaroesner2020,
  title = {Mixed {{Reality}}: {{Security Privacy}} and {{Safety}}},
  editor = {{Franziska Roesner} and Kohno, Tadayoshi},
  year = {2020},
  institution = {{University of Washington}},
  url = {https://ar-sec.cs.washington.edu/files/MixedReality_SecurityPrivacySafety_Summit2019.pdf},
  urldate = {2020-05-25},
  file = {Human Computer Interaction\\Mixed Reality\\2020 - Mixed Reality.pdf}
}

@book{frith1996,
  title = {Performing Rites: On the Value of Popular Music},
  shorttitle = {Performing Rites},
  author = {Frith, Simon},
  year = {1996},
  publisher = {{Harvard University Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-674-66195-0},
  language = {en},
  lccn = {ML3795 .F738 1996},
  keywords = {History and criticism,Music,Philosophy and aesthetics,Popular music,Social aspects},
  annotation = {ZSCC: 0000061},
  file = {Arts & Humanities\\Musicology\\Frith, 1996 - Performing rites.pdf}
}

@article{fruend2001,
  title = {The {{Augmented Reality Personal Digital Assistant}}},
  author = {Fruend, Juergen and Geiger, Christian and Grafe, Michael and Kleinjohann, Bernd and Institut, Heinz-Nixdorf},
  year = {2001},
  pages = {3},
  abstract = {This short paper describes a German research project on mobile augmented reality. The idea is to develop a framework that provides AR-services for the consumer market using a personal digital assistant.},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Fruend et al., 2001 - The Augmented Reality Personal Digital Assistant.pdf}
}

@book{fuchs2014,
  title = {Social Media: A Critical Introduction},
  shorttitle = {Social Media},
  author = {Fuchs, Christian},
  year = {2014},
  publisher = {{SAGE}},
  address = {{Los Angeles}},
  abstract = {"Now more than ever, we need to understand social media - the good as well as the bad. We need critical knowledge that helps us to navigate the controversies and contradictions of this complex digital media landscape. Only then can we make informed judgments about what's happening in our media world, and why. Showing the reader how to ask the right kinds of questions about social media, Christian Fuchs takes us on a journey across social media, delving deep into case studies on Google, Facebook, Twitter, WikiLeaks and Wikipedia. The result lays bare the structures and power relations at the heart of our media landscape." -- Publisher's description},
  isbn = {978-1-4462-5730-2 978-1-4462-5731-9},
  language = {en},
  lccn = {HM742 .F84 2014},
  keywords = {Social media},
  annotation = {ZSCC: 0000002  OCLC: ocn868268664},
  file = {Arts & Humanities\\Media Studies\\Fuchs, 2014 - Social media.pdf}
}

@book{gallagher2017,
  title = {Enactivist Interventions: Rethinking the Mind},
  shorttitle = {Enactivist Interventions},
  author = {Gallagher, Shaun},
  year = {2017},
  edition = {First edition},
  publisher = {{Oxford University Press}},
  address = {{Oxford New York}},
  abstract = {Enactivist Interventions' is an interdisciplinary work that explores how theories of embodied cognition illuminate many aspects of the mind, including intentionality, representation, the affect, perception, action and free will, higher-order cognition, and intersubjectivity. Gallagher argues for a rethinking of the concept of mind, drawing on pragmatism, phenomenology and cognitive science. Enactivism is presented as a philosophy of nature that has significant methodological and theoretical implications for the scientific investigation of the mind. Gallagher argues that, like the basic phenomena of perception and action, sophisticated cognitive phenomena like reflection, imagining, and mathematical reasoning are best explained in terms of an affordance-based skilled coping. He offers an account of the continuity that runs between basic action, affectivity, and a rationality that in every case remains embodied. Gallagher's analysis also addresses recent predictive models of brain function and outlines an alternative, enactivist interpretation that emphasizes the close coupling of brain, body and environment rather than a strong boundary that isolates the brain in its internal processes. The extensive relational dynamics that integrates the brain with the extra-neural body opens into an environment that is physical, social and cultural and that recycles back into the enactive process. Cognitive processes are in-the-world rather than in-the-head; they are situated in affordance spaces defined across evolutionary, developmental and individual histories, and are constrained by affective processes and normative dimensions of social and cultural practices--},
  isbn = {978-0-19-879432-5},
  language = {en},
  annotation = {ZSCC: 0000596  OCLC: 1002415983},
  file = {Philosophy\\Extended Cognition\\Gallagher, 2017 - Enactivist interventions.pdf}
}

@phdthesis{gamper2014,
  title = {Enabling Technologies for Audio Augmented Reality Systems},
  author = {Gamper, Hannes},
  year = {2014},
  abstract = {Audio augmented reality (AAR) refers to technology that embeds computer-generated auditory content into a user's real acoustic environment. An AAR system has specific requirements that set it apart from regular human--computer interfaces: an audio playback system to allow the simultaneous perception of real and virtual sounds; motion tracking to enable interactivity and location-awareness; the design and implementation of auditory display to deliver AAR content; and spatial rendering to display spatialised AAR content. This thesis presents a series of studies on enabling technologies to meet these requirements.},
  language = {en},
  school = {Aalto University},
  annotation = {ZSCC: 0000011},
  file = {Human Computer Interaction\\Augmented Reality\\Gamper, 2014 - Enabling technologies for audio augmented reality systems.pdf}
}

@book{geroimenko2014,
  title = {Augmented {{Reality Art}}: {{From}} an {{Emerging Technology}} to a {{Novel Creative Medium}}},
  shorttitle = {Augmented {{Reality Art}}},
  editor = {Geroimenko, Vladimir},
  year = {2014},
  series = {Springer {{Series}} on {{Cultural Computing}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-06203-7},
  isbn = {978-3-319-06202-0 978-3-319-06203-7},
  language = {en},
  annotation = {ZSCC: NoCitationData[s3]},
  file = {Human Computer Interaction\\Augmented Reality\\Geroimenko, 2014 - Augmented Reality Art.pdf}
}

@book{geroimenko2018,
  title = {Augmented {{Reality Art}}},
  editor = {Geroimenko, Vladimir},
  year = {2018},
  series = {Springer {{Series}} on {{Cultural Computing}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-69932-5},
  isbn = {978-3-319-69931-8 978-3-319-69932-5},
  language = {en},
  annotation = {ZSCC: 0000002},
  file = {Human Computer Interaction\\Augmented Reality\\Geroimenko, 2018 - Augmented Reality Art.pdf}
}

@article{gerzon1973,
  title = {Periphony: {{With}}-{{Height Sound Reproduction}}},
  author = {Gerzon, Michael},
  year = {1973},
  journal = {Journal of The Audio Engeneering Society},
  volume = {21},
  number = {1},
  pages = {2--10},
  url = {http://decoy.iki.fi/dsound/ambisonic/motherlode/source/Periphony_With-height_sound_reproduction_Michael%20Gerzon_JAES_Jan_Feb_1973.pdf},
  urldate = {2020-07-22},
  annotation = {ZSCC: 0000878},
  file = {Human Computer Interaction\\Audio Interfacing\\Gerzon, 1973 - Periphony.pdf}
}

@article{gibson2014,
  title = {The {{Ecological Approach}} to {{Visual Perception}}},
  author = {Gibson, James J},
  year = {2014},
  pages = {347},
  language = {en},
  annotation = {ZSCC: 0000002},
  file = {Cognitive Science\\Visual Psychology\\Gibson, 2014 - The Ecological Approach to Visual Perception.pdf}
}

@book{giddens1984,
  title = {The Constitution of Society: Outline of the Theory of Structuration},
  shorttitle = {The Constitution of Society},
  author = {Giddens, Anthony},
  year = {1984},
  publisher = {{University of California Press}},
  address = {{Berkeley}},
  isbn = {978-0-520-05292-5},
  lccn = {HM24 .G4465 1984},
  keywords = {Political sociology,Social institutions,Social structure,Sociology}
}

@incollection{godoy2006,
  title = {Playing ``{{Air Instruments}}'': {{Mimicry}} of {{Sound}}-{{Producing Gestures}} by {{Novices}} and {{Experts}}},
  shorttitle = {Playing ``{{Air Instruments}}''},
  booktitle = {Gesture in {{Human}}-{{Computer Interaction}} and {{Simulation}}},
  author = {God{\o}y, Rolf Inge and Haga, Egil and Jensenius, Alexander Refsum},
  editor = {Gibet, Sylvie and Courty, Nicolas and Kamp, Jean-Fran{\c c}ois},
  year = {2006},
  volume = {3881},
  pages = {256--267},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11678816_29},
  abstract = {Both musicians and non-musicians can often be seen making sound-producing gestures in the air without touching any real instruments. Such ''air playing'' can be regarded as an expression of how people perceive and imagine music, and studying the relationships between these gestures and sound might contribute to our knowledge of how gestures help structure our experience of music.},
  isbn = {978-3-540-32624-3 978-3-540-32625-0},
  language = {en},
  file = {Arts & Humanities\\Computational Art\\Godøy et al., 2006 - Playing “Air Instruments”.pdf}
}

@misc{google2020,
  title = {{{ARCore}}},
  author = {Google},
  year = {2020},
  journal = {ARCore},
  urldate = {2020-05-25},
  abstract = {With ARCore, build new augmented reality experiences that seamlessly blend the digital and physical worlds. Transform the way people play, shop, learn, create, and experience the world together\textemdash at Google scale.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\YTYX7MTZ\\ar.html},
  url = {https://developers.google.com/ar}
}

@misc{gordijn2017,
  title = {Concrete {{Storm}}, {{Studio Drift}}},
  author = {Gordijn, Lonneke and Nauta, Ralph},
  year = {2017},
  annotation = {ZSCC: NoCitationData[s0]}
}

@misc{gottschalk2017,
  title = {Move {{Over}}, {{Virtual Reality}}\textemdash a {{New Artistic Medium Is}} about to {{Emerge}}},
  author = {Gottschalk, Molly},
  year = {2017},
  month = mar,
  journal = {Artsy},
  urldate = {2020-05-27},
  abstract = {This week at The Armory Show, a new technology-driven medium, mixed reality, makes its debut courtesy of Studio Drift and Microsoft's HoloLens.},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {..\\..\\Zotero\\storage\\RFCTUW5Z\\artsy-editorial-move-virtual-reality-new-artistic-medium-emerge.html},
  url = {https://www.artsy.net/article/artsy-editorial-move-virtual-reality-new-artistic-medium-emerge}
}

@article{gould2014,
  title = {Invisible Visualities: {{Augmented}} Reality Art and the Contemporary Media Ecology},
  shorttitle = {Invisible Visualities},
  author = {Gould, Amanda Starling},
  year = {2014},
  month = feb,
  journal = {Convergence: The International Journal of Research into New Media Technologies},
  volume = {20},
  number = {1},
  pages = {25--32},
  issn = {1354-8565, 1748-7382},
  doi = {10.1177/1354856513514332},
  abstract = {Augmented reality (AR) art is a form of artistic expression that complicates traditional notions of the visual arts. A visual AR artist trades in what we might call invisible visualities. In this essay, I consider the questions why does AR art matter as a cultural form of expression? and what does AR art contribute to contemporary technoliterary theoretical discourse? by putting several recent AR artworks into dialogue with some of today's most important literary-media theorists.},
  language = {en},
  annotation = {ZSCC: 0000010},
  file = {Human Computer Interaction\\Augmented Reality\\Gould, 2014 - Invisible visualities.pdf}
}

@article{graham2013,
  title = {Augmented Reality in Urban Places: Contested Content and the Duplicity of Code: {{{\emph{Augmented}}}}{\emph{ Reality in Urban Places}}},
  shorttitle = {Augmented Reality in Urban Places},
  author = {Graham, Mark and Zook, Matthew and Boulton, Andrew},
  year = {2013},
  month = jul,
  journal = {Transactions of the Institute of British Geographers},
  volume = {38},
  number = {3},
  pages = {464--479},
  issn = {00202754},
  doi = {10.1111/j.1475-5661.2012.00539.x},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Graham et al., 2013 - Augmented reality in urban places.pdf}
}

@inproceedings{graham2019,
  title = {Composing Spatial Soundscapes Using Acoustic Metasurfaces},
  booktitle = {Proceedings of the 14th {{International Audio Mostly Conference}}: {{A Journey}} in {{Sound}}},
  author = {Graham, Thomas J. and Magnusson, Thor and Rajguru, Chinmay and Yazdan, Arash Pour and Jacobs, Alex and Memoli, Gianluca},
  year = {2019},
  month = sep,
  pages = {103--110},
  publisher = {{ACM}},
  address = {{Nottingham United Kingdom}},
  doi = {10.1145/3356590.3356607},
  abstract = {In this work, we explore the use of acoustic metamaterials in delivering spatially significant acoustic experiences. In particular, we discuss a user study run in a space where a dedicated composition is played through a metamaterial "prism". Results show users perceive sound to be louder in the direction determined by the metamaterial, depending on its frequency. This demonstrates how an acoustic metamaterial prism, in combination with an electronic composer, may be used to deliver different sound messages to different parts of an audience, even with a single speaker. We underpin our conclusions with user observations and heuristic considerations on possible application scenarios.},
  isbn = {978-1-4503-7297-8},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Graham et al., 2019 - Composing spatial soundscapes using acoustic metasurfaces.pdf}
}

@article{grasset2008,
  title = {Art and {{Mixed Reality}}: {{New Technology}} for {{Seamless Merging Between Virtual}} and {{Real}}},
  author = {Grasset, Rapha{\"e}l and Woods, Eric and Billinghurst, Mark},
  year = {2008},
  journal = {IMedia-Space},
  number = {1},
  pages = {10},
  abstract = {Mixed Reality (MR) describes new technology that intrinsically supports the mixing between the real world with the virtual world. In this paper, we present different interactive Mixed Reality experiences we have been developing that explore the artistic applications of the technology. We discuss our approach, the knowledge we have gained and review issues raised by these diverse experiences. Finally, we introduce some initial design guidelines to help others to develop their own interactive Mixed Reality artistic creations.},
  language = {en},
  annotation = {ZSCC: 0000015},
  file = {Human Computer Interaction\\Augmented Reality\\Grasset et al., 2008 - Art and Mixed Reality.pdf}
}

@article{gupta2018,
  title = {On the Use of Closed-Back Headphones for Active Hear-through Equalization in Augmented Reality Applications},
  author = {Gupta, Rishabh and Ranjan, Rishabh and He, Jianjun and Gan, Woon-Seng},
  year = {2018},
  pages = {12},
  abstract = {Augmented Reality (AR) audio refers to techniques where virtual sounds are superimposed with real sounds to produce immersive digital content. Headphones are widely used in consumer devices for playback of virtual sounds. However, for AR audio, an important step is to make sure that headphones allow external sounds to pass through naturally. To achieve this, a technique called Hear-Through (HT) processing is commonly employed to reproduce the incoming real sound by playing back processed version of it. In this context, open-back and headphones and closed in-ear headphones have been employed for HT processing. Closed-back headphones provide strong isolation unlike open-back headphones and do not have fittings issue as well as modified ear canal resonance effect found in closed in-ear headphones. In this paper, an investigation of HT design using closed-back circumaural headphones equipped with two pairs of microphones was conducted. An adaptive filtering algorithm was used to derive the ideal equalization filter. To alleviate the direction dependency of the ideal equalization filter and simplify HT filter design, two simplified equalization filters were also introduced. Experiments with objective evaluation using spectral difference and subjective evaluation on both timbre and spatial performance were conducted. These experimental results indicate a close match of the ideal equalized signal with the reference signal in open ear listening, which is slightly degraded in the simplified equalization filters, but outperforms the default ambient hear-through mode in the commercial headphones.},
  language = {en},
  annotation = {ZSCC: 0000008},
  file = {Human Computer Interaction\\Augmented Reality\\Gupta et al., 2018 - On the use of closed-back headphones for active hear-through equalization in.pdf}
}

@inproceedings{guzman-serrano2019,
  title = {Where {{There Are Flies}}, {{Media Art You}}'ll {{Find}}: {{Digital}} ({{Im}})Materiality, {{Artistic Medium}}, and {{Media Art Decay}}},
  shorttitle = {Where {{There Are Flies}}, {{Media Art You}}'ll {{Find}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Digital}} and {{Interactive Arts}}},
  author = {{Guzman-Serrano}, Rodrigo},
  year = {2019},
  month = oct,
  pages = {1--7},
  publisher = {{ACM}},
  address = {{Braga Portugal}},
  doi = {10.1145/3359852.3359903},
  abstract = {The digital revolution has already left its footprint in the cultural industry by not only introducing new aesthetics and art forms, but also by crucially transforming the practices of museums, libraries, archives, and cultural institutions in general. However, although most museums rely on the use of digital technologies in one way or another, few actively collect and preserve media and digital art. This apparent contradiction has not only endangered the future availability of specific artworks but it has also impeded the proper contextualization and historicization of media art.},
  isbn = {978-1-4503-7250-3},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Philosophy\\Materiality\\Guzman-Serrano, 2019 - Where There Are Flies, Media Art You'll Find.pdf}
}

@inproceedings{hafidh2013,
  title = {F-{{Glove}}: {{A}} Glove with Force-Audio Sensory Substitution System for Diabetic Patients},
  shorttitle = {F-{{Glove}}},
  booktitle = {2013 {{IEEE International Symposium}} on {{Haptic Audio Visual Environments}} and {{Games}} ({{HAVE}})},
  author = {Hafidh, Basim and Osman, Hussein Al and Alowaidi, Majed and {El-Saddik}, Abdulmotaleb and Liu, Xiaoping P.},
  year = {2013},
  month = oct,
  pages = {34--38},
  publisher = {{IEEE}},
  address = {{Istanbul, Turkey}},
  doi = {10.1109/HAVE.2013.6679607},
  abstract = {Some diabetic patients experience difficulties in modulating the grip force magnitude when they manipulate objects using their hands. This difficulty is caused by the sensory loss at the fingertips that impairs the feedback loop between the brain and the aforementioned sensors. In this paper, we present a sensory substitution system called ``F-Glove'', which is aimed at helping diabetic patients to manipulate objects more efficiently by using appropriate forces. This is achieved by substituting the force felt at the fingertips when grip an object with an audio feedback displayed through the earphones of a mobile phone. The patient wears a glove integrated with pressure sensors mounted on the fingertips, and the sensors' pressure signals are conditioned and wirelessly transmitted to a mobile phone interface to display an audio with a volume linearly proportional to the pressure applied by the fingers of patients.},
  isbn = {978-1-4799-0849-3 978-1-4799-0848-6},
  language = {en},
  annotation = {ZSCC: 0000005},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Hafidh et al., 2013 - F-Glove.pdf}
}

@inproceedings{hashizume2018,
  title = {Trans-Scale {{Playground}}: {{An Immersive Visual Telexistence System}} for {{Human Adaptation}}},
  shorttitle = {Trans-Scale {{Playground}}},
  booktitle = {The 31st {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology Adjunct Proceedings}}},
  author = {Hashizume, Satoshi and Ishii, Akira and Suzuki, Kenta and Takazawa, Kazuki and Ochiai, Yoichi},
  year = {2018},
  month = oct,
  pages = {66--68},
  publisher = {{ACM}},
  address = {{Berlin Germany}},
  doi = {10.1145/3266037.3266103},
  abstract = {In this paper, we present a novel telexistence system and design methods for telexistence studies to explore spatialscale deconstruction. There have been studies on the experience of dwarf-sized or giant-sized telepresence have been conducted over a period of many years. In this study, we discuss the scale of movements, image transformation, technical components of telepresence robots, and user experiences of telexistence-based spatial transformations. We implemented two types of telepresence robots with an omnidirectional stereo camera setup for a spatial trans-scale experience, wheeled robots, and quadcopters. These telepresence robots provide users with a trans-scale experience for a distance ranging from 15 cm to 30 m. We conducted user studies for different camera positions on robots and for different image transformation method.},
  isbn = {978-1-4503-5949-8},
  language = {en},
  annotation = {ZSCC: 0000001},
  file = {Human Computer Interaction\\Augmented Reality\\Hashizume et al., 2018 - Trans-scale Playground.pdf}
}

@article{hayes2011,
  title = {Vibrotactile {{Feedback}}-{{Assisted Performance}}},
  author = {Hayes, Lauren},
  year = {2011},
  pages = {4},
  abstract = {When performing digital music it is important to be able to acquire a comparable level of sensitivity and control to what can be achieved with acoustic instruments. By examining the links between sound and touch, new compositional and performance strategies start to emerge for performers using digital instruments1. These involve technological implementations utilizing the haptic2 information channels, o{$\carriagereturn$}ering insight into how our tacit knowledge of the physical world can be introduced to the digital domain, enforcing the view that sound is a `species of touch' [14].},
  language = {en},
  annotation = {ZSCC: 0000031},
  file = {Human Computer Interaction\\Augmented Reality\\Hayes, 2011 - Vibrotactile Feedback-Assisted Performance.pdf}
}

@inproceedings{hayes2018,
  title = {Live {{Electronic Music Performance}}: {{Embodied}} and {{Enactive Approaches}}},
  shorttitle = {Live {{Electronic Music Performance}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Movement}} and {{Computing}}},
  author = {Hayes, Lauren},
  year = {2018},
  month = jun,
  pages = {1--3},
  publisher = {{ACM}},
  address = {{Genoa Italy}},
  doi = {10.1145/3212721.3212891},
  abstract = {Mini Savior Opt. (2017) is a twenty-five minute live electronic performance which demonstrates an enactive and embodied approach to interactive and improvisational music systems. The piece was formed out of a playful exploration of my most recent hybrid analogue/digital performance system. An excessive number of components mutually affect each other through an ecological network of sound analysis and digital signal processing (DSP). Engaging with different parts of the instrument through tangible and haptic controllers, I bring a sense of immediacy into my hands: the slightest movement may trigger a drastic change in sound, which in turn may activate other processes within the network. Through the physical struggle, the performer, vulnerable to the fragile instabilities that have been potentialised, attempts to navigate the performance space.},
  isbn = {978-1-4503-6504-8},
  language = {en},
  annotation = {ZSCC: 0000002},
  file = {Arts & Humanities\\Computational Art\\Hayes, 2018 - Live Electronic Music Performance.pdf}
}

@article{hayes2019,
  title = {Beyond {{Skill Acquisition}}: {{Improvisation}}, {{Interdisciplinarity}}, and {{Enactive Music Cognition}}},
  shorttitle = {Beyond {{Skill Acquisition}}},
  author = {Hayes, Lauren},
  year = {2019},
  month = sep,
  journal = {Contemporary Music Review},
  volume = {38},
  number = {5},
  pages = {446--462},
  issn = {0749-4467, 1477-2256},
  doi = {10.1080/07494467.2019.1684059},
  language = {en},
  file = {Philosophy\\Extended Cognition\\Hayes, 2019 - Beyond Skill Acquisition.pdf}
}

@article{hennion1983,
  title = {The Production of Success: An Anti-Musicology of the Pop Song},
  shorttitle = {The Production of Success},
  author = {Hennion, Antoine},
  year = {1983},
  month = jan,
  journal = {Popular Music},
  volume = {3},
  pages = {159},
  issn = {0261-1430, 1474-0095},
  language = {en}
}

@article{hesmondhalgh1998,
  title = {The {{British Dance Music Industry}}: {{A Case Study}} of {{Independent Cultural Production}}},
  shorttitle = {The {{British Dance Music Industry}}},
  author = {Hesmondhalgh, David},
  year = {1998},
  month = jun,
  journal = {The British Journal of Sociology},
  volume = {49},
  number = {2},
  pages = {234},
  issn = {00071315},
  doi = {10.2307/591311},
  abstract = {This article analyses the British dance music industry and assesses claims that it offers a powerful alternative to the 'mainstream' music business. Two unusual features of the sector are identified. Whereas the recording industry as a whole is marked by concentration and centralization, the UK dance music industry is relatively decentralized and is made up of large numbers of 'independent' companies. Reasons for the success of small, local companies are offered, in particular the emphasis amongst dance audiences on genre, rather than on performer identity; and the low promotional costs enabled by negative press coverage of 'acid house' in the late 1980s. But the article argues that a number of features of the British dance music industry work against a view of the sector as a radical challenge to prevailing cultural-industry practices. These are as follows: firstly, the reliance of dance music companies on crossover hits and compilation albums; secondly, close ties between the independents and corporate partners; and thirdly, the pressures placed upon small companies to follow the standard ways of dealing with risk in the recording industry- in particular, the development of a star system.},
  language = {en},
  annotation = {ZSCC: 0000289},
  file = {Arts & Humanities\\Musicology\\Hesmondhalgh, 1998 - The British Dance Music Industry.pdf}
}

@book{holland2019,
  title = {New {{Directions}} in {{Music}} and {{Human}}-{{Computer Interaction}}},
  editor = {Holland, Simon and Mudd, Tom and {Wilkie-McKenna}, Katie and McPherson, Andrew and Wanderley, Marcelo M.},
  year = {2019},
  series = {Springer {{Series}} on {{Cultural Computing}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-92069-6},
  isbn = {978-3-319-92068-9 978-3-319-92069-6},
  language = {en},
  annotation = {ZSCC: 0000007},
  file = {Arts & Humanities\\Computational Art\\Holland et al., 2019 - New Directions in Music and Human-Computer Interaction.pdf}
}

@incollection{hollerer2004,
  title = {Mobile {{Augmented Reality}}},
  booktitle = {Telegeoinformatics: {{Location}}-{{Based Computing}} and {{Services}}},
  author = {H{\"o}llerer, T. and Feiner, S.},
  year = {2004},
  month = mar,
  edition = {First},
  publisher = {{CRC Press}},
  doi = {10.1201/b12395},
  isbn = {978-0-203-50107-8},
  language = {en},
  annotation = {ZSCC: 0000509},
  file = {Human Computer Interaction\\Augmented Reality\\Höllerer and Feiner, 2004 - Mobile Augmented Reality.pdf}
}

@article{holloway-attaway2014,
  title = {Performing Materialities: {{Exploring}} Mixed Media Reality and {{{\emph{Moby}}}}{\emph{-}}{{{\emph{Dick}}}}},
  shorttitle = {Performing Materialities},
  author = {{Holloway-Attaway}, Lissa},
  year = {2014},
  month = feb,
  journal = {Convergence: The International Journal of Research into New Media Technologies},
  volume = {20},
  number = {1},
  pages = {55--68},
  issn = {1354-8565, 1748-7382},
  doi = {10.1177/1354856513514337},
  abstract = {In my research, I explore mixed reality applications developed to engage and sustain collaborative and participatory digital narratives. In particular, I provide a theoretical context for a collaborative research project, The (re-)Mapping Moby Project, to illustrate how augmented reality tools and social media applications are used to sustain a critical/creative reading of Herman Melville's 1851 work Moby-Dick through participatory, performative, and locative digital practices. I address how both `texts' and `bodies' assume ontological properties through interfaces and responses that foreground affect, and I demonstrate methods to map locative and narrative shifts as they move from print to digital forms.},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Holloway-Attaway, 2014 - Performing materialities.pdf}
}

@misc{htc2020,
  title = {{{VIVE}}},
  author = {HTC},
  year = {2020},
  urldate = {2020-07-12},
  abstract = {Whether it's for gaming or business, discover the ideal VIVE device for you, and learn more about what features should be included in a high-end VR solution.},
  file = {..\\..\\Zotero\\storage\\6UNN9RA9\\product.html},
  url = {https://www.vive.com/uk/product/}
}

@book{huber2013,
  title = {Modern {{Recording Techniques}}},
  author = {Huber, David Miles and Runstein, Robert E.},
  year = {2013},
  month = aug,
  publisher = {{CRC Press}},
  abstract = {As the most popular and authoritative guide to recording Modern Recording Techniques provides everything you need to master the tools and day to day practice of music recording and production. From room acoustics and running a session to mic placement and designing a studio Modern Recording Techniques will give you a really good grounding in the theory and industry practice. Expanded to include the latest digital audio technology the 7th edition now includes sections on podcasting, new surround sound formats and HD and audio.If you are just starting out or looking for a step up in industry, Modern Recording Techniques provides an in depth excellent read- the must have book},
  googlebooks = {RfiBAAAAQBAJ},
  isbn = {978-1-136-11782-4},
  language = {en},
  keywords = {Computers / Digital Media / Audio,Music / General,Music / Recording \& Reproduction,Technology \& Engineering / Acoustics \& Sound,TECHNOLOGY \& ENGINEERING / Mechanical}
}

@book{hugill2018,
  title = {The {{Digital Musician}}},
  author = {Hugill, Andrew},
  year = {2018},
  edition = {Third edition},
  publisher = {{Routledge}},
  address = {{New York ; London}},
  isbn = {978-0-203-70421-9},
  language = {en},
  lccn = {ML3876},
  keywords = {Computer music,History and criticism,Instruction and study,Music,Philosophy and aesthetics},
  file = {Arts & Humanities\\Computational Art\\Hugill, 2018 - The Digital Musician.pdf}
}

@article{hutmacher2019,
  title = {Why {{Is There So Much More Research}} on {{Vision Than}} on {{Any Other Sensory Modality}}?},
  author = {Hutmacher, Fabian},
  year = {2019},
  month = oct,
  journal = {Frontiers in Psychology},
  volume = {10},
  pages = {2246},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02246},
  abstract = {Why is there so much more research on vision than on any other sensory modality? There is a seemingly easy answer to this question: It is because vision is our most important and most complex sense. Although there are arguments in favor of this explanation, it can be challenged in two ways: by showing that the arguments regarding the importance and complexity of vision are debatable and by demonstrating that there are other aspects that need to be taken into account. Here, I argue that the explanation is debatable, as there are various ways of defining ``importance'' and ``complexity'' and, as there is no clear consensus that vision is indeed the most important and most complex of our senses. Hence, I propose two additional explanations: According to the methodological-structural explanation, there is more research on vision because the available, present-day technology is better suited for studying vision than for studying other modalities \textendash{} an advantage which most likely is the result of an initial bias toward vision, which reinforces itself. Possible reasons for such an initial bias are discussed. The cultural explanation emphasizes that the dominance of the visual is not an unchangeable constant, but rather the result of the way our societies are designed and thus heavily influenced by human decision-making. As it turns out, there is no universal hierarchy of the senses, but great historical and cross-cultural variation. Realizing that the dominance of the visual is socially and culturally reinforced and not simply a law of nature, gives us the opportunity to take a step back and to think about the kind of sensory environments we want to create and about the kinds of theories that need to be developed in research.},
  language = {en},
  file = {Cognitive Science\\Multisensory Integration\\Hutmacher, 2019 - Why Is There So Much More Research on Vision Than on Any Other Sensory Modality.pdf}
}

@book{hutto2017,
  title = {Radicalizing Enactivism: Basic Minds without Content},
  shorttitle = {Radicalizing Enactivism},
  author = {Hutto, Daniel D and Myin, Erik and {MIT Press}},
  year = {2017},
  publisher = {{MIT Press}},
  address = {{Cambridge, Massachusetts; London}},
  isbn = {978-0-262-01854-8 978-0-262-53464-2},
  language = {English},
  annotation = {OCLC: 1027053426}
}

@book{hutto2017a,
  title = {Evolving Enactivism: Basic Minds Meet Content},
  shorttitle = {Evolving Enactivism},
  author = {Hutto, Daniel D. and Myin, Erik},
  year = {2017},
  publisher = {{MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-03611-5},
  lccn = {BD418.3 .H88 2017},
  keywords = {Act (Philosophy),Cognitive science,Content (Psychology),Intentionalism,Intentionality (Philosophy),Mental representation,Phenomenology,Philosophy of mind}
}

@inproceedings{iber2019,
  title = {Auditory {{Augmented Reality}} for {{Cyber Physical Production Systems}}},
  booktitle = {Proceedings of the 14th {{International Audio Mostly Conference}}: {{A Journey}} in {{Sound}}},
  author = {Iber, Michael and Lechner, Patrik and Jandl, Christian and Mader, Manuel and Reichmann, Michael},
  year = {2019},
  month = sep,
  pages = {53--60},
  publisher = {{ACM}},
  address = {{Nottingham United Kingdom}},
  doi = {10.1145/3356590.3356600},
  abstract = {We describe a proof-of-concept approach on the sonification of estimated operation states of 3D printing processes. The results of this study form the basis for the development of an ``intelligent'' noise protection headphone as part of Cyber Physical Production Systems, which provides auditorily augmented information to machine operators and enables radio communication between them. Further application areas are implementations in control rooms (equipped with multichannel loudspeaker systems) and utilization for training purposes. The focus of our research lies on situation-specific acoustic processing of conditioned machine sounds and operation related data with high information content, considering the often highly auditorily influenced working knowledge of skilled workers. As a proof-of-concept the data stream of error probability estimations regarding partly manipulated 3D printing processes was mapped to three sonification models, giving evidence about momentary operation states. The neural network applied indicates a high accuracy ({$>$}93\%) concerning error estimation distinguishing between normal and manipulated operation states. None of the manipulated states could be identified by listening. An auditory augmentation, respectively sonification of these error estimations provides a considerable benefit to process monitoring.},
  isbn = {978-1-4503-7297-8},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Human Computer Interaction\\Augmented Reality\\Iber et al., 2019 - Auditory Augmented Reality for Cyber Physical Production Systems.pdf}
}

@misc{ircam2014,
  title = {Leap {{Motion}} Skeletal Tracking in {{Max}}},
  author = {IRCAM},
  year = {2014},
  month = nov,
  journal = {Sound Music Movement Interaction - ISMM},
  urldate = {2020-05-25},
  abstract = {We~developed a new object for using the Leap Motion in Max, based on~the Leap~Motion SDK V2~Skeletal Tracking~Beta.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\RVSX4FD7\\leapmotion.html},
  url = {http://ismm.ircam.fr/leapmotion/}
}

@inproceedings{jarvis2020,
  title = {Composing in Spacetime with Rainbows: {{Spatial}} Metacomposition in the Real World},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Jarvis, Robert and Verhagen, Darrin},
  year = {2020},
  series = {{{AM}} '20},
  pages = {175--182},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411136},
  abstract = {There exists a long tradition of incorporating acoustic space as a creative parameter in musical composition and performance. This creative potential has been extended by way of modern sensing and computing technology which allows the position of the listener to act as an input to interactive musical works in immersive, digital environments. Furthermore, the sophistication of sensing technology has reached a point where barriers to implementing these digital interactive musical systems in the physical world are dissolving.In this research we have set out to understand what new modes of artistic performance might be enabled by these interactive spatial musical systems, and what the analysis of these systems can tell us about the compositional principles of arranging musical elements in space as well as time.We have applied a practice-based approach, leveraging processes of software development, composition, and performance to create a complete system for composing and performing what we refer to as spatial metacompositions. The system is tested at scale in the realisation of a musical work based upon the path of a sailplane in flight.Analysis of the work and the supporting system leads us to suggest opportunities exist for extending existing intermodal composition theory through the analysis of audiovisual renderings of performed spatial works. We also point to unique challenges posed by spatial arrangement, such as effective strategies for structuring musical notes in three dimensions as to produce strong harmonic movement.Beyond enabling new modes of artistic expression, the understanding garnered from these musical structures may help inform a more generalisable approach to non-linear composition, leveraging virtual representations of musical space that respond to arbitrary input data.},
  isbn = {978-1-4503-7563-4},
  keywords = {composition,flight,metacomposition,music,music technology,musical performance,performing arts,sonification},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Jarvis and Verhagen, 2020 - Composing in spacetime with rainbows.pdf}
}

@article{johnson2017,
  title = {{{VRMin}}: {{Using Mixed Reality}} to {{Augment}} the {{Theremin}} for {{Musical Tutoring}}},
  author = {Johnson, David and Tzanetakis, George},
  year = {2017},
  pages = {6},
  abstract = {The recent resurgence of Virtual Reality (VR) technologies provide new platforms for augmenting traditional music instruments. Instrument augmentation is a common approach for designing new interfaces for musical expression, as shown through hyperinstrument research. New visual affordances present in VR give designers new methods for augmenting instruments to extend not only their expressivity, but also their capabilities for computer assisted tutoring. In this work, we present VRMin, a mobile Mixed Reality (MR) application for augmenting a physical theremin, with an immersive virtual environment (VE), for real time computer assisted tutoring. We augment a physical theremin with 3D visual cues to indicate correct hand positioning for performing given notes and volumes. The physical theremin acts as a domain specific controller for the resulting MR environment. The initial effectiveness of this approach is measured by analyzing a performer's hand position while training with and without the VRMin. We also evaluate the usability of the interface using heuristic evaluation based on a newly proposed set of guidelines designed for VR musical environments.},
  language = {en},
  annotation = {ZSCC: 0000007},
  file = {Human Computer Interaction\\Virtual Reality\\Johnson and Tzanetakis, 2017 - VRMin.pdf}
}

@article{julier2000,
  title = {{{BARS}}: {{Battlefield Augmented Reality System}}},
  author = {Julier, Simon and Baillot, Yohan and Lanzagorta, Marco and Brown, Dennis and Rosenblum, Lawrence},
  year = {2000},
  pages = {8},
  abstract = {Many future military operations are expected to occur in urban environments. These complex, 3D battlefields are extremely demanding and introduce many challenges to the dismounted warfighter. These include limited visibility, lack of familiarity with the environment, sniper threats, concealment of enemy forces, ineffective communications, and a general problem of locating and identifying enemy and friendly forces. Better situational awareness is required for effective operation in the urban environment.},
  language = {en},
  annotation = {ZSCC: 0000183},
  file = {Human Computer Interaction\\Augmented Reality\\Julier et al., 2000 - BARS.pdf}
}

@article{jung2011,
  title = {Form and {{Materiality}} in {{Interaction Design}}: {{A New Approach}} to {{HCI}}},
  author = {Jung, Heekyoung and Stolterman, Erik},
  year = {2011},
  pages = {10},
  abstract = {This paper is motivated by the increasing significance of form in design and use of interactive artifacts. The objective of this paper is to conceptualize what we mean by form in the context of interaction design and HCI research and how we can approach it in regard to emerging type of digital materiality. To do this, we first examine conceptual dimensions of form in interactive artifacts through the lens of three existing perspectives with their respective focus on: material, meaning, and making. We then apply these perspectives in our analysis of specific forms of interactive artifacts. Based on this analysis, we suggest a model of four different types of forms: the cognitive, embodied, expressive, and exploratory forms. Reflecting on this model, we propose form-driven interaction design research with its epistemological and methodological implications.},
  language = {en},
  file = {Philosophy\\Materiality\\Jung and Stolterman, 2011 - Form and Materiality in Interaction Design.pdf}
}

@inproceedings{jung2011a,
  title = {Material Probe: Exploring Materiality of Digital Artifacts},
  shorttitle = {Material Probe},
  booktitle = {Proceedings of the Fifth International Conference on {{Tangible}}, Embedded, and Embodied Interaction - {{TEI}} '11},
  author = {Jung, Heekyoung and Stolterman, Erik},
  year = {2011},
  pages = {153},
  publisher = {{ACM Press}},
  address = {{Funchal, Portugal}},
  doi = {10.1145/1935701.1935731},
  abstract = {We present an approach for exploring materiality of digital artifacts by suggesting a study method\textemdash material probe. The purpose with the method is to understand how people perceive material qualities of artifacts and to discuss how designers could intentionally and methodologically include such non-functional user desires related to material qualities in the design of digital artifacts. The study procedure and results from preliminary studies are described with their implications for future work.},
  isbn = {978-1-4503-0478-8},
  language = {en},
  file = {Philosophy\\Materiality\\Jung and Stolterman, 2011 - Material probe.pdf}
}

@inproceedings{jung2012,
  title = {Digital Form and Materiality: Propositions for a New Approach to Interaction Design Research},
  shorttitle = {Digital Form and Materiality},
  booktitle = {Proceedings of the 7th {{Nordic Conference}} on {{Human}}-{{Computer Interaction Making Sense Through Design}} - {{NordiCHI}} '12},
  author = {Jung, Heekyoung and Stolterman, Erik},
  year = {2012},
  pages = {645},
  publisher = {{ACM Press}},
  address = {{Copenhagen, Denmark}},
  doi = {10.1145/2399016.2399115},
  abstract = {Advanced information and interaction technology pervades everyday life, introducing new forms and meanings of computer applications beyond desktop computers\textemdash from varying types of digital devices to interactive fashion and architecture. Motivated by the notion of digital technology as a material for interaction design, this research aims to develop a theoretical foundation to create and critique digital artifacts in the context of interaction design and HCI research. Specifically we conceptualize digital form and materiality as two reciprocal aspects of digital artifact based on the perspectives from relevant disciplines including design, arts, craft, material culture and philosophy of technology. The conceptualization emphasizes the process of making, personal meanings, and socio-cultural values of digital artifacts, constructing a new theoretical framework for exploratory and critical research approaches. In the end we discuss a proposal for form-driven interaction design research as a new approach to HCI with its focus on form and materiality aspects of digital artifacts based on the reflection on our theoretical propositions.},
  isbn = {978-1-4503-1482-4},
  language = {en},
  file = {Philosophy\\Materiality\\Jung and Stolterman, 2012 - Digital form and materiality.pdf}
}

@inproceedings{kalkusch2002,
  title = {Structured Visual Markers for Indoor Pathfinding},
  booktitle = {The {{First IEEE International Workshop Agumented Reality Toolkit}},},
  author = {Kalkusch, M. and Lidy, T. and Knapp, N. and Reitmayr, G. and Kaufmann, H. and Schmalstieg, D.},
  year = {2002},
  pages = {8},
  publisher = {{IEEE}},
  address = {{Darmstadt, Germany}},
  doi = {10.1109/ART.2002.1107018},
  abstract = {We present a mobile augmented reality (AR) system to guide a user through an unfamiliar building to a destination room. The system presents a world-registered wire frame model of the building labeled with directional information in a see-through heads-up display, and a three-dimensional world-in-miniature (WIM) map on a wrist-worn pad that also acts as an input device. Tracking is done using a combination of wall-mounted ARToolkit markers observed by a head-mounted camera, and an inertial tracker. To allow coverage of arbitrarily large areas with a limited set of markers, a structured marker re-use scheme based on graph coloring has been developed.},
  isbn = {978-0-7803-7680-9},
  language = {en},
  annotation = {ZSCC: 0000165},
  file = {..\\..\\Zotero\\storage\\KGY2EG3A\\Kalkusch et al. - 2002 - Structured visual markers for indoor pathfinding.pdf}
}

@article{kania2006,
  title = {Making {{Tracks}}: {{The Ontology}} of {{Rock Music}}},
  shorttitle = {Making {{Tracks}}},
  author = {Kania, Andrew},
  year = {2006},
  month = sep,
  journal = {Journal of Aesthetics and Art Criticism},
  volume = {64},
  number = {4},
  pages = {401--414},
  issn = {0021-8529, 1542-6245},
  doi = {10.1111/j.1540-594X.2006.00219.x},
  language = {en},
  annotation = {ZSCC: 0000094},
  file = {Arts & Humanities\\Musicology\\Kania, 2006 - Making Tracks.pdf}
}

@inproceedings{kato1999,
  title = {Marker Tracking and {{HMD}} Calibration for a Video-Based Augmented Reality Conferencing System},
  booktitle = {Proceedings 2nd {{IEEE}} and {{ACM International Workshop}} on {{Augmented Reality}} ({{IWAR}}'99)},
  author = {Kato, H. and Billinghurst, M.},
  year = {1999},
  pages = {85--94},
  publisher = {{IEEE Comput. Soc}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1109/IWAR.1999.803809},
  abstract = {We describe an augmented reality conferencing system which uses the overlay of virtual images on the real world. Remote collaborators are represented on Virtual Monitors which can be freely positioned about a user in space. Users can collaboratively view and interact with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and accurate computer vision techniques and HMD calibration. We propose a method for tracking fiducial markers and a calibration method for optical see-through HMD based on the marker tracking.},
  isbn = {978-0-7695-0359-2},
  language = {en},
  annotation = {ZSCC: 0003437},
  file = {Human Computer Interaction\\Augmented Reality\\Kato and Billinghurst, 1999 - Marker tracking and HMD calibration for a video-based augmented reality.pdf}
}

@article{kendall2010,
  ids = {kendallSpatialPerceptionCognition2010a},
  title = {Spatial {{Perception}} and {{Cognition}} in {{Multichannel Audio}} for {{Electroacoustic Music}}},
  author = {Kendall, Gary S.},
  year = {2010},
  month = dec,
  journal = {Organised Sound},
  volume = {15},
  number = {03},
  pages = {228--238},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S1355771810000336},
  language = {en},
  annotation = {ZSCC: 0000052},
  file = {Arts & Humanities\\Computational Art\\Kendall, 2010 - Spatial Perception and Cognition in Multichannel Audio for Electroacoustic Music.pdf}
}

@article{kendall2010a,
  ids = {kendallMeaningElectroacousticMusic2010a},
  title = {Meaning in {{Electroacoustic Music}} and the {{Everyday Mind}}},
  author = {Kendall, Gary S.},
  year = {2010},
  month = apr,
  journal = {Organised Sound},
  volume = {15},
  number = {01},
  pages = {63},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S1355771809990276},
  language = {en},
  annotation = {ZSCC: 0000033},
  file = {Arts & Humanities\\Computational Art\\Kendall, 2010 - Meaning in Electroacoustic Music and the Everyday Mind.pdf}
}

@article{kendall2011,
  ids = {kendallWHYTHINGSDON2011,kendallWHYTHINGSDON2011a,kendallWHYTHINGSDON2011b},
  title = {Why {{Things Don}}'t {{Work}}: {{What}} You Need to Know about Spatial Audio},
  author = {Kendall, Gary S},
  year = {2011},
  pages = {4},
  abstract = {Composers engaged in the sonic arts have frequently found themselves attempting to use spatial audio in ways that didn't work as intended. Maybe more than any other facet of technological music, mastering spatial audio seems to involve a learning process in which one slowly discovers the things that work and those that don't. The purpose of this paper is to foster understanding of spatial audio through examples of practical problems. These problems reveal some general misconceptions about spatial hearing that explain why things go wrong. A particular lesson to be gleaned from this discussion is that there is no silver bullet for solving spatial audio problems, and every situation needs to be understood in its proper context.},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Computational Art\\Kendall, 2011 - Why Things Don't Work.pdf}
}

@inproceedings{kern2020,
  title = {The Influence of Mood Induction by Music or a Soundscape on Presence and Emotions in a Virtual Reality Park Scenario},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Kern, Angelika C. and Ellermeier, Wolfgang and Jost, Lina},
  year = {2020},
  series = {{{AM}} '20},
  pages = {233--236},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411129},
  abstract = {Music and background sound are often used in virtual realities for creating an emotional atmosphere. The present study investigates how music or an ambient soundscape influence presence, the feeling of "being there", as well as positive and negative affect. Fifty-one subjects participated, taking a stroll through a virtual park presented via a head-mounted display while they were walking on a treadmill. Sound was varied within subjects in four audio conditions: In a randomized sequence, participants experienced silence, a nature soundscape and music of positive or negative valence. In addition, time of day (daytime vs. nighttime walk) in the virtual environment was varied between subjects. Afterwards they were asked to rate their experience of presence and the positive and negative affect experienced. Results indicated that replaying any kind of sound lead to higher presence ratings compared to no sound at all, but there was no difference between playing a soundscape or music. Background music, however, tended to induce the expected emotions, though somewhat dependent on the musical pieces chosen. Further studies might evaluate whether it is possible to induce emotions through positive or negative (non-musical) soundscapes as well.},
  isbn = {978-1-4503-7563-4},
  keywords = {emotion,music,presence,soundscape,virtual reality},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Kern et al., 2020 - The influence of mood induction by music or a soundscape on presence and.pdf}
}

@article{kerruish2019,
  title = {Arranging Sensations: Smell and Taste in Augmented and Virtual Reality},
  shorttitle = {Arranging Sensations},
  author = {Kerruish, Erika},
  year = {2019},
  month = jan,
  journal = {The Senses and Society},
  volume = {14},
  number = {1},
  pages = {31--45},
  issn = {1745-8927, 1745-8935},
  doi = {10.1080/17458927.2018.1556952},
  abstract = {The development of digital taste and smell underscores the importance of cultural dimensions of bodily perception in augmented reality (AR) and virtual reality (VR) devices. This can be seen in Vocktail and Season Traveller, two digital devices incorporating taste and smell. Vocktail is an AR technology that augments the experience of drinking water, or even air, through the electrical stimulation of tastebuds and the manipulation of color and smell. Season Traveller is a VR game in which the user moves through four seasonal landscapes. It uses wind, odor, and temperature in addition to the more standard audio-visual displays. The cultural dimensions of these devices can be examined using phenomenological terms. They instigate perceptual circuits, and call on and create sedimented habits. Although VR and AR are often thought of in terms of their similitude to reality, understanding Vocktail and Season Traveller this way illustrates the world-creating dimension of multisensory devices. These technologies structure and shift thresholds of taste and smell, reworking past perceptual styles and habits to develop new perceptual experiences. In so doing, Season Traveller and Vocktail throw to the fore questions about the conditions according to which people exercise their senses in digitally dominated environments.},
  language = {en},
  annotation = {ZSCC: 0000010},
  file = {Human Computer Interaction\\Augmented Reality\\Kerruish, 2019 - Arranging sensations.pdf}
}

@phdthesis{kiefer2012,
  ids = {kieferMultiparametricInterfacesFineGrained,kieferMultiparametricInterfacesFineGraineda},
  title = {Multiparametric {{Interfaces For Fine}}-{{Grained Control}} of {{Digital Music}}},
  author = {Kiefer, Chris},
  year = {2012},
  language = {en},
  school = {University of Sussex},
  annotation = {ZSCC: 0000005},
  file = {Arts & Humanities\\Computational Art\\Kiefer, 2012 - Multiparametric Interfaces For Fine-Grained Control of Digital Music.pdf}
}

@inproceedings{kiefer2018,
  title = {Towards New Modes of Collective Musical Expression through Audio Augmented Reality},
  booktitle = {Proceedings of the {{International Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Kiefer, Chris and Chevalier, C{\'e}cile},
  year = {2018},
  month = jun,
  pages = {25--28},
  address = {{Virginia, USA}},
  doi = {10.5281/zenodo.1302661},
  abstract = {We investigate how audio augmented reality can engender new collective modes of musical expression in the context of a sound art installation, Listening Mirrors, exploring the creation of interactive sound environments for musicians and non-musicians alike. Listening Mirrors is designed to incorporate physical objects and computational systems for altering the acoustic environment, to enhance collective listening and challenge traditional musician-instrument performance. At a formative stage in exploring audio AR technology, we conducted an audience experience study investigating questions around the potential of audio AR in creating sound installation environments for collective musical expression. We collected interview evidence about the participants' experience and analysed the data with using a grounded theory approach. The results demonstrated that the technology has the potential to create immersive spaces where an audience can feel safe to experiment musically, and showed how AR can intervene in sound perception to instrumentalise an environment. The results also revealed caveats about the use of audio AR, mainly centred on social inhibition and seamlessness of experience, and finding a balance between mediated worlds to create space for interplay between the two.},
  keywords = {augmented reality,collective musical expression,mobile music making,sound art installation},
  annotation = {ZSCC: 0000004},
  file = {Human Computer Interaction\\Augmented Reality\\Kiefer and Chevalier, 2018 - Towards new modes of collective musical expression through audio augmented.pdf}
}

@article{kiefer2019,
  ids = {kieferSamplelevelSoundSynthesis2019,kieferSamplelevelSoundSynthesis2019a},
  title = {Sample-Level Sound Synthesis with Recurrent Neural Networks and Conceptors},
  author = {Kiefer, Chris},
  year = {2019},
  journal = {PeerJ Computer Science},
  volume = {5},
  pages = {e205},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.205},
  abstract = {Conceptors are a recent development in the field of reservoir computing; they can be used to influence the dynamics of recurrent neural networks (RNNs), enabling generation of arbitrary patterns based on training data. Conceptors allow interpolation and extrapolation between patterns, and also provide a system of boolean logic for combining patterns together. Generation and manipulation of arbitrary patterns using conceptors has significant potential as a sound synthesis method for applications in computer music but has yet to be explored. Conceptors are untested with the generation of multi-timbre audio patterns, and little testing has been done on scalability to longer patterns required for audio. A novel method of sound synthesis based on conceptors is introduced. Conceptular Synthesis is based on granular synthesis; sets of conceptors are trained to recall varying patterns from a single RNN, then a runtime mechanism switches between them, generating short patterns which are recombined into a longer sound. The quality of sound resynthesis using this technique is experimentally evaluated. Conceptor models are shown to resynthesise audio with a comparable quality to a close equivalent technique using echo state networks with stored patterns and output feedback. Conceptor models are also shown to excel in their malleability and potential for creative sound manipulation, in comparison to echo state network models which tend to fail when the same manipulations are applied. Examples are given demonstrating creative sonic possibilities, by exploiting conceptor pattern morphing, boolean conceptor logic and manipulation of RNN dynamics.},
  language = {en},
  annotation = {ZSCC: 0000001},
  file = {Arts & Humanities\\Computational Art\\Kiefer, 2019 - Sample-level sound synthesis with recurrent neural networks and conceptors.pdf}
}

@inproceedings{kiefer2020,
  title = {Shaping the Behaviour of Feedback Instruments with Complexity-Controlled Gain Dynamics},
  booktitle = {New {{Interfaces}} for {{Musical Expression}}},
  author = {Kiefer, Chris and Overholt, Dan and Eldridge, Alice},
  year = {2020},
  pages = {6},
  publisher = {{New Interfaces for Musical Expression}},
  address = {{Birmingham, UK}},
  abstract = {Feedback instruments offer radical new ways of engaging with instrument design and musicianship. They are defined by recurrent circulation of signals through the instrument, which give the instrument `a life of its own' and a 'stimulating uncontrollability'. Arguably, the most interesting musical behaviour in these instruments happens when their dynamic complexity is maximised, without falling into saturating feedback. It is often challenging to keep the instrument in this zone; this research looks at algorithmic ways to manage the behaviour of feedback loops in order to make feedback instruments more playable and musical; to expand the `sweet spot'. We propose a solution that manages gain dynamics based on measurement of complexity, using a realtime implementation of the Effort to Compress algorithm. The system was evaluated with four musicians, all who have different variations of string-based feedback instruments, following an autobiographical design approach. Qualitative feedback was gathered, showing that the system was successful in modifying the behaviour of these instruments to allow easier access to edge transition zones, sometimes at the expense of losing some of the more compelling dynamics of the instruments. Basic efficacy of the system is evidenced by descriptive audio analysis. This paper is accompanied by a dataset of sounds collected during the study, and open source software that was written to support the research.},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Kiefer et al., 2020 - Shaping the behaviour of feedback instruments with complexity-controlled gain.pdf}
}

@article{kim2018,
  ids = {kimRevisitingTrendsAugmented2018},
  title = {Revisiting {{Trends}} in {{Augmented Reality Research}}: {{A Review}} of the 2nd {{Decade}} of {{ISMAR}} (2008\textendash 2017)},
  shorttitle = {Revisiting {{Trends}} in {{Augmented Reality Research}}},
  author = {Kim, Kangsoo and Billinghurst, Mark and Bruder, Gerd and Duh, Henry Been-Lirn and Welch, Gregory F.},
  year = {2018},
  month = nov,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {24},
  number = {11},
  pages = {2947--2962},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2018.2868591},
  abstract = {In 2008, Zhou et al. presented a survey paper summarizing the previous ten years of ISMAR publications, which provided invaluable insights into the research challenges and trends associated with that time period. Ten years later, we review the research that has been presented at ISMAR conferences since the survey of Zhou et al., at a time when both academia and the AR industry are enjoying dramatic technological changes. Here we consider the research results and trends of the last decade of ISMAR by carefully reviewing the ISMAR publications from the period of 2008\textendash 2017, in the context of the first ten years. The numbers of papers for different research topics and their impacts by citations were analyzed while reviewing them\textemdash which reveals that there is a sharp increase in AR evaluation and rendering research. Based on this review we offer some observations related to potential future research areas or trends, which could be helpful to AR researchers and industry members looking ahead.},
  language = {en},
  annotation = {ZSCC: 0000074},
  file = {Human Computer Interaction\\Augmented Reality\\Kim et al., 2018 - Revisiting Trends in Augmented Reality Research.pdf}
}

@article{kirchhoff2015,
  title = {Extended {{Cognition}} \& the {{Causal}}-{{Constitutive Fallacy}}: {{In Search}} for a {{Diachronic}} and {{Dynamical Conception}} of {{Constitution}}},
  shorttitle = {Extended {{Cognition}} \& the {{Causal}}-{{Constitutive Fallacy}}},
  author = {Kirchhoff, Michael D.},
  year = {2015},
  month = mar,
  journal = {Philosophy and Phenomenological Research},
  volume = {90},
  number = {2},
  pages = {320--360},
  issn = {00318205},
  doi = {10.1111/phpr.12039},
  language = {en},
  file = {Philosophy\\Extended Cognition\\Kirchhoff, 2015 - Extended Cognition & the Causal-Constitutive Fallacy.pdf}
}

@article{kirchhoff2020,
  title = {Attuning to the {{World}}: {{The Diachronic Constitution}} of the {{Extended Conscious Mind}}},
  shorttitle = {Attuning to the {{World}}},
  author = {Kirchhoff, Michael D. and Kiverstein, Julian},
  year = {2020},
  month = aug,
  journal = {Frontiers in Psychology},
  volume = {11},
  pages = {1966},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2020.01966},
  abstract = {It is a near consensus among materialist philosophers of mind that consciousness must somehow be constituted by internal neural processes, even if we remain unsure quite how this works. Even friends of the extended mind theory have argued that when it comes to the material substrate of conscious experience, the boundary of skin and skull is likely to prove somehow to be privileged. Such arguments have, however, typically conceived of the constitution of consciousness in synchronic terms, making a firm separation between proximate mechanisms and their ultimate causes. We argue that the processes involved in the constitution of some conscious experiences are diachronic, not synchronic. We focus on what we call phenomenal attunement in this paper\textemdash the feeling of being at home in a familiar, culturally constructed environment. Such a feeling is missing in cases of culture shock. Phenomenal attunement is a structure of our conscious experience of the world that is ubiquitous and taken for granted. We will argue that it is constituted by cycles of embodied and world-involving engagement whose dynamics are constrained by cultural practices. Thus, it follows that an essential structure of the conscious mind, the absence of which profoundly transforms conscious experience, is extended.},
  language = {en},
  file = {Philosophy\\Extended Cognition\\Kirchhoff and Kiverstein, 2020 - Attuning to the World.pdf}
}

@article{klopfer2008,
  title = {Environmental {{Detectives}}\textemdash the Development of an Augmented Reality Platform for Environmental Simulations},
  author = {Klopfer, Eric and Squire, Kurt},
  year = {2008},
  month = apr,
  journal = {Educational Technology Research and Development},
  volume = {56},
  number = {2},
  pages = {203--228},
  issn = {1042-1629, 1556-6501},
  doi = {10.1007/s11423-007-9037-6},
  language = {en},
  annotation = {ZSCC: 0000853},
  file = {Human Computer Interaction\\Augmented Reality\\Klopfer and Squire, 2008 - Environmental Detectives—the development of an augmented reality platform for.pdf}
}

@article{knight2016,
  title = {A {{Companion}} to {{Public Art}}},
  author = {Knight, Cher Krause},
  year = {2016},
  pages = {514},
  language = {en},
  file = {Arts & Humanities\\Computational Art\\Knight, 2016 - A Companion to Public Art.pdf}
}

@article{krueger1991,
  title = {Artificial Reality: {{Past}} and Future},
  shorttitle = {Artificial Reality},
  author = {Krueger, Myron W.},
  year = {1991},
  journal = {Virtual Reality: Theory, Practi\cyrchar\cyrs e and Promise/Ed. SK Helsel, JP Roth.\textendash Westport, London: Meckler},
  pages = {19--26}
}

@book{kwastek2013a,
  title = {Aesthetics of {{Interaction}} in {{Digital Art}}},
  author = {Kwastek, Katja},
  year = {2013},
  publisher = {{MIT Press}},
  isbn = {0-262-31722-2}
}

@book{landy2007,
  title = {Understanding the Art of Sound Organization},
  author = {Landy, Leigh},
  year = {2007},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-12292-4},
  language = {en},
  lccn = {ML1380 .L28 2007},
  keywords = {Computer music,Electro-acoustics,Electronic music,History and criticism},
  file = {Arts & Humanities\\Computational Art\\Landy, 2007 - Understanding the art of sound organization.pdf}
}

@book{latour2003,
  title = {Science in Action: How to Follow Scientists and Engineers through Society},
  shorttitle = {Science in Action},
  author = {Latour, Bruno},
  year = {2003},
  edition = {11. print},
  publisher = {{Harvard Univ. Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-674-79291-3},
  language = {eng},
  annotation = {OCLC: 254704565}
}

@book{latour2007,
  title = {Reassembling the Social: An Introduction to {{Actor}}-{{Network}}-{{Theory}}},
  shorttitle = {Reassembling the Social},
  author = {Latour, Bruno},
  year = {2007},
  series = {Clarendon Lectures in Management Studies},
  publisher = {{Oxford Univ. Press}},
  address = {{Oxford}},
  isbn = {978-0-19-925605-1 978-0-19-925604-4},
  language = {eng},
  annotation = {OCLC: 254570772}
}

@inproceedings{lawton2020,
  title = {Nature Soundscapes: {{An}} Audio Augmented Reality Experience},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Lawton, Mark and Cunningham, Stuart and Convery, Ian},
  year = {2020},
  series = {{{AM}} '20},
  pages = {85--92},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411142},
  abstract = {Augmented Reality (AR) has developed to be a popular and exciting technology domain, gaining notable public interest from 2009 to the present day. AR applications have traditionally focused upon paradigms that are visually led. In this paper, we document an Audio Augmented Reality (AAR) project, which considers soundscapes and how they might be transformed via the application of music and sound technologies. This work is concerned with the augmentation of nature soundscapes and explores how this may be used to enhance public understanding of the natural world. At present, we are concerned with the augmentation of spaces with biophony. Two examples of acoustic augmented reality are described: an initial pilot study to investigate the feasibility of the approach and an installation at the Timber International Forest Festival 2019. A technical description of each is provided alongside our own reflection and participant feedback, garnered from a soundwalk inspired approach to evaluation by audiences at the festival.},
  isbn = {978-1-4503-7563-4},
  keywords = {augmented reality,nature,soundscapes},
  annotation = {ZSCC: 0000000},
  file = {Human Computer Interaction\\Augmented Reality\\Lawton et al., 2020 - Nature soundscapes.pdf}
}

@misc{leapmotion2015,
  title = {Touch {{Everything}} \textendash{} {{Leap Motion Gallery}}},
  author = {Leap Motion},
  year = {2015},
  urldate = {2020-07-22},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {..\\..\\Zotero\\storage\\5VZLZEDL\\touch-everything.html},
  url = {https://gallery.leapmotion.com/touch-everything/}
}

@misc{leapmotion2016,
  title = {{{HTC Vive Setup}}},
  author = {Leap Motion},
  year = {2016},
  journal = {Leap Motion Developer},
  urldate = {2020-07-22},
  abstract = {The VR Developer Kit is available only from the  Leap Motion web store . This setup guide will get you started in minutes.   \_},
  language = {en-US},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {..\\..\\Zotero\\storage\\JIZAGSF7\\vive.html},
  url = {https://developer.leapmotion.com/vr-setup/vive}
}

@misc{leapmotion2017,
  title = {Geco {{MIDI}} \textendash{} {{Leap Motion Gallery}}},
  author = {Leap Motion},
  year = {2017},
  urldate = {2020-07-22},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {..\\..\\Zotero\\storage\\KKLCMS4S\\geco-midi.html},
  url = {https://gallery.leapmotion.com/geco-midi/}
}

@misc{leapmotion2018,
  title = {Project {{North Star}}},
  author = {Leap Motion},
  year = {2018},
  journal = {Leap Motion Developer},
  urldate = {2020-05-25},
  language = {en-US},
  file = {..\\..\\Zotero\\storage\\7LB5G435\\northstar.html},
  url = {https://developer.leapmotion.com/northstar}
}

@book{lefebvre1991,
  title = {The Production of Space},
  author = {Lefebvre, Henri},
  year = {1991},
  publisher = {{Blackwell}},
  address = {{Oxford, OX, UK ; Cambridge, Mass., USA}},
  isbn = {978-0-631-14048-1},
  language = {eng},
  lccn = {BD621 .L4813 1991},
  keywords = {Space and time},
  annotation = {ZSCC: 0034752},
  file = {Philosophy\\Space\\Lefebvre, 1991 - The production of space.pdf}
}

@article{lidji2007,
  title = {Spatial Associations for Musical Stimuli: {{A}} Piano in the Head?},
  shorttitle = {Spatial Associations for Musical Stimuli},
  author = {Lidji, Pascale and Kolinsky, R{\'e}gine and Lochy, Aliette and Morais, Jos{\'e}},
  year = {2007},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {33},
  number = {5},
  pages = {1189--1207},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.33.5.1189},
  abstract = {This study was aimed at examining whether pitch height and pitch change are mentally represented along spatial axes. A series of experiments explored, for isolated tones and 2-note intervals, the occurrence of effects analogous to the spatial numerical association of response codes (SNARC) effect. Response device orientation (horizontal vs. vertical), task, and musical expertise of the participants were manipulated. The pitch of isolated tones triggered the automatic activation of a vertical axis independently of musical expertise, but the contour of melodic intervals did not. By contrast, automatic associations with the horizontal axis seemed linked to music training for pitch and, to a lower extent, for intervals. These results, discussed in the light of studies on number representation, provide a new example of the effects of musical expertise on music cognition.},
  language = {en},
  file = {Cognitive Science\\Psychophysics\\Lidji et al., 2007 - Spatial associations for musical stimuli.pdf}
}

@misc{lightform2020,
  title = {Lightform: {{Design Tools}} for {{Projection}}},
  shorttitle = {Lightform},
  author = {{Lightform}},
  year = {2020},
  journal = {Lightform},
  urldate = {2020-10-03},
  abstract = {Introducing the next generation of Lightform. LF2 is the first AR projector, and provides everything you need to make magic with light. LFC Kit can be used to go big with your own projector. It's like LF1, but more flexible and faster.},
  language = {en-US},
  file = {..\\..\\Zotero\\storage\\MAAJJZXK\\lightform.com.html},
  url = {https://lightform.com/}
}

@inproceedings{lin2020,
  title = {{{ARchitect}}: {{Building Interactive Virtual Experiences}} from {{Physical Affordances}} by {{Bringing Human}}-in-the-{{Loop}}},
  shorttitle = {{{ARchitect}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lin, Chuan-en and Cheng, Ta Ying and Ma, Xiaojuan},
  year = {2020},
  month = apr,
  pages = {1--13},
  publisher = {{ACM}},
  address = {{Honolulu HI USA}},
  doi = {10.1145/3313831.3376614},
  abstract = {Automatic generation of Virtual Reality (VR) worlds which adapt to physical environments have been proposed to enable safe walking in VR. However, such techniques mainly focus on the avoidance of physical objects as obstacles and overlook their interaction affordances as passive haptics. Current VR experiences involving interaction with physical objects in surroundings still require verbal instruction from an assisting partner. We present ARchitect, a proof-of-concept prototype that allows flexible customization of a VR experience with human-in-the-loop. ARchitect brings in an assistant to map physical objects to virtual proxies of matching affordances using Augmented Reality (AR). In a within-subjects study (9 user pairs) comparing ARchitect to a baseline condition, assistants and players experienced decreased workload and players showed increased VR presence and trust in the assistant. Finally, we defined design guidelines of ARchitect for future designers and implemented three demonstrative experiences.},
  isbn = {978-1-4503-6708-0},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Human Computer Interaction\\Augmented Reality\\Lin et al., 2020 - ARchitect.pdf}
}

@inproceedings{lindeman2007,
  title = {A Classification Scheme for Multi-Sensory Augmented Reality},
  booktitle = {Proceedings of the 2007 {{ACM}} Symposium on {{Virtual}} Reality Software and Technology  - {{VRST}} '07},
  author = {Lindeman, Robert W. and Noma, Haruo},
  year = {2007},
  pages = {175},
  publisher = {{ACM Press}},
  address = {{Newport Beach, California}},
  doi = {10.1145/1315184.1315216},
  abstract = {We present a new classification framework for describing augmented reality (AR) applications based on where the mixing of real and computer-generated stimuli takes place. In addition to "classical" visual AR techniques, such as optical-see-through and video-see-through AR, our framework encompasses AR directed at the other senses as well. This "axis of mixing location" is a continuum ranging from the physical environment to the human brain. There are advantages and disadvantages of mixing at different points along the continuum, and while there is no "best" location, we present sample usage scenarios that illustrate the expressiveness of this classification approach.},
  isbn = {978-1-59593-863-3},
  language = {en},
  annotation = {ZSCC: 0000031},
  file = {Human Computer Interaction\\Augmented Reality\\Lindeman and Noma, 2007 - A classification scheme for multi-sensory augmented reality.pdf}
}

@inproceedings{lindeman2008,
  title = {An {{Empirical Study}} of {{Hear}}-{{Through Augmented Reality}}: {{Using Bone Conduction}} to {{Deliver Spatialized Audio}}},
  shorttitle = {An {{Empirical Study}} of {{Hear}}-{{Through Augmented Reality}}},
  booktitle = {2008 {{IEEE Virtual Reality Conference}}},
  author = {Lindeman, Robert W. and Noma, Haruo and {de Barros}, Paulo Goncalves},
  year = {2008},
  pages = {35--42},
  publisher = {{IEEE}},
  address = {{Reno, NV, USA}},
  doi = {10.1109/VR.2008.4480747},
  abstract = {Augmented reality (AR) is the mixing of computer-generated stimuli with real-world stimuli. In this paper, we present results from a controlled, empirical study comparing three ways of delivering spatialized audio for AR applications: a speaker array, headphones, and a bone-conduction headset. Analogous to optical-see-through AR in the visual domain, Hear-Through AR allows users to receive computer-generated audio using the bone-conduction headset, and real-world audio using their unoccluded ears. Our results show that subjects achieved the best accuracy using a speaker array physically located around the listener when stationary sounds were played, but that there was no difference in accuracy between the speaker array and the bone-conduction device for sounds that were moving, and that both devices outperformed standard headphones for moving sounds. Subjective comments by subjects following the experiment support this performance data.},
  isbn = {978-1-4244-1971-5},
  language = {en},
  file = {Human Computer Interaction\\Augmented Reality\\Lindeman et al., 2008 - An Empirical Study of Hear-Through Augmented Reality.pdf}
}

@article{litovsky1998,
  title = {The {{Precedence Effect}}},
  author = {Litovsky, Ruth Y and Colburn, H Steven and Yost, William A and Guzman, Sandra J},
  year = {1998},
  pages = {23},
  doi = {10.1121/1.427914},
  language = {en},
  annotation = {ZSCC: NoCitationData[s3]},
  file = {Cognitive Science\\Psychophysics\\Litovsky et al., 1998 - The Precedence Effect.pdf}
}

@inproceedings{lopes2017,
  title = {Providing {{Haptics}} to {{Walls}} \& {{Heavy Objects}} in {{Virtual Reality}} by {{Means}} of {{Electrical Muscle Stimulation}}},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lopes, Pedro and You, Sijing and Cheng, Lung-Pan and Marwecki, Sebastian and Baudisch, Patrick},
  year = {2017},
  month = may,
  pages = {1471--1482},
  publisher = {{ACM}},
  address = {{Denver Colorado USA}},
  doi = {10.1145/3025453.3025600},
  abstract = {We explore how to add haptics to walls and other heavy objects in virtual reality. When a user tries to push such an object, our system actuates the user's shoulder, arm, and wrist muscles by means of electrical muscle stimulation, creating a counter force that pulls the user's arm backwards. Our device accomplishes this in a wearable form factor.},
  isbn = {978-1-4503-4655-9},
  language = {en},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Lopes et al., 2017 - Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of.pdf}
}

@inproceedings{lopes2018,
  title = {Adding {{Force Feedback}} to {{Mixed Reality Experiences}} and {{Games}} Using {{Electrical Muscle Stimulation}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}  - {{CHI}} '18},
  author = {Lopes, Pedro and You, Sijing and Ion, Alexandra and Baudisch, Patrick},
  year = {2018},
  pages = {1--13},
  publisher = {{ACM Press}},
  address = {{Montreal QC, Canada}},
  doi = {10.1145/3173574.3174020},
  abstract = {We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered\textemdash not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMSconstraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMSdetents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline.},
  isbn = {978-1-4503-5620-6},
  language = {en},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Lopes et al., 2018 - Adding Force Feedback to Mixed Reality Experiences and Games using Electrical.pdf}
}

@inproceedings{lucero2019,
  title = {A {{Sample}} of {{One}}: {{First}}-{{Person Research Methods}} in {{HCI}}},
  shorttitle = {A {{Sample}} of {{One}}},
  booktitle = {Companion {{Publication}} of the 2019 on {{Designing Interactive Systems Conference}} 2019 {{Companion}} - {{DIS}} '19 {{Companion}}},
  author = {Lucero, Andr{\'e}s and Desjardins, Audrey and Neustaedter, Carman and H{\"o}{\"o}k, Kristina and Hassenzahl, Marc and Cecchinato, Marta E.},
  year = {2019},
  pages = {385--388},
  publisher = {{ACM Press}},
  address = {{San Diego, CA, USA}},
  doi = {10.1145/3301019.3319996},
  abstract = {First-person research (i.e., research that involves data collection and experiences from the researcher themselves) continues to become a viable addition and, possibly even, alternative to more traditional HCI methods. While we have seen the benefits of using methods such as autoethnography, autobiographical design, and autoethnographical research through design, we also see the need to further explore, define, and investigate the practices, techniques, tactics, and implications of first-person research in HCI. To address this, this one-day workshop aims to bring together a community of researchers, designers, and practitioners who are interested in exploring and reimagining research in HCI and interaction design, with an emphasis on first-person methods.},
  isbn = {978-1-4503-6270-2},
  language = {en},
  annotation = {ZSCC: 0000004},
  file = {Research Methods\\Autobiographical Design\\Lucero et al., 2019 - A Sample of One.pdf}
}

@inproceedings{macintyre2001,
  title = {Augmented Reality as a New Media Experience},
  booktitle = {Proceedings {{IEEE}} and {{ACM International Symposium}} on {{Augmented Reality}}},
  author = {MacIntyre, B. and Bolter, J.D. and Moreno, E. and Hannigan, B.},
  year = {2001},
  pages = {197--206},
  publisher = {{IEEE Comput. Soc}},
  address = {{New York, NY, USA}},
  doi = {10.1109/ISAR.2001.970538},
  abstract = {In this paper we discuss our work on applying media theory to the creation of narrative augmented reality (AR) experiences. We summarize the concepts of remediation and media forms as they relate to our work, argue for their importance to the development of a new medium such as AR, and present two example AR experiences we have designed using these conceptual tools. In particular, we focus on leveraging the interaction between the physical and virtual world, remediating existing media (film, stage and interactive CD-ROM), and building on the cultural expectations of our users.},
  isbn = {978-0-7695-1375-1},
  language = {en},
  annotation = {ZSCC: 0000090},
  file = {Human Computer Interaction\\Augmented Reality\\MacIntyre et al., 2001 - Augmented reality as a new media experience.pdf}
}

@article{mackay1996,
  title = {Augmenting {{Reality}}: {{A}} New Paradigm for Interacting with Computers},
  author = {Mackay, Wendy E},
  year = {1996},
  pages = {9},
  abstract = {A revolution in computer interface design is changing the way we think about computers. Rather than typing on a keyboard and watching a television monitor, Augmented Reality lets people use familiar, everyday objects in ordinary ways. The difference is that these objects also provide a link into a computer network. Doctors can examine patients while viewing superimposed medical images; children can program their own LEGO constructions; and construction engineers can use ordinary paper engineering drawings to make live video connections to colleagues far away. Rather than immersing people in an artifically-created virtual world, the goal is to augment everyday objects in the physical world by enhancing them with a wealth of digital information and communication capabilities.},
  language = {en},
  annotation = {ZSCC: 0000029},
  file = {Human Computer Interaction\\Augmented Reality\\Mackay, 1996 - Augmenting Reality.pdf}
}

@book{madinger2000,
  ids = {madingerEightArmsHold2000},
  title = {Eight Arms to Hold You: The Solo {{Beatles}} Compendium},
  shorttitle = {Eight Arms to Hold You},
  author = {Madinger, Chip and Easter, Mark},
  year = {2000},
  publisher = {{44.1 Productions}},
  address = {{Chesterfield, Missouri}},
  isbn = {978-0-615-11724-9},
  language = {English},
  annotation = {OCLC: 844028930}
}

@inproceedings{maggioni2017,
  title = {Measuring the Added Value of Haptic Feedback},
  booktitle = {2017 {{Ninth International Conference}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  author = {Maggioni, Emanuela and Agostinelli, Erika and Obrist, Marianna},
  year = {2017},
  month = may,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Erfurt, Germany}},
  doi = {10.1109/QoMEX.2017.7965670},
  abstract = {While there is an increased appreciation for integrating haptic feedback with audio-visual content, there is still a lack of understanding of how to quantify the added value of touch for a user's experience (UX) of multimedia content. Here we focus on three main concepts to measure this added value: UX, emotions, and expectations. We present a case study measuring the added value of haptic feedback for a standardized set of audio-visual content (i.e., short video clips), comparing two haptic stimulation modalities (i.e., mid-air vs. vibro-tactile stimuli). Our findings demonstrate that UX of hapticallyenhanced audio-visual content is perceived as a more pleasant, unpredictable, and creative experience. The users' overall liking increases together with a positive change of the users' expectations, independently from the haptic stimulation modality. We discuss how our approach provides the foundation for future work on developing a measurement model to predict the added value of haptic feedback for users' experiences within and beyond the multimedia context.},
  isbn = {978-1-5386-4024-1},
  language = {en},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Maggioni et al., 2017 - Measuring the added value of haptic feedback.pdf}
}

@article{maggioni2019,
  title = {{{OWidgets}}: {{A}} Toolkit to Enable Smell-Based Experience Design},
  shorttitle = {{{OWidgets}}},
  author = {Maggioni, Emanuela and Cobden, Robert and Obrist, Marianna},
  year = {2019},
  month = oct,
  journal = {International Journal of Human-Computer Studies},
  volume = {130},
  pages = {248--260},
  issn = {10715819},
  doi = {10.1016/j.ijhcs.2019.06.014},
  abstract = {Interactive technologies are transforming the ways in which people experience, interact and share information. Advances in technology have made it possible to generate real and virtual environments with breath-taking graphics and high-fidelity audio. However, without stimulating the other senses such as touch and smell, and even taste in some cases, such experiences feel hollow and fictitious; they lack realism. One of the main stumbling blocks for progress towards creating truly compelling multisensory experiences is the lack of appropriate tools and guidance for designing beyond audio-visual applications. Here we focus particularly on the sense of smell and how smell-based design can be enabled to create novel user experiences. We present a design toolkit for smell (i.e., OWidgets). The toolkit consists of a graphical user interface and the underlying software framework. The framework uses two main components: a Mapper and Scheduler facilitating the device-independent replication of olfactory experiences. We discuss how our toolkit reduces the complexity of designing with smell and enables a creative exploration based on specific design features. We conclude by reflecting on future directions to extend the toolkit and integrate it into the wider audio-visual ecosystem.},
  language = {en},
  annotation = {ZSCC: 0000002},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Maggioni et al., 2019 - OWidgets.pdf}
}

@misc{magicleap2018,
  title = {Magic {{Leap}} 1},
  author = {Magic Leap},
  year = {2018},
  urldate = {2020-05-25},
  abstract = {Step aside VR and smartphone AR. Magic Leap 1 is a wearable spatial computer that brings the physical and digital worlds together as one.},
  language = {en-us},
  file = {..\\..\\Zotero\\storage\\56CBS9SS\\magic-leap-1.html},
  url = {https://www.magicleap.com/magic-leap-1}
}

@article{magnusson2009,
  title = {Of {{Epistemic Tools}}: Musical Instruments as Cognitive Extensions},
  shorttitle = {Of {{Epistemic Tools}}},
  author = {Magnusson, Thor},
  year = {2009},
  month = aug,
  journal = {Organised Sound},
  volume = {14},
  number = {2},
  pages = {168--176},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S1355771809000272},
  abstract = {This paper explores the differences in the design and performance of acoustic and new digital musical instruments, arguing that with the latter there is an increased encapsulation of musical theory. The point of departure is the phenomenology of musical instruments, which leads to the exploration of designed artefacts as extensions of human cognition \textendash{} as scaffolding onto which we delegate parts of our cognitive processes. The paper succinctly emphasises the pronounced epistemic dimension of digital instruments when compared to acoustic instruments. Through the analysis of material epistemologies it is possible to describe the digital instrument as an               epistemic tool               : a designed tool with such a high degree of symbolic pertinence that it becomes a system of knowledge and thinking in its own terms. In conclusion, the paper rounds up the phenomenological and epistemological arguments, and points at issues in the design of digital musical instruments that are germane due to their strong aesthetic implications for musical culture.},
  language = {en},
  file = {Arts & Humanities\\Computational Art\\Magnusson, 2009 - Of Epistemic Tools.pdf}
}

@phdthesis{magnusson2009a,
  title = {Epistemic {{Tools}}: {{The Phenomenology}} of {{Digital Musical Instruments}}},
  author = {Magnusson, Thor},
  year = {2009},
  url = {https://sro.sussex.ac.uk/id/eprint/83540/1/Magnusson%2C%20Thor%282%29.pdf},
  urldate = {2020-12-15},
  school = {University of Sussex},
  file = {Arts & Humanities\\Computational Art\\Magnusson, 2009 - Epistemic Tools.pdf}
}

@book{magnusson2019,
  title = {Sonic Writing: Technologies of Material, Symbolic and Signal Inscriptions},
  shorttitle = {Sonic Writing},
  author = {Magnusson, Thor},
  year = {2019},
  publisher = {{Bloomsbury Academic}},
  address = {{New York, NY}},
  isbn = {978-1-5013-1385-1 978-1-5013-1386-8 978-1-5013-1389-9},
  lccn = {ML3800 .M23776 2019},
  keywords = {Music,Musical instruments,Musical notation,Philosophy and aesthetics,Recording and reproducing Philosophy,Sound}
}

@article{magnusson2021,
  title = {The Migration of Musical Instruments: {{On}} the Socio-Technological Conditions of Musical Evolution},
  shorttitle = {The Migration of Musical Instruments},
  author = {Magnusson, Thor},
  year = {2021},
  month = mar,
  journal = {Journal of New Music Research},
  volume = {50},
  number = {2},
  pages = {175--183},
  issn = {0929-8215, 1744-5027},
  doi = {10.1080/09298215.2021.1907420},
  abstract = {Music technologies reflect the most advanced human technologies in most historical periods. Examples range from 40 thousand years old bone flutes found in caves in the Swabian Jura, through ancient Greek water organs or medieval Arabic musical automata, to today's electronic and digital instruments with deep learning. Music technologies incorporate the musical ideas of a time and place and they disseminate those ideas when adopted by other musical cultures. This article explores how contemporary music technologies are culturally conditioned and applies the concept of ethno-organology to describe the nature of migration of instruments between musical cultures.},
  language = {en},
  file = {Arts & Humanities\\Computational Art\\Magnusson, 2021 - The migration of musical instruments.pdf}
}

@techreport{mann1994,
  title = {Mediated Reality},
  author = {Mann, Steve},
  year = {1994},
  number = {MIT-ML Percom TR-260},
  pages = {21},
  institution = {{University of Toronto}},
  url = {https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=F4FD2D3356DBAD357773B5939176DB12?doi=10.1.1.48.5056&rep=rep1&type=pdf},
  urldate = {2021-02-10},
  annotation = {ZSCC: 0000103},
  file = {Human Computer Interaction\\Augmented Reality\\Mann, 1994 - Mediated reality.pdf}
}

@article{mann2018,
  title = {All {{Reality}}: {{Values}}, Taxonomy, and Continuum, for {{Virtual}}, {{Augmented}}, {{eXtended}}/{{MiXed}} ({{X}}), {{Mediated}} ({{X}},{{Y}}), and {{Multimediated Reality}}/{{Intelligence}}},
  author = {Mann, Steve and Havens, John C and Iorio, Jay and Yuan, Yu and Furness, Tom},
  year = {2018},
  pages = {10},
  abstract = {Humans are creating a world of eXtended/Artificial Reality/Intelligence (AR, AI, XR, XI or EI), that in many ways is hypocritical, e.g. where cars and buildings are always ``allowed'' to ``wear'' cameras, but humans sometimes aren't, and where machines sense our every movement, yet we can't even understand how they work. We're constructing a system of values that gives more rights and less responsibilities to AI (Artificial Intelligence) than to HI (Humanistic Intelligence). Whereas it is becoming common to separate the notions of IRL (In Real Life) and ``Augmented'' or ``Virtual'' Reality (AR, VR) into completely disparate realms with clearly delineated boundaries, we propose here the notion of ``All Reality'' to more holistically represent the links between these soon-to-be-outdated culturally accepted norms of various levels of consciousness. Inclusive in the notion of ``All Reality'' is also the idea of ``ethically aligned reality'', recognizing values-based biases, cultural norms, and applied ethics of the creators of technology.},
  language = {en},
  annotation = {ZSCC: 0000013},
  file = {Human Computer Interaction\\Augmented Reality\\Mann et al., 2018 - All Reality.pdf}
}

@inproceedings{mann2018a,
  title = {Phenomenological {{Augmented Reality}} with the {{Sequential Wave Imprinting Machine}} ({{SWIM}})},
  booktitle = {2018 {{IEEE Games}}, {{Entertainment}}, {{Media Conference}} ({{GEM}})},
  author = {Mann, Steve},
  year = {2018},
  pages = {1--9},
  publisher = {{IEEE}},
  address = {{Galway}},
  doi = {10.1109/GEM.2018.8516502},
  abstract = {SWIM (Sequential Wave Imprinting Machine) is an invention that makes for visual art as well as scientific discovery of otherwise invisible physical phenomenology around us, such as sound waves, radio waves, etc.. It uses multimediated reality (sensing, computation, and display) to turn phenomena such as interference patterns between multiple sound sources, into pictures ``painted'' by nature itself (rather than from computer graphics). This gives us a glimpse into the nature of the real world arouond us, i.e. phenomena arising from physics (natural philosophy).},
  isbn = {978-1-5386-6304-2},
  language = {en},
  annotation = {ZSCC: 0000012},
  file = {Human Computer Interaction\\Augmented Reality\\Mann, 2018 - Phenomenological Augmented Reality with the Sequential Wave Imprinting Machine.pdf}
}

@book{manovich2001,
  title = {The Language of New Media},
  author = {Manovich, Lev},
  year = {2001},
  series = {Leonardo},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  isbn = {978-0-262-63255-3 978-0-262-13374-6},
  language = {eng},
  annotation = {ZSCC: 0013068  OCLC: 231923976},
  file = {Arts & Humanities\\Media Studies\\Manovich, 2001 - The language of new media.pdf}
}

@article{manovich2006,
  ids = {manovichPoeticsAugmentedSpace2006},
  title = {The Poetics of Augmented Space},
  author = {Manovich, Lev},
  year = {2006},
  month = jun,
  journal = {Visual Communication},
  volume = {5},
  number = {2},
  pages = {219--240},
  issn = {1470-3572, 1741-3214},
  doi = {10.1177/1470357206065527},
  abstract = {This article discusses how people experience spatial forms when they are filled in with dynamic and rich multimedia information; spaces such as shopping or entertainment areas or other spaces where various information can be accessed wirelessly. The author calls such spaces `augmented space': the physical space overlaid with dynamically changing information, multimedia in form and localized for each user. The article asks whether this form becomes irrelevant and `invisible' or if people end up with a new experience in which the spatial and information layers are equally important. The author also discusses the general dynamic between spatial form and information and how this might function differently in today's computer culture. Throughout the article, augmentation is reconceptualized as an idea and cultural and aesthetic practice rather than as technology. Various practices in professional and vernacular architecture and built environments, cinema, 20th-century art and media art are discussed in terms of augmentation.},
  language = {en},
  annotation = {ZSCC: 0000544},
  file = {Human Computer Interaction\\Augmented Reality\\Manovich, 2006 - The poetics of augmented space.pdf}
}

@article{martin2017,
  title = {Percussionist-{{Centred Design}} for {{Touchscreen Digital Musical Instruments}}},
  author = {Martin, Charles P.},
  year = {2017},
  month = mar,
  journal = {Contemporary Music Review},
  volume = {36},
  number = {1-2},
  pages = {64--85},
  issn = {0749-4467, 1477-2256},
  doi = {10.1080/07494467.2017.1370794},
  language = {en},
  file = {Arts & Humanities\\Computational Art\\Martin, 2017 - Percussionist-Centred Design for Touchscreen Digital Musical Instruments.pdf}
}

@article{martina2001,
  title = {Raumsoziologie},
  author = {Martina, L{\"o}w},
  year = {2001},
  journal = {Frankfurt am Main: Suhrkamp},
  annotation = {ZSCC: 0000019}
}

@article{mcalpine1999,
  ids = {mcalpineMakingMusicAlgorithms1999,mcalpineMakingMusicAlgorithms1999a},
  title = {Making {{Music}} with {{Algorithms}}: {{A Case}}-{{Study System}}},
  author = {McAlpine, Kenneth and Miranda, Eduardo and Hoggar, Stuart},
  year = {1999},
  journal = {Computer Music Journal},
  volume = {23},
  number = {2},
  pages = {19--30},
  url = {http://www.jstor.org/stable/3680733},
  language = {en},
  annotation = {ZSCC: 0000121},
  file = {Arts & Humanities\\Computational Art\\McAlpine et al., 1999 - Making Music with Algorithms.pdf}
}

@inproceedings{mcduff2017,
  title = {Pulse and Vital Sign Measurement in Mixed Reality Using a {{HoloLens}}},
  booktitle = {Proceedings of the 23rd {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},
  author = {McDuff, Daniel and Hurter, Christophe and {Gonzalez-Franco}, Mar},
  year = {2017},
  month = nov,
  pages = {1--9},
  publisher = {{ACM}},
  address = {{Gothenburg Sweden}},
  doi = {10.1145/3139131.3139134},
  abstract = {Cardiography, quantitative measurement of the functioning of the heart, traditionally requires customized obtrusive contact sensors. Using new methods photoplethysmography and ballistocardiography signals can be captured using ubiquitous sensors, such as webcams and accelerometers. However, these signals are not visible to the unaided eye. We present Cardiolens - a mixed reality system that enables real-time, hands-free measurement and visualization of blood ow and vital signs from multiple people. e system combines a front-facing webcam, imaging ballistocardiography, and remote imaging photoplethysmography methods for recovering pulse signals. A heads up display allows users to view their own heart rate whenever they are wearing the device and the heart rate and heart rate variability of another person simply by looking at them. Cardiolens provides the wearer with a new way to understand physiological signals and has applications in human-computer interaction and in the study of social psychology.},
  isbn = {978-1-4503-5548-3},
  language = {en},
  file = {Cognitive Science\\Interoception\\McDuff et al., 2017 - Pulse and vital sign measurement in mixed reality using a HoloLens.pdf}
}

@inproceedings{melchior2005,
  title = {Authoring and User Interaction for the Production of Wave Field Synthesis Content in an Augmented Reality System},
  booktitle = {Fourth {{IEEE}} and {{ACM International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}}'05)},
  author = {Melchior, F. and Laubach, T. and {de Vries}, D.},
  year = {2005},
  pages = {48--51},
  publisher = {{IEEE}},
  address = {{Vienna, Austria}},
  doi = {10.1109/ISMAR.2005.20},
  abstract = {Wave field synthesis (WFS) enables the accurate reproduction of a sound field for a large listening area with correct characteristics for each listener position. An exact perspective on the synthesized wave field is provided for every listener. Therefore, WFS-technology is ideally suited to be combined with augmented reality systems, where every user perceives his own visual perspective of a given scene. This paper presents a concept for authoring and user interaction for the production of wave field synthesis content in an augmented reality system. Also, the implementation of a prototype WFS-AR System based on ARToolkit is explained.},
  isbn = {978-0-7695-2459-7},
  language = {en},
  annotation = {ZSCC: 0000014},
  file = {Human Computer Interaction\\Augmented Reality\\Melchior et al., 2005 - Authoring and user interaction for the production of wave field synthesis.pdf}
}

@inproceedings{mendonca2020,
  title = {Surround Sound Spreads Visual Attention and Increases Cognitive Effort in Immersive Media Reproductions},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Mendon{\c c}a, Catarina and Korshunova, Victoria},
  year = {2020},
  series = {{{AM}} '20},
  pages = {16--21},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411118},
  abstract = {The goal of this study was to explore the effects of different spatial sound configurations on visual attention and cognitive effort in an immersive environment. For that purpose, different groups of people were exposed to the same immersive video, but with different soundtrack conditions: mono, stereo, 5.1 and 7.4.1. The different sound conditions consisted of different artistic adaptations of the same soundtrack. During the visualization of the video, participants wore an eye-tracking device and were asked to perform a counting task. Gaze direction and pupil dilation metrics were obtained, as measures of attention and cognitive effort. Results demonstrate that the conditions 5.1 and 7.4.1 were associated with larger distributions of the visual attention, with subjects spending more time gazing at task-irrelevant areas on the screen. The sound condition which led to more concentrated attention on the task-relevant area was mono. The wider the spatial sound configuration, the greater the gaze distribution. Conditions 7.4.1 and 5.1 were also associated with larger pupil dilations than the mono and stereo conditions, showing that these conditions might lead to increased cognitive demand and therefore increased task difficulty. We conclude that sound design should be carefully planned to prevent visual distraction. More surrounding spatialized sounds may lead to more distraction and more difficulty in following audiovisual contents than less distributed sounds. We propose that sound spatialization and soundtrack design should be adapted to the audiovisual content and the task at hand, varying in immersiveness accordingly.},
  isbn = {978-1-4503-7563-4},
  keywords = {attention,audiovisual,difficulty,perception,pupil dilation,sound design,spatial audio,virtual reality},
  annotation = {ZSCC: 0000000},
  file = {Human Computer Interaction\\Augmented Reality\\Mendonça and Korshunova, 2020 - Surround sound spreads visual attention and increases cognitive effort in.pdf}
}

@inproceedings{metatla2016,
  title = {Tap the {{ShapeTones}}: {{Exploring}} the {{Effects}} of {{Crossmodal Congruence}} in an {{Audio}}-{{Visual Interface}}},
  shorttitle = {Tap the {{ShapeTones}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Metatla, Oussama and Correia, Nuno N. and Martin, Fiore and {Bryan-Kinns}, Nick and Stockman, Tony},
  year = {2016},
  month = may,
  pages = {1055--1066},
  publisher = {{ACM}},
  address = {{San Jose California USA}},
  doi = {10.1145/2858036.2858456},
  abstract = {There is growing interest in the application of crossmodal perception to interface design. However, most research has focused on task performance measures and often ignored user experience and engagement. We present an examination of crossmodal congruence in terms of performance and engagement in the context of a memory task of audio, visual, and audio-visual stimuli. Participants in a first study showed improved performance when using a visual congruent mapping that was cancelled by the addition of audio to the baseline conditions, and a subjective preference for the audio-visual stimulus that was not reflected in the objective data. Based on these findings, we designed an audio-visual memory game to examine the effects of crossmodal congruence on user experience and engagement. Results showed higher engagement levels with congruent displays with some reported preference for potential challenge and enjoyment that an incongruent display may support, particularly for increased task complexity.},
  isbn = {978-1-4503-3362-7},
  language = {en},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Metatla et al., 2016 - Tap the ShapeTones.pdf}
}

@misc{microsoft2019,
  title = {Microsoft {{HoloLens}}},
  author = {Microsoft},
  year = {2019},
  journal = {Microsoft HoloLens 2},
  urldate = {2020-05-25},
  abstract = {Introducing HoloLens 2, an untethered mixed reality headset that's designed to help you solve real business problems today using intelligent apps and solutions.},
  language = {en-us},
  file = {..\\..\\Zotero\\storage\\QXFBA8XE\\hololens.html},
  url = {https://www.microsoft.com/en-us/hololens}
}

@article{milgram1994,
  title = {A {{Taxonomy}} of {{Mixed Reality Visual Displays}}},
  author = {Milgram, Paul and Kishino, Fumio},
  year = {1994},
  month = dec,
  journal = {IEICE Transactions on Information and Systems},
  volume = {E77-D},
  number = {12},
  pages = {8},
  abstract = {Mixed Reality (MR) visual displays, a particular subset of Virtual Reality (VR) related technologies, involve the merging of real and virtual worlds somewhere along the 'virtuality continuum' which connects completely real environments to completely virtual ones. Augmented Reality (AR), probably the best known of these, refers to all cases in which the display of an otherwise real environment is augmented by means of virtual (computer graphic) objects. The converse case on the virtuality continuum is therefore Augmented Virtuality (AV). Six classes of hybrid MR display environments are identified. However quite different groupings are possible and this demonstrates the need for an efficient taxonomy, or classification framework, according to which essential differences can be identified. An approximately three-dimensional taxonomy is proposed comprising the following dimensions: extent of world knowledge, reproduction fidelity, and extent of presence metaphor.},
  annotation = {ZSCC: 0004839},
  file = {Human Computer Interaction\\Augmented Reality\\Milgram and Kishino, 1994 - A Taxonomy of Mixed Reality Visual Displays.pdf}
}

@incollection{misker2010,
  title = {Authoring {{Immersive Mixed Reality Experiences}}},
  booktitle = {The {{Engineering}} of {{Mixed Reality Systems}}},
  author = {Misker, Jan M.V. and {van der Ster}, Jelle},
  editor = {Dubois, Emmanuel and Gray, Philip and Nigay, Laurence},
  year = {2010},
  pages = {275--291},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-84882-733-2_14},
  abstract = {Creating a mixed reality experience is a complicated endeavour. From our practice as a media lab in the artistic domain we found that engineering is `only' a first step in creating a mixed reality experience. Designing the appearance and directing the user experience are equally important for creating an engaging, immersive experience. We found that mixed reality artworks provide a very good test bed for studying these topics. This chapter details three steps required for authoring mixed reality experiences: engineering, designing and directing. We will describe a platform (VGE) for creating mixed reality environments that incorporates these steps. A case study (EI4) is presented in which this platform was used to not only engineer the system, but in which an artist was given the freedom to explore the artistic merits of mixed reality as an artistic medium, which involved areas such as the look and feel, multimodal experience and interaction, immersion as a subjective emotion and game play scenarios.},
  isbn = {978-1-84882-732-5 978-1-84882-733-2},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Misker and van der Ster, 2010 - Authoring Immersive Mixed Reality Experiences.pdf}
}

@book{moorefield2010,
  title = {The Producer as Composer: Shaping the Sounds of Popular Music},
  shorttitle = {The Producer as Composer},
  author = {Moorefield, Virgil},
  year = {2010},
  edition = {1st MIT Press pbk. ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-51405-7},
  lccn = {ML3470 .M66 2010},
  keywords = {Analysis; appreciation,Popular music,Production and direction History,Sound recordings}
}

@article{mori2017,
  title = {A Survey of Diminished Reality: {{Techniques}} for Visually Concealing, Eliminating, and Seeing through Real Objects},
  shorttitle = {A Survey of Diminished Reality},
  author = {Mori, Shohei and Ikeda, Sei and Saito, Hideo},
  year = {2017},
  month = dec,
  journal = {IPSJ Transactions on Computer Vision and Applications},
  volume = {9},
  number = {1},
  pages = {17},
  issn = {1882-6695},
  doi = {10.1186/s41074-017-0028-1},
  abstract = {In this paper, we review diminished reality (DR) studies that visually remove, hide, and see through real objects from the real world. We systematically analyze and classify publications and present a technology map as a reference for future research. We also discuss future directions, including multimodal diminished reality. We believe that this paper will be useful mainly for students who are interested in DR, beginning DR researchers, and teachers who introduce DR in their classes.},
  language = {en},
  annotation = {ZSCC: 0000061},
  file = {Human Computer Interaction\\Augmented Reality\\Mori et al., 2017 - A survey of diminished reality.pdf}
}

@book{murchison2010,
  title = {Ethnography Essentials: Designing, Conducting, and Presenting Your Research},
  shorttitle = {Ethnography Essentials},
  author = {Murchison, Julian M.},
  year = {2010},
  series = {Research Methods for the Social Sciences},
  edition = {1st ed},
  publisher = {{Jossey-Bass}},
  address = {{San Francisco}},
  isbn = {978-0-470-34389-0},
  language = {en},
  lccn = {GN345 .M87 2010},
  keywords = {Ethnology,Methodology},
  annotation = {ZSCC: 0000524  OCLC: ocn426796733},
  file = {Research Methods\\Autoethnography\\Murchison, 2010 - Ethnography essentials.pdf}
}

@misc{musgrave1975,
  title = {Orfeo {{II}}: {{An Improvisation}} on a {{Theme}}},
  author = {Musgrave, Thea},
  year = {1975},
  journal = {Music Sales Classical},
  urldate = {2018-04-13},
  copyright = {Music Sales Classical},
  file = {..\\..\\Zotero\\storage\\48K8ZPMX\\8436.html},
  url = {http://www.musicsalesclassical.com/composer/work/8436}
}

@misc{musgrave1975a,
  title = {Orfeo {{I}}},
  author = {Musgrave, Thea},
  year = {1975},
  journal = {Music Sales Classical},
  urldate = {2018-04-13},
  copyright = {Music Sales Classical},
  file = {..\\..\\Zotero\\storage\\Q65CPZ7B\\8432.html},
  url = {http://www.musicsalesclassical.com/composer/work/8432}
}

@misc{musgrave1987,
  title = {Narcissus},
  author = {Musgrave, Thea},
  year = {1987},
  journal = {Music Sales Classical},
  urldate = {2018-04-13},
  copyright = {Music Sales Classical},
  file = {..\\..\\Zotero\\storage\\CWP6SYDJ\\11633.html},
  url = {http://www.musicsalesclassical.com/composer/work/11633}
}

@misc{musgrave1993,
  title = {Orfeo {{III}}},
  author = {Musgrave, Thea},
  year = {1993},
  journal = {Music Sales Classical},
  urldate = {2018-04-13},
  copyright = {Music Sales Classical},
  file = {..\\..\\Zotero\\storage\\YVKTMIL5\\8437.html},
  url = {http://www.musicsalesclassical.com/composer/work/1098/8437#}
}

@article{nagel2005,
  title = {Beyond Sensory Substitution\textemdash Learning the Sixth Sense},
  author = {Nagel, Saskia K and Carl, Christine and Kringe, Tobias and M{\"a}rtin, Robert and K{\"o}nig, Peter},
  year = {2005},
  month = dec,
  journal = {Journal of Neural Engineering},
  volume = {2},
  number = {4},
  pages = {R13-R26},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2560/2/4/R02},
  abstract = {Rapid advances in neuroscience have sparked numerous efforts to study the neural correlate of consciousness. Prominent subjects include higher sensory area, distributed assemblies bound by synchronization of neuronal activity and neurons in specific cortical laminae. In contrast, it has been suggested that the quality of sensory awareness is determined by systematic change of afferent signals resulting from behaviour and knowledge thereof. Support for such skill-based theories of perception is provided by experiments on sensory substitution. Here, we pursue this line of thought and create new sensorimotor contingencies and, hence, a new quality of perception. Adult subjects received orientation information, obtained by a magnetic compass, via vibrotactile stimulation around the waist. After six weeks of training we evaluated integration of the new input by a battery of tests. The results indicate that the sensory information provided by the belt (1) is processed and boosts performance, (2) if inconsistent with other sensory signals leads to variable performance, (3) does interact with the vestibular nystagmus and (4) in half of the experimental subjects leads to qualitative changes of sensory experience. These data support the hypothesis that new sensorimotor contingencies can be learned and integrated into behaviour and affect perceptual experience.},
  language = {en},
  annotation = {ZSCC: 0000266},
  file = {Cognitive Science\\Multisensory Integration\\Nagel et al., 2005 - Beyond sensory substitution—learning the sixth sense.pdf}
}

@inproceedings{naimark1991,
  title = {Elements of Real-Space Imaging: A Proposed Taxonomy},
  shorttitle = {Elements of Real-Space Imaging},
  booktitle = {Electronic {{Imaging}} '91, {{San Jose}},{{CA}}},
  author = {Naimark, Michael},
  editor = {Merritt, John O. and Fisher, Scott S.},
  year = {1991},
  month = aug,
  pages = {169--179},
  address = {{San Jose, CA}},
  doi = {10.1117/12.46305},
  abstract = {Along with the marriage of motion pictures and computers has come an increasing interest in making images appear to have a greater degree of realness or presence, which I call "realspace imaging." Such topics as high definition television, 3D, fisheye lenses, surrogate travel, arid "cyberspace" reflect such interest. These topics are usually piled together and are unparsable, with the implicit assumptions that "the more resolution, the more presence" and "the more presence, the better." This paper proposes a taxonomy of the elements of realspace imaging. The taxonomy is organized around six sections: 1) monoscopic imaging, 2) stereoscopic imaging, 3) multiscopic imaging, 4) panoramics, 5) surrogate travel, and 6) realtime imaging.},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Naimark, 1991 - Elements of real-space imaging.pdf}
}

@inproceedings{narumi2011,
  title = {Augmented Reality Flavors: Gustatory Display Based on Edible Marker and Cross-Modal Interaction},
  shorttitle = {Augmented Reality Flavors},
  booktitle = {Proceedings of the 2011 Annual Conference on {{Human}} Factors in Computing Systems - {{CHI}} '11},
  author = {Narumi, Takuji and Nishizaka, Shinya and Kajinami, Takashi and Tanikawa, Tomohiro and Hirose, Michitaka},
  year = {2011},
  pages = {93},
  publisher = {{ACM Press}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1145/1978942.1978957},
  abstract = {The main contribution of this paper is to realize computer generated augmented flavors and establish a method to integrate gustatory information into computer human interactions. There are several reasons for the scarcity of research on gustatory information. One reason is that taste sensations are affected by a number of factors, such as vision, olfaction and memories. This produces a complex cognition mechanism for a user's gustatory sensation, and makes it difficult to build up a gustatory display which produces a specific taste on demand.},
  isbn = {978-1-4503-0228-9},
  language = {en},
  annotation = {ZSCC: 0000148},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Narumi et al., 2011 - Augmented reality flavors.pdf}
}

@incollection{ndalianis2003,
  title = {Architectures of the {{Senses}}: {{Neobaroque Entertainment Spectacles}}},
  booktitle = {Rethinking Media Change: The Aesthetics of Transition},
  author = {Ndalianis, Angela},
  year = {2003},
  series = {Media in Transition},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-20146-9},
  lccn = {P90 .R38 2003},
  keywords = {History,Mass media},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Aesthetics\\Ndalianis, 2003 - Architectures of the Senses.pdf}
}

@book{negus2009,
  title = {Popular Music in Theory: An Introduction},
  shorttitle = {Popular Music in Theory},
  author = {Negus, Keith},
  year = {2009},
  edition = {Reprinted},
  publisher = {{Polity Press}},
  address = {{Cambridge}},
  isbn = {978-0-7456-1318-5 978-0-7456-1317-8},
  language = {eng},
  annotation = {OCLC: 837647562}
}

@article{neustaedter2012,
  title = {Autobiographical Design in {{HCI}} Research: Designing and Learning through Use-It-Yourself},
  author = {Neustaedter, Carman and Sengers, Phoebe},
  year = {2012},
  pages = {10},
  abstract = {Designing a system with yourself as a target user and evaluating the design through your own self-usage is commonly considered a questionable approach in HCI research. Perhaps for this reason, HCI research including extensive self-usage of a design is underdocumented. Yet such self-usage does happen and many researchers have found great value in the lessons learned from it. Our goal in this paper is to bring these hidden practices to light and offer guidelines for how HCI researchers can usefully engage in what we term `autobiographical design'\textemdash design research drawing on extensive, genuine usage by those creating or building a system. Through interviews with HCI experts who have engaged in variations of autobiographical design, we draw out the possibilities and limitations of autobiographical design methods and lay out best practices for its use as an HCI research method.},
  language = {en},
  annotation = {ZSCC: 0000126},
  file = {Research Methods\\Autobiographical Design\\Neustaedter and Sengers, 2012 - Autobiographical design in HCI research.pdf}
}

@inproceedings{nishida2019,
  title = {Egocentric {{Smaller}}-Person {{Experience}} through a {{Change}} in {{Visual Perspective}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Nishida, Jun and Matsuda, Soichiro and Oki, Mika and Takatori, Hikaru and Sato, Kosuke and Suzuki, Kenji},
  year = {2019},
  month = may,
  pages = {1--12},
  publisher = {{ACM}},
  address = {{Glasgow Scotland Uk}},
  doi = {10.1145/3290605.3300926},
  isbn = {978-1-4503-5970-2},
  language = {en},
  annotation = {ZSCC: 0000008},
  file = {Human Computer Interaction\\Augmented Reality\\Nishida et al., 2019 - Egocentric Smaller-person Experience through a Change in Visual Perspective.pdf}
}

@inproceedings{nishida2020,
  title = {{{HandMorph}}: A {{Passive Exoskeleton}} That {{Miniaturizes Grasp}}},
  shorttitle = {{{HandMorph}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Nishida, Jun and Matsuda, Soichiro and Matsui, Hiroshi and Teng, Shan-Yuan and Liu, Ziwei and Suzuki, Kenji and Lopes, Pedro},
  year = {2020},
  month = oct,
  pages = {565--578},
  publisher = {{ACM}},
  address = {{Virtual Event USA}},
  doi = {10.1145/3379337.3415875},
  abstract = {We engineered an exoskeleton, which we call HandMorph, that approximates the experience of having a smaller grasp\- ing range. It uses mechanical links to transmit motion from the wearer's fingers to a smaller hand with five anatomically correct fingers. The result is that HandMorph miniaturizes a wearer's grasping range while transmitting haptic feedback.},
  isbn = {978-1-4503-7514-6},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Nishida et al., 2020 - HandMorph.pdf}
}

@inproceedings{normand2012,
  title = {A New Typology of Augmented Reality Applications},
  booktitle = {Proceedings of the 3rd {{Augmented Human International Conference}} on - {{AH}} '12},
  author = {Normand, Jean-Marie and Servi{\`e}res, Myriam and Moreau, Guillaume},
  year = {2012},
  pages = {1--8},
  publisher = {{ACM Press}},
  address = {{Meg\&\#232;ve, France}},
  doi = {10.1145/2160125.2160143},
  abstract = {In recent years Augmented Reality (AR) has become more and more popular, especially since the availability of mobile devices, such as smartphones or tablets, brought AR into our everyday life. Although the AR community has not yet agreed on a formal definition of AR, some work focused on proposing classifications of existing AR methods or applications. Such applications cover a wide variety of technologies, devices and goals, consequently existing taxonomies rely on multiple classification criteria that try to take into account AR applications diversity. In this paper we review existing taxonomies of augmented reality applications and we propose our own, which is based on (1) the number of degrees of freedom required by the application, as well as on (2) the visualization mode used, (3) the temporal base of the displayed content and (4) the rendering modalities used in the application. Our taxonomy covers location-based services as well as more traditional visionbased AR applications. Although AR is mainly based on the visual sense, other rendering modalities are also covered by the same degree-of-freedom criterion in our classification.},
  isbn = {978-1-4503-1077-2},
  language = {en},
  annotation = {ZSCC: 0000053},
  file = {Human Computer Interaction\\Augmented Reality\\Normand et al., 2012 - A new typology of augmented reality applications.pdf}
}

@misc{notion2020,
  title = {Notion \textendash{} {{The}} All-in-One Workspace for Your Notes, Tasks, Wikis, and Databases.},
  author = {Notion},
  year = {2020},
  journal = {Notion},
  urldate = {2020-05-25},
  abstract = {A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team},
  language = {en},
  file = {..\\..\\Zotero\\storage\\DXNI86HI\\www.notion.so.html},
  url = {https://www.notion.so}
}

@misc{nreal2020,
  title = {Nreal {{Light}}},
  author = {{Nreal}},
  year = {2020},
  urldate = {2020-10-01},
  file = {..\\..\\Zotero\\storage\\FPP375TI\\product.html},
  url = {https://nreal.ai/product/}
}

@article{nystrom2011,
  ids = {nystromTextonsPropagationSpace2011,nystromTextonsPropagationSpace2011a},
  title = {Textons and the {{Propagation}} of {{Space}} in {{Acousmatic Music}}},
  author = {Nystr{\"o}m, Erik},
  year = {2011},
  month = apr,
  journal = {Organised Sound},
  volume = {16},
  number = {1},
  pages = {14--26},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S1355771810000397},
  abstract = {The concepts introduced by Smalley in the context of space-form (2007) have firmly put acousmatic music on a discourse of spatial exploration, holding much potential for the developing of aesthetics in new directions. This article approaches space from the low level of musical structure, with a multi-dimensional attitude to space-form, exploring               spatial texture               , a concept introduced by Smalley to describe the temporal formations of space in spectromorphology (1997). Spatial articulation is investigated in the context of granular-oriented textures, proposing a micro-spatial, perceptual morphology \textendash{} the               texton               \textendash{} as an aesthetic approach to acousmatic music. This follows Albert Bregman's speculation regarding equivalents to visual perception in texture, where the theory of textons was first developed by the neuroscientist B\'ela Julesz.                          The article discusses acousmatic textons, in terms of intrinsic properties, the way they propagate in time, and how they organise in distributions to form spatial textures. The emergent macroscopic qualities of textonal formations are also reflected upon in the introduction of a group of textural states, where source-bonded spaces and abstract musical thinking coalesce.},
  language = {en},
  annotation = {ZSCC: 0000007},
  file = {Arts & Humanities\\Computational Art\\Nyström, 2011 - Textons and the Propagation of Space in Acousmatic Music.pdf}
}

@phdthesis{nystrom2013,
  title = {Topology of {{Spatial Texture}} in the {{Acousmatic Medium}}},
  author = {Nystr{\"o}m, Erik},
  year = {2013},
  language = {en},
  school = {City University London},
  annotation = {ZSCC: 0000008},
  file = {Arts & Humanities\\Computational Art\\Nyström, 2013 - Topology of Spatial Texture in the Acousmatic Medium.pdf}
}

@article{nystrom2015,
  ids = {nystromLowLevelTopologySpatial2015,nystromLowLevelTopologySpatial2015a},
  title = {Low-{{Level Topology}} of {{Spatial Texture}}},
  author = {Nystr{\"o}m, Erik},
  year = {2015},
  pages = {5},
  abstract = {Low-level topology of spatial texture is here introduced as the basis of an aesthetic principle of sonic texture and spatial structure in electroacoustic music. The term spatial texture is used to describe aggregate sound structures which have a perceived three-dimensional spatial presence, specifically meaning that they occupy several areas or a stretch of horizontal perspectival space1 whilst also having a dynamic behaviour in spectral space2. The word topology refers to properties, qualities and structural features which remain distinct to a texture despite continuous change or recurrent incarnations in different specific shapes throughout a work.3 Ultimately, topology of spatial texture may be thought of as the core principle behind an attitude to music which considers all elements of structure to be part of an elastic spatiotemporal sound fabric. Rather than conceiving a work as built from time-finite morphological `objects', this view emphasises processes of deformation, where any singular shapes may be seen as instances of textural topologies. The terminology presented here is intended as a contribution to discourse on spatiality in music, with special relevance to multichannel compositions.4 This article focuses on the low-level, internal, structure of a spatial texture.},
  language = {en},
  annotation = {ZSCC: 0000002},
  file = {Arts & Humanities\\Computational Art\\Nyström, 2015 - Low-Level Topology of Spatial Texture.pdf}
}

@inproceedings{obrist2014,
  title = {Temporal, Affective, and Embodied Characteristics of Taste Experiences: A Framework for Design},
  shorttitle = {Temporal, Affective, and Embodied Characteristics of Taste Experiences},
  booktitle = {Proceedings of the 32nd Annual {{ACM}} Conference on {{Human}} Factors in Computing Systems - {{CHI}} '14},
  author = {Obrist, Marianna and Comber, Rob and Subramanian, Sriram and {Piqueras-Fiszman}, Betina and Velasco, Carlos and Spence, Charles},
  year = {2014},
  pages = {2853--2862},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/2556288.2557007},
  abstract = {We present rich descriptions of taste experience through an analysis of the diachronic and synchronic experiences of each of the five basic taste qualities: sweet, sour, salt, bitter, and umami. Our findings, based on a combination of user experience evaluation techniques highlight three main themes: temporality, affective reactions, and embodiment. We present the taste characteristics as a framework for design and discuss each taste in order to elucidate the design qualities of individual taste experiences. These findings add a semantic understanding of taste experiences, their temporality enhanced through descriptions of the affective reactions and embodiment that the five basic tastes elicit. These findings are discussed on the basis of established psychological and behavioral phenomena, highlighting the potential for taste-enhanced design.},
  isbn = {978-1-4503-2473-1},
  language = {en},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Obrist et al., 2014 - Temporal, affective, and embodied characteristics of taste experiences.pdf}
}

@article{obrist2016,
  title = {Sensing the Future of {{HCI}}: Touch, Taste, and Smell User Interfaces},
  shorttitle = {Sensing the Future of {{HCI}}},
  author = {Obrist, Marianna and Velasco, Carlos and Vi, Chi and Ranasinghe, Nimesha and Israr, Ali and Cheok, Adrian and Spence, Charles and Gopalakrishnakone, Ponnampalam},
  year = {2016},
  month = aug,
  journal = {interactions},
  volume = {23},
  number = {5},
  pages = {40--49},
  issn = {10725520},
  doi = {10.1145/2973568},
  language = {en},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Obrist et al., 2016 - Sensing the future of HCI.pdf}
}

@inproceedings{obrist2016a,
  title = {Touch, {{Taste}}, \& {{Smell User Interfaces}}: {{The Future}} of {{Multisensory HCI}}},
  shorttitle = {Touch, {{Taste}}, \& {{Smell User Interfaces}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}  - {{CHI EA}} '16},
  author = {Obrist, Marianna and Velasco, Carlos and Vi, Chi Thanh and Ranasinghe, Nimesha and Israr, Ali and Cheok, Adrian D. and Spence, Charles and Gopalakrishnakone, Ponnampalam},
  year = {2016},
  pages = {3285--3292},
  publisher = {{ACM Press}},
  address = {{San Jose, California, USA}},
  doi = {10.1145/2851581.2856462},
  abstract = {The senses we call upon when interacting with technology are very restricted. We mostly rely on vision and audition, increasingly harnessing touch, whilst taste and smell remain largely underexploited. In spite of our current knowledge about sensory systems and sensory devices, the biggest stumbling block for progress concerns the need for a deeper understanding of people's multisensory experiences in HCI. It is essential to determine what tactile, gustatory, and olfactory experiences we can design for, and how we can meaningfully stimulate such experiences when interacting with technology. Importantly, we need to determine the contribution of the different senses along with their interactions in order to design more effective and engaging digital multisensory experiences. Finally, it is vital to understand what the limitations are that come into play when users need to monitor more than one sense at a time. The aim of this workshop is to deepen and expand the discussion on touch, taste, and smell within the CHI community and promote the relevance of multisensory experience design and research in HCI.},
  isbn = {978-1-4503-4082-3},
  language = {en},
  annotation = {ZSCC: 0000039},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Obrist et al., 2016 - Touch, Taste, & Smell User Interfaces.pdf}
}

@inproceedings{obrist2017,
  title = {Mastering the {{Senses}} in {{HCI}}: {{Towards Multisensory Interfaces}}},
  shorttitle = {Mastering the {{Senses}} in {{HCI}}},
  booktitle = {Proceedings of the 12th {{Biannual Conference}} on {{Italian SIGCHI Chapter}} - {{CHItaly}} '17},
  author = {Obrist, Marianna},
  year = {2017},
  pages = {1--2},
  publisher = {{ACM Press}},
  address = {{Cagliari, Italy}},
  doi = {10.1145/3125571.3125603},
  abstract = {With the proliferation of sensory technologies that do not only stimulate the sense of vision and hearing, but also our sense of touch, smell, and taste, we are confronted with the challenge of mastering those ``new'' senses in the design of interactive systems. To meaningfully design multisensory interfaces and enrich human-technology interactions we need to systematically investigate the technical, perceptual, and experiential parameters of sensory and multisensory stimulation. Here, I particularly focus on the study of tactile, gustatory, and olfactory experiences facilitated by the use of novel technologies (e.g., mid-air haptic devices, olfactory devices) and the combination of objective and subjective measures within sensory science, psychology, HCI, and user experience research.},
  isbn = {978-1-4503-5237-6},
  language = {en},
  annotation = {ZSCC: 0000007},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Obrist, 2017 - Mastering the Senses in HCI.pdf}
}

@article{obrist2017a,
  title = {Multisensory {{Experiences}} in {{HCI}}},
  author = {Obrist, Marianna and Gatti, Elia and Maggioni, Emanuela and Vi, Chi Thanh and Velasco, Carlos},
  year = {2017},
  month = apr,
  journal = {IEEE MultiMedia},
  volume = {24},
  number = {2},
  pages = {9--13},
  issn = {1070-986X},
  doi = {10.1109/MMUL.2017.33},
  language = {en},
  annotation = {ZSCC: 0000044},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Obrist et al., 2017 - Multisensory Experiences in HCI.pdf}
}

@misc{oculus2019,
  title = {Oculus {{Quest}}},
  author = {Oculus},
  year = {2019},
  urldate = {2020-07-12},
  abstract = {Defy reality and distance with Oculus. Our VR headsets connect people and redefine digital gaming and entertainment. Learn more about Rift, Rift S, Quest and Go.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\87A4C3IR\\www.oculus.com.html},
  url = {https://www.oculus.com/}
}

@misc{oculus2020,
  title = {Oculus {{Quest}} 2},
  shorttitle = {Oculus {{Quest}} 2},
  author = {Oculus},
  year = {2020},
  urldate = {2020-10-01},
  abstract = {Oculus Quest 2 is our most advanced all-in-one VR system yet. Explore an expansive library of awe-inspiring games and immersive experiences with unparalleled freedom.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\HPA26UYD\\quest-2.html},
  url = {https://www.oculus.com/quest-2/}
}

@inproceedings{odea2019,
  title = {Auditory {{Distraction}} in {{HCI}}: {{Towards}} a {{Framework}} for the {{Design}} of {{Hierarchically}}-{{Graded Auditory Notifications}}},
  shorttitle = {Auditory {{Distraction}} in {{HCI}}},
  booktitle = {Proceedings of the 14th {{International Audio Mostly Conference}}: {{A Journey}} in {{Sound}}},
  author = {O'Dea, Ronan and Jedir, Rokaia and Neff, Flaithri},
  year = {2019},
  month = sep,
  pages = {61--66},
  publisher = {{ACM}},
  address = {{Nottingham United Kingdom}},
  doi = {10.1145/3356590.3356601},
  abstract = {This paper discusses hierarchical structure of auditory distractors based on two human perceptual systems responsible for distinct pre-attentive process:1. the auditory perceptual system and 2. the working memory (WM) system. Specifically, the authors propose accounting for WM function and capacity when designing auditory notifications for multimodal applications, due to interaction between certain auditory attention mechanisms and WM. A review of literature concerning WM disruption caused by auditory streams, as well as reference to relevant ISO (International Organization for Standardization) standards, is also presented.},
  isbn = {978-1-4503-7297-8},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\O'Dea et al., 2019 - Auditory Distraction in HCI.pdf}
}

@inproceedings{onate2020,
  title = {Seeking for Spectral Manipulation of the Sound of Musical Instruments Using Metamaterials},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {O{\~n}ate, Carolina Espinoza and Arancibia, Alonso and Cartes, Gabriel and Beas, Claudio Falc{\'o}n},
  year = {2020},
  series = {{{AM}} '20},
  pages = {277--280},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411127},
  abstract = {The sound of practically all traditional musical instruments is unique and depends on the collective behavior of various vibrators, each one with their own acoustic and mechanical properties. If we pluck a string of an acoustic guitar, a part of the wave is reflected by the sound box, and the other fraction of the elastic energy sets in motion the sound box surfaces. Thereby, the vibration of the surfaces (soundboard, ribs, back and sound hole), are the basis of the guitar sound production.In this work, we explore the effect of locally coupling tunable mechanical metamaterials to the soundboard of an acoustic guitar, in order to absorb specific ranges of frequencies and change its vibrational properties. We show the preliminary results of our research, which involves a mechano-acoustic characterization of tunable mechanical metamaterials and the analysis of the effect of applying them to an acoustic guitar when a string tuned to a convenient frequency is plucked. We observe that this simple mechanism is an alternative to manipulate the spectral properties of the sound signal produced by the instrument. Although the results are inaudible, they seem promising for future explorations of sound manipulation of musical instruments.},
  isbn = {978-1-4503-7563-4},
  keywords = {acoustic,musical instruments,new materials,sonic interaction design},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Oñate et al., 2020 - Seeking for spectral manipulation of the sound of musical instruments using.pdf}
}

@misc{oxfordreference2020,
  title = {Ocularcentrism},
  author = {Oxford Reference},
  year = {2020},
  journal = {Oxford Reference},
  doi = {10.1093/oi/authority.20110803100245338},
  abstract = {"ocularcentrism" published on  by null.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\KJKD38JW\\authority.html}
}

@article{panciroli2018,
  title = {Educating about {{Art}} by {{Augmented Reality}}: {{New Didactic Mediation Perspectives}} at {{School}} and in {{Museums}}},
  shorttitle = {Educating about {{Art}} by {{Augmented Reality}}},
  author = {Panciroli, Chiara and Macauda, Anita and Russo, Veronica},
  year = {2018},
  month = mar,
  journal = {Proceedings},
  volume = {1},
  number = {9},
  pages = {1107},
  issn = {2504-3900},
  doi = {10.3390/proceedings1091107},
  abstract = {Different national and international researches have stressed relevant aspects concerning the application of augmented reality in formal and non-formal educational contexts, especially at school and in museums. In fact, augmented reality plays a meaningful role in the relationship between technologies and didactic mediation; its applications are the prerequisite for an augmented learning, through the reproduction of specific scenarios which go beyond the pure theoretical dimension. More specifically the present contribution aims to set out an option for a reflection on the relationship between art education and augmented reality technologies from the didactic mediation point of view, with reference to a shared and collaborative construction of knowledge of artistic and cultural heritage.},
  language = {en},
  annotation = {ZSCC: 0000006},
  file = {Human Computer Interaction\\Augmented Reality\\Panciroli et al., 2018 - Educating about Art by Augmented Reality.pdf}
}

@article{papagiannis2014,
  title = {Working towards Defining an Aesthetics of Augmented Reality: {{A}} Medium in Transition},
  shorttitle = {Working towards Defining an Aesthetics of Augmented Reality},
  author = {Papagiannis, Helen},
  year = {2014},
  month = feb,
  journal = {Convergence: The International Journal of Research into New Media Technologies},
  volume = {20},
  number = {1},
  pages = {33--40},
  issn = {1354-8565, 1748-7382},
  doi = {10.1177/1354856513514333},
  abstract = {The present is a critical time in augmented reality's (AR) definition, as a new medium with aesthetics and conventions just beginning to emerge. We are at a moment when we can look both to the future and to the past: still seeing the previous forms that are shaping AR as a medium while paving new paths, contributing to novel styles and tropes. This article will work toward defining an aesthetics of AR as a new medium in transition, discussing my work as both an artist and a researcher in AR.},
  language = {en},
  annotation = {ZSCC: 0000009},
  file = {Human Computer Interaction\\Augmented Reality\\Papagiannis, 2014 - Working towards defining an aesthetics of augmented reality.pdf}
}

@incollection{papagiannis2017,
  title = {The {{Critical Role}} of {{Artists}} in {{Advancing Augmented Reality}}},
  booktitle = {The {{Next Step}}: {{Exponential Life}}},
  author = {Papagiannis, Helen},
  year = {2017},
  pages = {124--139},
  publisher = {{Turner Libros}},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Papagiannis, 2017 - The Critical Role of Artists in Advancing Augmented Reality.pdf}
}

@misc{parikka2010,
  title = {What Is {{Media Archaeology}}?},
  shorttitle = {What Is {{Media Archaeology}}?},
  author = {Parikka, Jussi},
  year = {2010},
  month = oct,
  url = {http://mediacartographies.blogspot.com/2010/10/what-is-media-archaeology-beta.html},
  urldate = {2018-04-13}
}

@article{parikka2015,
  title = {Mutating {{Media Ecologies}}},
  author = {Parikka, Jussi},
  year = {2015},
  pages = {8},
  language = {en},
  annotation = {ZSCC: 0000006},
  file = {Arts & Humanities\\Media Studies\\Parikka, 2015 - Mutating Media Ecologies.pdf}
}

@incollection{paul2015,
  title = {Genealogies of the {{New Aesthetic}}},
  booktitle = {Postdigital {{Aesthetics}}},
  author = {Paul, Christiane and Levy, Malcolm},
  editor = {Berry, David M. and Dieter, Michael},
  year = {2015},
  pages = {27--43},
  publisher = {{Palgrave Macmillan UK}},
  address = {{London}},
  isbn = {978-1-349-49378-4 978-1-137-43720-4},
  language = {en},
  annotation = {ZSCC: 0000017[s0]},
  file = {Arts & Humanities\\Media Studies\\Paul and Levy, 2015 - Genealogies of the New Aesthetic.pdf}
}

@inproceedings{paul2015a,
  title = {From {{Immateriality}} to {{Neomateriality}}: {{Art}} and the {{Conditions}} of {{Digital Materiality}}},
  booktitle = {Proceedings of the 21st {{International Symposium}} on {{Electronic Art}}},
  author = {Paul, Christiane},
  year = {2015},
  pages = {4},
  address = {{Vancouver, BC, Canada}},
  abstract = {This paper explores the evolution of materialities in the context of art and digital technologies and proposes ``neomateriality'' as a current condition of material and objecthood. It traces the evolution from dematerialization and the immaterial to hypermateriality and neomateriality as a term capturing various disruptions that introduce new aesthetic paradigms. The concept of neomateriality strives to describe an objecthood that incorporates networked digital technologies, and embeds, processes, and reflects back the data of humans and the environment, or reveals its own coded materiality and the way in which digital processes see our world.},
  language = {en},
  annotation = {ZSCC: 0000026},
  file = {Philosophy\\Materiality\\Paul, 2015 - From Immateriality to Neomateriality.pdf}
}

@incollection{paul2016,
  title = {Augmented {{Realities}}: {{Digital Art}} in the {{Public Sphere}}},
  shorttitle = {Augmented {{Realities}}},
  booktitle = {A {{Companion}} to {{Public Art}}},
  author = {Paul, Christiane},
  editor = {Knight, Cher Krause and Senie, Harriet F.},
  year = {2016},
  month = aug,
  edition = {First},
  pages = {205--225},
  publisher = {{Wiley}},
  doi = {10.1002/9781118475331.ch10},
  isbn = {978-1-118-47532-4 978-1-118-47533-1},
  language = {en},
  file = {Human Computer Interaction\\Augmented Reality\\Paul, 2016 - Augmented Realities2.pdf}
}

@inproceedings{piekarski2001,
  title = {Tinmith-{{Metro}}: New Outdoor Techniques for Creating City Models with an Augmented Reality Wearable Computer},
  shorttitle = {Tinmith-{{Metro}}},
  booktitle = {Proceedings {{Fifth International Symposium}} on {{Wearable Computers}}},
  author = {Piekarski, W. and Thomas, B.H.},
  year = {2001},
  pages = {31--38},
  publisher = {{IEEE Comput. Soc}},
  address = {{Zurich, Switzerland}},
  doi = {10.1109/ISWC.2001.962093},
  abstract = {This paper presents new techniques for capturing and viewing on site 3D graphical models for large outdoor objects. Using an augmented reality wearable computer, we have developed a software system, known as TinmithMetro. Tinmith-Metro allows users to control a 3D constructive solid geometry modeller for building graphical objects of large physical artefacts, for example buildings, in the physical world. The 3D modeller is driven by a new user interface known as Tinmith-Hand, which allows the user to control the modeller using a set of pinch gloves and hand tracking. These techniques allow user to supply their AR renderers with models that would previously have to be captured with manual, time-consuming, and/or expensive methods.},
  isbn = {978-0-7695-1318-8},
  language = {en},
  annotation = {ZSCC: 0000199},
  file = {Human Computer Interaction\\Augmented Reality\\Piekarski and Thomas, 2001 - Tinmith-Metro.pdf}
}

@phdthesis{pirro2017,
  title = {Composing {{Interactions}}},
  author = {Pirr{\`o}, David},
  year = {2017},
  address = {{Austria}},
  language = {en},
  school = {Institute of Electronic Music and Acoustics, University of Music and Performing Arts Graz},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Computational Art\\Pirrò, 2017 - Composing Interactions.pdf}
}

@inproceedings{quinton2020,
  title = {Sonification of an Exoplanetary Atmosphere},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Quinton, Michael and McGregor, Iain and Benyon, David},
  year = {2020},
  series = {{{AM}} '20},
  pages = {191--198},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411117},
  abstract = {This study investigates the effectiveness of user design methods to create a sonification for an astronomer who analyses exoplanet meteorological data situated in habitable zones. Requirements about the astronomer's work, the dataset and how to sonify it utilising Grounded Theory were identified. Parameter mapping sonification was used to represent effective transiting radii measurements through subtractive synthesis and spatialization. The design was considered to be effective, allowing the instantaneous identification of a water feature overlooked on a visual graph, even when noise within the dataset overlapped the source signal. The results suggest that multiple parameter mappings provide richer auditory stimuli and semantic qualities in order to allow an improved understanding of the dataset.},
  isbn = {978-1-4503-7563-4},
  keywords = {astronomy,exoplanet atmospheres,grounded theory,parameter mapping sonification,sonification,user centred design},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Quinton et al., 2020 - Sonification of an exoplanetary atmosphere.pdf}
}

@article{ramo2012,
  title = {Digital {{Augmented Reality Audio Headset}}},
  author = {R{\"a}m{\"o}, Jussi and V{\"a}lim{\"a}ki, Vesa},
  year = {2012},
  journal = {Journal of Electrical and Computer Engineering},
  volume = {2012},
  pages = {1--13},
  issn = {2090-0147, 2090-0155},
  doi = {10.1155/2012/457374},
  abstract = {Augmented reality audio (ARA) combines virtual sound sources with the real sonic environment of the user. An ARA system can be realized with a headset containing binaural microphones. Ideally, the ARA headset should be acoustically transparent, that is, it should not cause audible modification to the surrounding sound. A practical implementation of an ARA mixer requires a low-latency headphone reproduction system with additional equalization to compensate for the attenuation and the modified ear canal resonances caused by the headphones. This paper proposes digital IIR filters to realize the required equalization and evaluates a real-time prototype ARA system. Measurements show that the throughput latency of the digital prototype ARA system can be less than 1.4\,ms, which is sufficiently small in practice. When the direct and processed sounds are combined in the ear, a comb filtering effect is brought about and appears as notches in the frequency response. The comb filter effect in speech and music signals was studied in a listening test and it was found to be inaudible when the attenuation is 20\,dB. Insert ARA headphones have a sufficient attenuation at frequencies above about 1\,kHz. The proposed digital ARA system enables several immersive audio applications, such as a virtual audio tourist guide and audio teleconferencing.},
  language = {en},
  annotation = {ZSCC: 0000038},
  file = {Human Computer Interaction\\Augmented Reality\\Rämö and Välimäki, 2012 - Digital Augmented Reality Audio Headset.pdf}
}

@phdthesis{ranjan2016,
  title = {{{3D Audio Reproduction}}: {{Natural Augmented Reality Headset And Next Generation Entertainment System Using Wave Field Synthesis}}},
  author = {Ranjan, Rishabh},
  year = {2016},
  language = {en},
  school = {Nanyang Technological University},
  annotation = {ZSCC: 0000004},
  file = {Human Computer Interaction\\Augmented Reality\\Ranjan, 2016 - 3D Audio Reproduction.pdf}
}

@article{raskar1998,
  title = {Spatially {{Augmented Reality}}},
  author = {Raskar, Ramesh and Welch, Greg and Fuchs, Henry},
  year = {1998},
  month = sep,
  pages = {8},
  abstract = {To create an effective illusion of virtual objects coexisting with the real world, see-through HMD-based Augmented Reality techniques supplement the user's view with images of virtual objects. We introduce here a new paradigm, Spatially Augmented Reality (SAR), where virtual objects are rendered directly within or on the user's physical space.},
  language = {en},
  file = {Human Computer Interaction\\Augmented Reality\\Raskar et al., 1998 - Spatially Augmented Reality.pdf}
}

@article{raskin2005,
  title = {Comments Are {{More Important}} than {{Code}}},
  author = {Raskin, Jef},
  year = {2005},
  month = mar,
  journal = {Queue},
  volume = {3},
  number = {2},
  pages = {64--65},
  issn = {1542-7730, 1542-7749},
  doi = {10.1145/1053331.1053354},
  language = {en},
  annotation = {ZSCC: 0000042},
  file = {Arts & Humanities\\Media Studies\\Raskin, 2005 - Comments are More Important than Code.pdf}
}

@misc{ray2018,
  title = {Brian {{Eno}} and {{Peter Chilvers}} Create `Quite Magical' Flower Garden of Sound in {{Amsterdam}} with `{{Bloom}}: {{Open Space}}' - {{Stories}}},
  author = {Ray, Susanna},
  year = {2018},
  month = feb,
  urldate = {2020-01-10},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {..\\..\\Zotero\\storage\\6CWBQ7NR\\brian-eno-peter-chilvers-create-quite-magical-flower-garden-sound-amsterdam-bloom-open-space.html},
  url = {https://news.microsoft.com/features/brian-eno-peter-chilvers-create-quite-magical-flower-garden-sound-amsterdam-bloom-open-space/}
}

@incollection{rhodes2018,
  title = {Augmented {{Reality}} in {{Art}}: {{Aesthetics}} and {{Material}} for {{Expression}}},
  shorttitle = {Augmented {{Reality}} in {{Art}}},
  booktitle = {Augmented {{Reality Art}}},
  author = {Rhodes, Geoffrey Alan},
  editor = {Geroimenko, Vladimir},
  year = {2018},
  pages = {163--172},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-69932-5_7},
  isbn = {978-3-319-69931-8 978-3-319-69932-5},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Rhodes, 2018 - Augmented Reality in Art.pdf}
}

@article{robinett1991,
  title = {A {{Computational Model}} for the {{Stereoscopic Optics}} of a {{Head}}-{{Mounted Display}}},
  author = {Robinett, Warren and Rolland, Jannick P},
  year = {1991},
  pages = {21},
  abstract = {For stereoscopic photography or telepresence, orthostereoscopy occurs when the perceived size, shape, and relative position of objects in the three-dimensional scene being viewed match those of the physical objects in front of the camera. In Virtual Reality, the simulated scene has no physical counterpart, so orthostereoscopy must be defined in this case as constancy, as the head moves around, of the perceived size, shape and relative positions of the simulated objects.},
  language = {en},
  annotation = {ZSCC: 0000003},
  file = {Human Computer Interaction\\Augmented Reality\\Robinett and Rolland, 1991 - A Computational Model for the Stereoscopic Optics of a Head-Mounted Display.pdf}
}

@article{robinett1992,
  title = {Synthetic Experience: {{A}} Proposed Taxonomy},
  author = {Robinett, Warren},
  year = {1992},
  month = jan,
  journal = {Presence: Teleoper. Virtual Environ.},
  volume = {1},
  number = {2},
  pages = {229--247},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  issn = {1054-7460},
  abstract = {A taxonomy is proposed to classify all varieties of technologically mediated experience. This includes virtual reality and teleoperation, and also earlier devices such as the microscope and telephone. The model of mediated interaction assumes a sensor-display link from the world to the human, and an action-actuator link going back from the human to the world, with the mediating technology transforming the transmitted experience in some way. The taxonomy is used to classify a number of example systems.Two taxonomies proposed earlier are compared with the ideas presented in this paper. Then the long-term prospects of this field are speculated on, ignoring constraints of cost, effort, or time to develop. Finally, the ultimate limits of synthetic experience are discussed, which derive from properties of the physical universe and the human neural apparatus.},
  issue_date = {Spring 1992},
  annotation = {ZSCC: 0000121},
  file = {Human Computer Interaction\\Augmented Reality\\Robinett, 1992 - Synthetic experience.pdf}
}

@misc{rode2020,
  title = {{{SoundField}} by {{R\O DE Plugin}}},
  author = {R{\O}DE},
  year = {2020},
  urldate = {2020-05-25},
  file = {..\\..\\Zotero\\storage\\KISLRKDL\\soundfieldplugin.html},
  url = {https://www.rode.com/soundfieldplugin}
}

@inproceedings{rodger2020,
  title = {What {{Makes}} a {{Good Musical Instrument}}? {{A Matter}} of {{Processes}}, {{Ecologies}} and {{Specificities}}},
  booktitle = {Proceedings of the {{International Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Rodger, Matthew and Stapleton, Paul and {van Walstijn}, Maarten and Ortiz, Miguel and Pardue, Laurel},
  year = {2020},
  pages = {6},
  address = {{Birmingham, UK}},
  abstract = {Understanding the question of what makes a good musical instrument raises several conceptual challenges. Researchers have regularly adopted tools from traditional HCI as a framework to address this issue, in which instrumental musical activities are taken to comprise a device and a user, and should be evaluated as such. We argue that this approach is not equipped to fully address the conceptual issues raised by this question. It is worth reflecting on what exactly an instrument is, and how instruments contribute toward meaningful musical experiences. Based on a theoretical framework that incorporates ideas from ecological psychology, enactivism, and phenomenology, we propose an alternative approach to studying musical instruments. According to this approach, instruments are better understood in terms of processes rather than as devices, while musicians are not users, but rather agents in musical ecologies. A consequence of this reframing is that any evaluations of instruments, if warranted, should align with the specificities of the relevant processes and ecologies concerned. We present an outline of this argument and conclude with a description of a current research project to illustrate how our approach can shape the design and performance of a musical instrument in-progress.},
  language = {en},
  file = {Philosophy\\Extended Cognition\\Rodger et al., 2020 - What Makes a Good Musical Instrument.pdf}
}

@book{rodgers,
  title = {Pink {{Noises}}: {{Women}} on {{Electronic Music}} and {{Sound}}},
  author = {Rodgers, Tara},
  language = {en},
  file = {Arts & Humanities\\Computational Art\\Rodgers, Pink Noises.pdf}
}

@article{rolland2000,
  title = {Optical {{Versus Video See}}-{{Through Head}}-{{Mounted Displays}} in {{Medical Visualization}}},
  author = {Rolland, Jannick P. and Fuchs, Henry},
  year = {2000},
  month = jun,
  journal = {Presence: Teleoperators and Virtual Environments},
  volume = {9},
  number = {3},
  pages = {287--309},
  issn = {1054-7460},
  doi = {10.1162/105474600566808},
  abstract = {We compare two technological approaches to augmented reality for 3-D medical visualization: optical and video see-through devices. We provide a context to discuss the technology by reviewing several medical applications of augmented-reality research efforts driven by real needs in the medical field, both in the United States and in Europe. We then discuss the issues for each approach, optical versus video, from both a technology and human-factor point of view. Finally, we point to potentially promising future developments of such devices including eye tracking and multifocus planes capabilities, as well as hybrid optical/video technology.},
  language = {en},
  annotation = {ZSCC: 0000384},
  file = {Human Computer Interaction\\Augmented Reality\\Rolland and Fuchs, 2000 - Optical Versus Video See-Through Head-Mounted Displays in Medical Visualization.pdf}
}

@inproceedings{rompapas2020,
  title = {Project {{Esky}}: {{Enabling High Fidelity Augmented Reality}} on an {{Open Source Platform}}},
  booktitle = {{{ISS}} '20: {{Proceedings}} of the 2020 {{ACM International Conference}} on {{Interactive Surfaces}} and {{Spaces}}},
  author = {Rompapas, Damien Constantine and Quiros, Daniel Flores and Rodda, Charlton and Brown, Bryan Christopher and Zerkin, Noah Benjamin and Cassinelli, Alvaro},
  year = {2020},
  month = nov,
  address = {{Lisbon, Portugal}},
  url = {https://doi.org/10.1145/3380867.3426220},
  urldate = {2020-10-14},
  file = {Human Computer Interaction\\Augmented Reality\\Rompapas et al., 2020 - Project Esky.pdf}
}

@inproceedings{rosenberg1993,
  title = {Virtual Fixtures: {{Perceptual}} Tools for Telerobotic Manipulation},
  shorttitle = {Virtual Fixtures},
  booktitle = {Proceedings of {{IEEE Virtual Reality Annual International Symposium}}},
  author = {Rosenberg, Louis},
  year = {1993},
  pages = {76--82},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/VRAIS.1993.380795},
  isbn = {978-0-7803-1363-7},
  annotation = {ZSCC: 0000658},
  file = {Human Computer Interaction\\Augmented Reality\\Rosenberg, 1993 - Virtual fixtures.pdf}
}

@book{ross1999,
  ids = {rossDigitalArchaeologyRescuing1999},
  title = {Digital Archaeology: Rescuing Neglected and Damaged Data Resources: A {{JISC}}/{{NPO}} Study with the {{Electronic Libraries}} ({{eLib}}) {{Programme}} on the Preservation of Electronic Materials},
  shorttitle = {Digital Archaeology},
  author = {Ross, Seamus and Gow, Ann},
  year = {1999},
  series = {Electronic Libraries Programme Studies {{P}}},
  number = {2},
  publisher = {{Library Information Technology Centre}},
  address = {{London}},
  isbn = {978-1-900508-51-3},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: 248949588},
  file = {Arts & Humanities\\Media Studies\\Ross and Gow, 1999 - Digital archaeology.pdf}
}

@article{ross2005,
  ids = {rossNewMediaArts2005},
  title = {New {{Media Arts Hybridity}}: {{The Vases}} ({{Dis}})Communicants {{Between Art}}, {{Affective Science}} and {{AR Technology}}},
  shorttitle = {New {{Media Arts Hybridity}}},
  author = {Ross, Christine},
  year = {2005},
  month = nov,
  journal = {Convergence: The International Journal of Research into New Media Technologies},
  volume = {11},
  number = {4},
  pages = {32--42},
  issn = {1354-8565, 1748-7382},
  doi = {10.1177//1354856505061051},
  abstract = {Following Annie Coombes's and Avtar Brah's (authors of Hybridity and its Discontents: Politics, Science, Culture, 2000) request that we not merely apply but in fact historicise hybridity, and arguing that the art and science explorations of new media art have produced some of the strongest new media hybridities to date, the author focuses on one of the important fields of investigation currently linking media art, science and technology: augmented reality or what should be called augmented perception of time and space. This aesthetic field of investigation has led to a reassessment of representation, one that is not without (1) sharing some of the fundamental concerns of current neuroscientific investigation of mental processes and (2) questioning the image/real continuum principle at the core of recent augmented reality technology research. The article examines media artist Bill Viola's The Passions series (2000\textendash 2001) to contend that new media's original contribution to the practice of hybridity lies in the interaction that it both articulates and encourages with affective sciences, an interaction that redefines representation as an approximation, a facilitator \textendash{} a projection screen for complex mental processes.},
  language = {en},
  annotation = {ZSCC: 0000010},
  file = {Human Computer Interaction\\Augmented Reality\\Ross, 2005 - New Media Arts Hybridity.pdf}
}

@article{roy2016,
  title = {Inside Sonicity},
  author = {Roy, Elodie A.},
  year = {2016},
  month = jul,
  journal = {Sound Studies},
  volume = {2},
  number = {2},
  pages = {192--194},
  issn = {2055-1940},
  doi = {10.1080/20551940.2016.1245990}
}

@article{rumsey2002,
  title = {Spatial {{Quality Evaluation}} for {{Reproduced Sound}}: {{Terminology}}, {{Meaning}}, and a {{Scene}}-{{Based Paradigm}}},
  author = {Rumsey, Francis},
  year = {2002},
  journal = {J. Audio Eng. Soc.},
  volume = {50},
  number = {9},
  pages = {16},
  language = {en},
  annotation = {ZSCC: 0000313},
  file = {Arts & Humanities\\Computational Art\\Rumsey, 2002 - Spatial Quality Evaluation for Reproduced Sound.pdf}
}

@article{rusconi2006,
  title = {Spatial Representation of Pitch Height: The {{SMARC}} Effect},
  shorttitle = {Spatial Representation of Pitch Height},
  author = {Rusconi, Elena and Kwan, Bonnie and Giordano, Bruno and Umilta, Carlo and Butterworth, Brian},
  year = {2006},
  month = mar,
  journal = {Cognition},
  volume = {99},
  number = {2},
  pages = {113--129},
  issn = {00100277},
  doi = {10.1016/j.cognition.2005.01.004},
  abstract = {Through the preferential pairing of response positions to pitch, here we show that the internal representation of pitch height is spatial in nature and affects performance, especially in musically trained participants, when response alternatives are either vertically or horizontally aligned. The finding that our cognitive system maps pitch height onto an internal representation of space, which in turn affects motor performance even when this perceptual attribute is irrelevant to the task, extends previous studies on auditory perception and suggests an interesting analogy between music perception and mathematical cognition. Both the basic elements of mathematical cognition (i.e. numbers) and the basic elements of musical cognition (i.e. pitches), appear to be mapped onto a mental spatial representation in a way that affects motor performance.},
  language = {en},
  file = {Cognitive Science\\Psychophysics\\Rusconi et al., 2006 - Spatial representation of pitch height.pdf}
}

@article{ruth2019,
  title = {Secure {{Multi}}-{{User Content Sharing}} for {{Augmented Reality Applications}}},
  author = {Ruth, Kimberly and Kohno, Tadayoshi and Roesner, Franziska},
  year = {2019},
  pages = {19},
  abstract = {Augmented reality (AR), which overlays virtual content on top of the user's perception of the real world, has now begun to enter the consumer market. Besides smartphone platforms, early-stage head-mounted displays such as the Microsoft HoloLens are under active development. Many compelling uses of these technologies are multi-user: e.g., inperson collaborative tools, multiplayer gaming, and telepresence. While prior work on AR security and privacy has studied potential risks from AR applications, new risks will also arise among multiple human users. In this work, we explore the challenges that arise in designing secure and private content sharing for multi-user AR. We analyze representative application case studies and systematize design goals for security and functionality that a multi-user AR platform should support. We design an AR content sharing control module that achieves these goals and build a prototype implementation (ShareAR) for the HoloLens. This work builds foundations for secure and private multi-user AR interactions.},
  language = {en},
  file = {Human Computer Interaction\\Augmented Reality\\Ruth et al., 2019 - Secure Multi-User Content Sharing for Augmented Reality Applications.pdf}
}

@phdthesis{rutz2014,
  title = {Tracing the {{Compositional Process}}. {{Sound}} Art That Rewrites Its Own Past: Formation, Praxis and a Computer Framework},
  author = {Rutz, Hans Holger},
  year = {2014},
  url = {https://pearl.plymouth.ac.uk/bitstream/handle/10026.1/3116/2014rutz10254321phd.pdf?sequence=6&isAllowed=y},
  urldate = {2020-06-11},
  school = {Plymouth University},
  file = {Arts & Humanities\\Computational Art\\Rutz, 2014 - Tracing the Compositional Process.pdf}
}

@article{rutz2016,
  title = {Agency and {{Algorithms}}},
  author = {Rutz, Hanns Holger},
  year = {2016},
  month = nov,
  journal = {Journal of Science and Technology of the Arts},
  volume = {8},
  number = {1},
  pages = {73},
  issn = {1646-9798},
  doi = {10.7559/citarj.v8i1.223},
  abstract = {Although the concept of algorithms has been established a long time ago, their current topicality indicates a shift in the discourse. Classical definitions based on logic seem to be inadequate to describe their aesthetic capabilities. New approaches stress their involvement in material practices as well as their incompleteness. Algorithmic aesthetics can no longer be tied to the static analysis of programs, but must take into account the dynamic and experimental nature of coding practices. It is suggested that the aesthetic objects thus produced articulate something that could be called algorithmicity or the space of algorithmic agency. This is the space or the medium \textendash following Luhmann's form/medium distinction \textendash{} where human and machine undergo mutual incursions. In the resulting coupled ``extimate'' writing process, human initiative and algorithmic speculation cannot be clearly divided out any longer. An observation is attempted of defining aspects of such a medium by drawing a trajectory across a number of sound pieces. The operation of exchange between form and medium I call reconfiguration and it is indicated by this trajectory.},
  language = {en},
  annotation = {ZSCC: 0000004},
  file = {Arts & Humanities\\Computational Art\\Rutz, 2016 - Agency and Algorithms.pdf}
}

@book{salen2003,
  title = {Rules of Play: Game Design Fundamentals},
  shorttitle = {Rules of Play},
  author = {Salen, Katie and Zimmerman, Eric},
  year = {2003},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-24045-1},
  lccn = {QA76.76.C672 S25 2003},
  keywords = {Computer games,Design,Programming}
}

@article{samanci2014,
  title = {Embodied Site-Specific Animation},
  author = {Samanci, Ozge},
  year = {2014},
  month = feb,
  journal = {Convergence: The International Journal of Research into New Media Technologies},
  volume = {20},
  number = {1},
  pages = {14--24},
  issn = {1354-8565, 1748-7382},
  doi = {10.1177/1354856513514335},
  abstract = {Embodied site-specific animation is one of the ways of expanding the conventions of animation and rethinking the animation medium in the digital context. On the air is an interactive installation based on embodied site-specific animation. Existing installations that are based on embodied site-specific animations rely on abstract visualizations and do not attempt to tell a story or portray characters. I am exploring the design strategies and new meaning-making opportunities that arise with the use of embodied site-specific animation for mixed reality art projects.},
  language = {en},
  annotation = {ZSCC: 0000003},
  file = {Human Computer Interaction\\Augmented Reality\\Samanci, 2014 - Embodied site-specific animation.pdf}
}

@inproceedings{sardana2020,
  title = {Perception of Spatial Data Properties in an Immersive Multi-Layered Auditory Environment},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Sardana, Disha and Joo, Woohun and Bukvic, Ivica Ico and Earle, Gregory},
  year = {2020},
  series = {{{AM}} '20},
  pages = {30--37},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411134},
  abstract = {We present a study of spatial sonification of multidimensional data using a spatial mask and an immersive high-density loudspeaker array. The study participants are asked to identify edges and perceived center of 2D shapes projected across the perimeter of an exocentric environment. The results show that the phase modulation technique results in less accurate user responses than the amplitude modulation or combined modulation techniques. No significant differences are found between stationary and mobile-user scenarios when comparing the angular miss distances of the perceived center of sonified shapes, but significant differences are identified in locating their left and top edges. Further research is warranted to determine why properties of some shapes are easier to pinpoint than others, and how sonification may be improved to minimize such discrepancies.},
  isbn = {978-1-4503-7563-4},
  keywords = {data sonification,immersive environments,perception,spatial audio},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Sardana et al., 2020 - Perception of spatial data properties in an immersive multi-layered auditory.pdf}
}

@book{sathian2019,
  title = {Multisensory Perception: From Laboratory to Clinic},
  shorttitle = {Multisensory Perception},
  editor = {Sathian, K. and Ramachandran, V S},
  year = {2019},
  edition = {First},
  publisher = {{Elsevier}},
  address = {{San Deigo}},
  isbn = {978-0-12-812492-5},
  language = {en},
  annotation = {ZSCC: 0000008},
  file = {Cognitive Science\\Multisensory Integration\\Sathian and Ramachandran, 2019 - Multisensory perception.pdf}
}

@inproceedings{satoh2001,
  title = {A Hybrid Registration Method for Outdoor Augmented Reality},
  booktitle = {Proceedings {{IEEE}} and {{ACM International Symposium}} on {{Augmented Reality}}},
  author = {Satoh, K. and Anabuki, M. and Yamamoto, H. and Tamura, H.},
  year = {2001},
  pages = {67--76},
  publisher = {{IEEE Comput. Soc}},
  address = {{New York, NY, USA}},
  doi = {10.1109/ISAR.2001.970516},
  abstract = {In this paper, a registration method for outdoor wearable AR systems is described. Our approach is based on using a high precision gyroscope, which can measure 3DOF angle of head direction accurately, but with some drift error. We solved the drift problem with a vision-based drift compensation algorithm, which tracks natural features in the outdoor environment as landmarks from images captured by a camera on an HMD. This paper first describes the detail of the vision-based drift compensation method. Then, a calibration method for the orientation sensor is proposed. Finally, using results from an actual wearable AR system, a comparison of registration error with and without vision-based drift compensation demonstrates the feasibility of the proposed method.},
  isbn = {978-0-7695-1375-1},
  language = {en},
  annotation = {ZSCC: 0000111},
  file = {..\\..\\Zotero\\storage\\9HHWAVS8\\Satoh et al. - 2001 - A hybrid registration method for outdoor augmented.pdf}
}

@inproceedings{schacher2006,
  title = {Ambisonics {{Spatialization Tools}} for {{Max}}/{{MSP}}},
  booktitle = {Proceedings of the 2006 {{International Computer Music Conference}}},
  author = {Schacher, Jan and Kocher, Philippe},
  year = {2006},
  volume = {2006},
  pages = {274--277},
  publisher = {{Michigan Publishing}},
  address = {{New Orleans, Louisiana, USA}},
  url = {http://decoy.iki.fi/dsound/ambisonic/motherlode/source/ICST_Ambisonics_ICMC2006.pdf},
  urldate = {2020-05-25},
  file = {Human Computer Interaction\\Audio Interfacing\\Schacher and Kocher, 2006 - Ambisonics Spatialization Tools for Max-MSP.pdf}
}

@book{schmidthorning2015,
  title = {Chasing Sound: Technology, Culture, and the Art of Studio Recording from {{Edison}} to the {{LP}}},
  shorttitle = {Chasing Sound},
  author = {Schmidt Horning, Susan},
  year = {2015},
  month = dec,
  series = {Studies in Industry and Society},
  isbn = {978-1-4214-1848-3},
  language = {eng},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Musicology\\Schmidt Horning, 2015 - Chasing sound.pdf}
}

@inproceedings{schraffenberger2015,
  title = {Sonically {{Tangible Objects}}},
  booktitle = {{{xCoAx}} 2015: {{Proceedings}} of the {{Third Conference}} on {{Computation}}, {{Communication}}, {{Aesthetics}} and {{X}}.},
  author = {Schraffenberger, Hanna and {van der Heide}, Edwin},
  year = {2015},
  pages = {233--248},
  address = {{Glasgow, Scotland}},
  file = {Human Computer Interaction\\Augmented Reality\\Schraffenberger and van der Heide, 2015 - Sonically Tangible Objects.pdf}
}

@inproceedings{schraffenberger2016,
  title = {Multimodal Augmented Reality: The Norm Rather than the Exception},
  shorttitle = {Multimodal Augmented Reality},
  booktitle = {Proceedings of the 2016 Workshop on {{Multimodal Virtual}} and {{Augmented Reality}} - {{MVAR}} '16},
  author = {Schraffenberger, Hanna and {van der Heide}, Edwin},
  year = {2016},
  pages = {1--6},
  publisher = {{ACM Press}},
  address = {{Tokyo, Japan}},
  doi = {10.1145/3001959.3001960},
  abstract = {Augmented reality (AR) is commonly seen as a technology that overlays virtual imagery onto a participant's view of the world. In line with this, most AR research is focused on what we see. In this paper, we challenge this focus on vision and make a case for an experience-focused and modalitiesencompassing understanding of AR. We argue that multimodality in AR is the norm rather than the exception, as AR environments consist of both virtual content and our real, physical, multimodal world. We explore the role multimodal and non-visual aspects of our physical reality can play when creating AR scenarios and the possibilities and challenges that emerge when approaching AR from a modalitiesencompassing perspective.},
  isbn = {978-1-4503-4559-0},
  language = {en},
  annotation = {ZSCC: 0000007},
  file = {Human Computer Interaction\\Augmented Reality\\Schraffenberger and van der Heide, 2016 - Multimodal augmented reality.pdf}
}

@phdthesis{schraffenberger2018,
  title = {Arguably Augmented Reality: Relationships between the Virtual and the Real},
  shorttitle = {Arguably Augmented Reality},
  author = {Schraffenberger, Hanna},
  year = {2018},
  language = {en},
  school = {University of Leiden},
  annotation = {ZSCC: 0000001  OCLC: 1082194193},
  file = {Human Computer Interaction\\Augmented Reality\\Schraffenberger, 2018 - Arguably augmented reality.pdf}
}

@incollection{seah2015,
  title = {Need for {{Touch}} in {{Human Space Exploration}}: {{Towards}} the {{Design}} of a {{Morphing Haptic Glove}} \textendash{} {{ExoSkin}}},
  shorttitle = {Need for {{Touch}} in {{Human Space Exploration}}},
  booktitle = {Human-{{Computer Interaction}} \textendash{} {{INTERACT}} 2015},
  author = {Seah, Sue Ann and Obrist, Marianna and Roudaut, Anne and Subramanian, Sriram},
  editor = {Abascal, Julio and Barbosa, Simone and Fetter, Mirko and Gross, Tom and Palanque, Philippe and Winckler, Marco},
  year = {2015},
  volume = {9299},
  pages = {18--36},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-22723-8_3},
  abstract = {The spacesuit, particularly the spacesuit glove, creates a barrier between astronauts and their environment. Motivated by the vision of facilitating full-body immersion for effortless space exploration, it is necessary to understand the sensory needs of astronauts during extra-vehicular activities (EVAs). In this paper, we present the outcomes from a two-week field study performed at the Mars Desert Research Station, a facility where crews carry out Marssimulated missions. We used a combination of methods (a haptic logbook, technology probes, and interviews) to investigate user needs for haptic feedback in EVAs in order to inform the design of a haptic glove. Our results contradict the common belief that a haptic technology should always convey as much information as possible, but should rather offer a controllable transfer. Based on these findings, we identified two main design requirements to enhance haptic feedback through the glove: (i) transfer of the shape and pressure features of haptic information and (ii) control of the amount of haptic information. We present the implementation of these design requirements in the form of the concept and first prototype of ExoSkin. ExoSkin is a morphing haptic feedback layer that augments spacesuit gloves by controlling the transfer of haptic information from the outside world onto the astronauts' skin.},
  isbn = {978-3-319-22722-1 978-3-319-22723-8},
  language = {en},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Seah et al., 2015 - Need for Touch in Human Space Exploration.pdf}
}

@misc{sennheiser2018,
  title = {Sennheiser {{AMBEO AR One Headphones}}},
  author = {Sennheiser},
  year = {2018},
  urldate = {2020-10-03},
  file = {..\\..\\Zotero\\storage\\477B7K3N\\shop.magicleap.com.html},
  url = {https://en-uk.sennheiser.com/ambeo-application-armr}
}

@article{serafin2016,
  title = {Virtual {{Reality Musical Instruments}}: {{State}} of the {{Art}}, {{Design Principles}}, and {{Future Directions}}},
  shorttitle = {Virtual {{Reality Musical Instruments}}},
  author = {Serafin, Stefania and Erkut, Cumhur and Kojs, Juraj and Nilsson, Niels C. and Nordahl, Rolf},
  year = {2016},
  month = sep,
  journal = {Computer Music Journal},
  volume = {40},
  number = {3},
  pages = {22--40},
  issn = {0148-9267, 1531-5169},
  doi = {10.1162/COMJ_a_00372},
  abstract = {The rapid development and availability of low-cost technologies have created a wide interest in virtual reality. In the field of computer music, the term ``virtual musical instruments'' has been used for a long time to describe software simulations, extensions of existing musical instruments, and ways to control them with new interfaces for musical expression. Virtual reality musical instruments (VRMIs) that include a simulated visual component delivered via a head-mounted display or other forms of immersive visualization have not yet received much attention. In this article, we present a field overview of VRMIs from the viewpoint of the performer. We propose nine design guidelines, describe evaluation methods, analyze case studies, and consider future challenges.},
  language = {en},
  annotation = {ZSCC: 0000063},
  file = {Human Computer Interaction\\Virtual Reality\\Serafin et al., 2016 - Virtual Reality Musical Instruments.pdf}
}

@article{seth2013,
  title = {Interoceptive Inference, Emotion, and the Embodied Self},
  author = {Seth, Anil K.},
  year = {2013},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {17},
  number = {11},
  pages = {565--573},
  issn = {13646613},
  doi = {10.1016/j.tics.2013.09.007},
  language = {en},
  annotation = {ZSCC: 0000839},
  file = {Cognitive Science\\Interoception\\Seth, 2013 - Interoceptive inference, emotion, and the embodied self.pdf}
}

@book{shafer1967,
  title = {Ear {{Cleaning}}: {{Notes}} for an {{Experimental Music Course}}},
  author = {Shafer, Raymond},
  year = {1967},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Musicology\\Shafer, 1967 - Ear Cleaning.pdf}
}

@article{sharma1998,
  title = {Toward Multimodal Human-Computer Interface},
  author = {Sharma, R. and Pavlovic, V.I. and Huang, T.S.},
  year = {1998},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {5},
  pages = {853--869},
  issn = {00189219},
  doi = {10.1109/5.664275},
  language = {en},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Sharma et al., 1998 - Toward multimodal human-computer interface.pdf}
}

@article{sharma2015,
  title = {Towards {{Understanding}} and {{Verbalising Spatial Sound Phenomena}} in {{Electronic Music}}},
  author = {Sharma, Gerriet K and Frank, Matthias and Zotter, Franz},
  year = {2015},
  abstract = {How do we describe spatial sound phenomena in electronic music? This paper is concerned with the questions whether the electronic music of today takes into account the perception of its audience at all, whether it is necessary to have a typology of electronic music sounds at all, and how to find or generate terms that could be helpful for composition and analysis of spatialized sound. Before going into detail about how to deal with spatial phenomena, it is valuable to review the main concepts of verbalization of sound from the last 100 years of sound-based music. Following this, approaches and methodologies are introduced to develop a specific terminology for a certain way of spatial sound projection within the framework of the artistic research project `Orchestrating Space by Icosahedral Loudspeaker' (OSIL). By this we intent to encourage the aesthetic discussion about spatial sound composition and therefore enlarge the compositional contingencies of this art.},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Computational Art\\Sharma et al., 2015 - Towards Understanding and Verbalising Spatial Sound Phenomena in Electronic.pdf}
}

@phdthesis{sharma2018,
  ids = {sharmaComposingSculpturalSound},
  title = {Composing with {{Sculptural Sound Phenomena}} in {{Computer Music Dissertation}}},
  author = {Sharma, Gerriet K},
  year = {2018},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Computational Art\\Sharma, 2018 - Composing with Sculptural Sound Phenomena in Computer Music Dissertation.pdf}
}

@inproceedings{sheffield2016,
  title = {The {{Haptic Capstans}}: {{Rotational Force Feedback}} for {{Music}} Using a {{FireFader Derivative Device}}},
  booktitle = {Proceedings of the {{International Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Sheffield, Eric and Berdahl, Edgar and Pfalz, Andrew},
  year = {2016},
  pages = {2},
  address = {{Brisbane, Australia}},
  doi = {10.5281/zenodo.1176124},
  abstract = {The Haptic Capstans are two rotational force-feedback knobs circumscribed by eye-catching LED rings. In this work, the Haptic Capstans are programmed using physical models in order to experiment with audio-visual-haptic interactions for music applications.},
  language = {en},
  annotation = {ZSCC: 0000002},
  file = {Human Computer Interaction\\Augmented Reality\\Shefﬁeld et al., 2016 - The Haptic Capstans.pdf}
}

@article{sheridan1992,
  title = {Musings on {{Telepresence}} and {{Virtual Presence}}},
  author = {Sheridan, Thomas B.},
  year = {1992},
  month = jan,
  journal = {Presence: Teleoperators and Virtual Environments},
  volume = {1},
  number = {1},
  pages = {120--126},
  issn = {1054-7460},
  doi = {10.1162/pres.1992.1.1.120},
  language = {en},
  annotation = {ZSCC: 0001681},
  file = {Human Computer Interaction\\Augmented Reality\\Sheridan, 1992 - Musings on Telepresence and Virtual Presence.pdf}
}

@article{shimojo2001,
  title = {Sensory Modalities Are Not Separate Modalities: Plasticity and Interactions},
  shorttitle = {Sensory Modalities Are Not Separate Modalities},
  author = {Shimojo, S},
  year = {2001},
  month = aug,
  journal = {Current Opinion in Neurobiology},
  volume = {11},
  number = {4},
  pages = {505--509},
  issn = {09594388},
  doi = {10.1016/S0959-4388(00)00241-5},
  language = {en},
  annotation = {ZSCC: 0000602},
  file = {Cognitive Science\\Multisensory Integration\\Shimojo, 2001 - Sensory modalities are not separate modalities.pdf}
}

@article{shivers1993,
  title = {{{BodyTalk}} and the {{BodyNet}}: {{A Personal Information Infrastructure}}},
  author = {Shivers, Olin},
  year = {1993},
  pages = {19},
  abstract = {The current evolution of personal information appliances, such as cellular telephones, personal digital assistants, and notebook computers, can be made more effective if re-structured into a personal network architecture. This architecture is based upon two central components: a hardware communications system, the BodyNet, and a common interface language, BodyTalk.},
  language = {en},
  annotation = {ZSCC: 0000031},
  file = {Human Computer Interaction\\Augmented Reality\\Shivers, 1993 - BodyTalk and the BodyNet.pdf}
}

@phdthesis{sjoberg2018,
  title = {Making Use of the Environmental Space in Augmented Reality},
  author = {Sjoberg, Jesper},
  year = {2018},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Human Computer Interaction\\Augmented Reality\\Sjoberg, 2018 - Making use of the environmental space in augmented reality.pdf}
}

@incollection{skwarek2018,
  title = {Augmented {{Reality Activism}}},
  booktitle = {Augmented {{Reality Art}}},
  author = {Skwarek, Mark},
  editor = {Geroimenko, Vladimir},
  year = {2018},
  pages = {3--40},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-69932-5_1},
  isbn = {978-3-319-69931-8 978-3-319-69932-5},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{smalley1997,
  ids = {smalleySpectromorphologyExplainingSoundshapes1997},
  title = {Spectromorphology: Explaining Sound-Shapes},
  shorttitle = {Spectromorphology},
  author = {Smalley, Denis},
  year = {1997},
  month = aug,
  journal = {Organised Sound},
  volume = {2},
  number = {2},
  pages = {107--126},
  issn = {13557718},
  doi = {10.1017/S1355771897009059},
  language = {en},
  annotation = {ZSCC: 0000825},
  file = {Arts & Humanities\\Computational Art\\Smalley, 1997 - Spectromorphology.pdf}
}

@article{smalley2007,
  ids = {smalleySpaceformAcousmaticImage2007,smalleySpaceformAcousmaticImage2007a},
  title = {Space-Form and the Acousmatic Image},
  author = {Smalley, Denis},
  year = {2007},
  month = apr,
  journal = {Organised Sound},
  volume = {12},
  number = {1},
  pages = {35--58},
  issn = {1355-7718, 1469-8153},
  doi = {10.1017/S1355771807001665},
  abstract = {Abstract             The analytical discussion of acousmatic music can benefit from being based on spatial concepts, and this article aims to provide a framework for investigation. A personal experience of soundscape listening is the starting point, and uncovers basic ideas relating to the disposition and behaviour of sounding content, and listening strategy. This enables the opening out of the discussion to include source-bonded sounds in general, giving particular consideration to how experience of sense modes other than the aural are implicated in our understanding of space, and in acousmatic listening. Attention then shifts to a source-bonded spatial model based on the production of space by the gestural activity of music performance, prior to focusing in more detail on acousmatic music, initially by delving into spectral space, where ideas about gravitation and diagonal forces are germane. This leads to concepts central to the structuring of perspectival space in relation to the vantage point of the listener. The final section considers a methodology for space-form investigation.},
  language = {en},
  annotation = {ZSCC: 0000310},
  file = {Arts & Humanities\\Computational Art\\Smalley, 2007 - Space-form and the acousmatic image.pdf}
}

@inproceedings{speicher2019,
  title = {What Is {{Mixed Reality}}?},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}  - {{CHI}} '19},
  author = {Speicher, Maximilian and Hall, Brian D. and Nebeling, Michael},
  year = {2019},
  pages = {1--15},
  publisher = {{ACM Press}},
  address = {{Glasgow, Scotland Uk}},
  doi = {10.1145/3290605.3300767},
  abstract = {What is Mixed Reality (MR)? To revisit this question given the many recent developments, we conducted interviews with ten AR/VR experts from academia and industry, as well as a literature survey of 68 papers. We find that, while there are prominent examples, there is no universally agreed on, one-size-fits-all definition of MR. Rather, we identified six partially competing notions from the literature and experts' responses. We then started to isolate the different aspects of reality relevant for MR experiences, going beyond the primarily visual notions and extending to audio, motion, haptics, taste, and smell. We distill our findings into a conceptual framework with seven dimensions to characterize MR applications in terms of the number of environments, number of users, level of immersion, level of virtuality, degree of interaction, input, and output. Our goal with this paper is to support classification and discussion of MR applications' design and provide a better means to researchers to contextualize their work within the increasingly fragmented MR landscape.},
  isbn = {978-1-4503-5970-2},
  language = {en},
  file = {Human Computer Interaction\\Mixed Reality\\Speicher et al., 2019 - What is Mixed Reality.pdf}
}

@article{spence2015,
  title = {Olfactory Dining: Designing for the Dominant Sense},
  shorttitle = {Olfactory Dining},
  author = {Spence, Charles and Youssef, Jozef},
  year = {2015},
  month = dec,
  journal = {Flavour},
  volume = {4},
  number = {1},
  pages = {32},
  issn = {2044-7248},
  doi = {10.1186/s13411-015-0042-0},
  abstract = {The majority of researchers agree that olfactory cues play a dominant role in our perception and enjoyment of the taste (or rather flavour) of food and drink. It is no surprise then that in recent years, a variety of modern (or dare we say it, modernist) solutions have been developed with the explicit aim of delivering an enhanced olfactory input to the diners/dishes served in the restaurant, and occasionally also in the home setting too. Such innovations include everything from aromatic cutlery and plateware through to the use of atomizers and dry ice. A few augmented reality (AR; i.e. an experience of a physical, real-world environment whose elements have been augmented, or supplemented, by computer-generated sensory input) solutions have also made their way out from well-funded technology labs, and scent-enabled plug-ins for mobile devices are slowly being commercialized. The latter could potentially be used to enhance the orthonasal olfactory component of our multisensory food experiences in the years to come. Ultimately, though, there is an important question here as to the authenticity of those food and flavour experiences that have been augmented/enhanced by aroma and fragrance cues that are not integral to the food or drink itself. It is this lack of authenticity that may, at least in your authors' humble opinion, limit the more widespread uptake of such a sense-by-sense approach to the contemporary construction of multisensory gastronomic experiences. The challenge, as always, remains to find the unique selling point (USP) of such approaches to olfactory stimulation, over and above their mere feasibility and inherent theatricality.},
  language = {en},
  annotation = {ZSCC: 0000018},
  file = {Human Computer Interaction\\Augmented Reality\\Spence and Youssef, 2015 - Olfactory dining.pdf}
}

@inproceedings{steil2019,
  title = {Privacy-Aware Eye Tracking Using Differential Privacy},
  booktitle = {Proceedings of the 11th {{ACM Symposium}} on {{Eye Tracking Research}} \& {{Applications}}},
  author = {Steil, Julian and Hagestedt, Inken and Huang, Michael Xuelin and Bulling, Andreas},
  year = {2019},
  month = jun,
  pages = {1--9},
  publisher = {{ACM}},
  address = {{Denver Colorado}},
  doi = {10.1145/3314111.3319915},
  abstract = {With eye tracking being increasingly integrated into virtual and augmented reality (VR/AR) head-mounted displays, preserving users' privacy is an ever more important, yet under-explored, topic in the eye tracking community. We report a large-scale online survey (N=124) on privacy aspects of eye tracking that provides the first comprehensive account of with whom, for which services, and to what extent users are willing to share their gaze data. Using these insights, we design a privacy-aware VR interface that uses differential privacy, which we evaluate on a new 20-participant dataset for two privacy sensitive tasks: We show that our method can prevent user re-identification and protect gender information while maintaining high performance for gaze-based document type classification. Our results highlight the privacy challenges particular to gaze data and demonstrate that differential privacy is a potential means to address them. Thus, this paper lays important foundations for future research on privacy-aware gaze interfaces.},
  isbn = {978-1-4503-6709-7},
  language = {en},
  file = {Human Computer Interaction\\Security & Privacy\\Steil et al., 2019 - Privacy-aware eye tracking using differential privacy.pdf}
}

@book{stern2013,
  title = {Interactive Art and Embodiment: The Implicit Body as Performance},
  shorttitle = {Interactive Art and Embodiment},
  author = {Stern, Nathaniel},
  year = {2013},
  publisher = {{Gylphi Limited}},
  address = {{Canterbury}},
  isbn = {978-1-78024-009-1 978-1-78024-010-7 978-1-78024-011-4},
  language = {eng},
  annotation = {ZSCC: 0000066  OCLC: 891741339}
}

@misc{subpac2020,
  title = {{{SUBPAC X1}}},
  author = {Subpac},
  year = {2020},
  journal = {SUBPAC},
  urldate = {2020-10-03},
  abstract = {EARLY BIRD PRE ORDER PRICE~ SHIPS FROM THE USA - CUSTOMS DUTIES INCLUDED IN SHIPPING. SUBPAC pre-orders are first come, first serve, with the first ones expected to ship by the end of the year. We wish we could give you an exact date but the global pandemic has meant that we have had to change our manufacturing and shi},
  language = {en},
  file = {..\\..\\Zotero\\storage\\DHVE9HUS\\subpac-x1.html},
  url = {https://uk.subpac.com/products/subpac-x1}
}

@article{suhr2018,
  title = {The {{Audience}} and {{Artist Interactivity}} in {{Augmented Reality Art}}: {{The Solo Exhibition}} on the {{{\emph{Flame}}}} {{Series}}},
  shorttitle = {The {{Audience}} and {{Artist Interactivity}} in {{Augmented Reality Art}}},
  author = {Suhr, H. Cecilia},
  year = {2018},
  month = may,
  journal = {Critical Arts},
  volume = {32},
  number = {3},
  pages = {111--125},
  issn = {0256-0046, 1992-6049},
  doi = {10.1080/02560046.2018.1493054},
  abstract = {Traditional art exhibitions are typically limited to artworks being viewed on the walls of galleries and museums. While walking around galleries, viewers often view artworks spontaneously within a matter of a few seconds or minutes. The act of viewing artworks can take many forms; one might quickly glance at the works, or stare at them, or intensely view the works as a whole group and/or in part. The frequent assumption is that the artist is communicating their vision from one direction. This paper explores how augmented reality art (AR) has affected viewing behaviours and norms, and the ways in which viewers experience traditional paintings, as well as new media arts. Based on Cecilia Suhr's invitational solo exhibition, Flame, at the Nameseoul University IANG Gallery in Seoul, Korea, in May 2017, the goal of this paper is threefold: 1) to unpack the conceptual framework of the Flame series by loosely drawing on Deleuze's notion of ``becoming''; 2) to explore audiences' viewing behaviours of AR art in relation to the inherent characteristics and ontology of the AR medium; and 3) to problematise how the AR medium affects the hegemonic tension between artist and audience in the dichotomisation of active/passive audiences.},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Human Computer Interaction\\Augmented Reality\\Suhr, 2018 - The Audience and Artist Interactivity in Augmented Reality Art.pdf}
}

@inproceedings{sutherland1965,
  title = {The {{Ultimate Display}}},
  booktitle = {Proceedings of the {{IFIP Congress}}},
  author = {Sutherland, Ivan},
  year = {1965},
  pages = {506--508},
  language = {en},
  annotation = {ZSCC: 0002210},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Sutherland, 1965 - The Ultimate Display.pdf}
}

@inproceedings{sutherland1968,
  title = {A Head-Mounted Three Dimensional Display},
  booktitle = {Proceedings of the {{December}} 9-11, 1968, {{Fall Joint Computer Conference}}},
  author = {Sutherland, Ivan},
  year = {1968},
  pages = {757--764},
  language = {en},
  annotation = {ZSCC: 0002841},
  file = {Human Computer Interaction\\Augmented Reality\\Sutherland, 1968 - A head-mounted three dimensional display.pdf}
}

@article{talsma2015,
  title = {Predictive Coding and Multisensory Integration: An Attentional Account of the Multisensory Mind},
  shorttitle = {Predictive Coding and Multisensory Integration},
  author = {Talsma, Durk},
  year = {2015},
  month = mar,
  journal = {Frontiers in Integrative Neuroscience},
  volume = {09},
  issn = {1662-5145},
  doi = {10.3389/fnint.2015.00019},
  abstract = {Multisensory integration involves a host of different cognitive processes, occurring at different stages of sensory processing. Here I argue that, despite recent insights suggesting that multisensory interactions can occur at very early latencies, the actual integration of individual sensory traces into an internally consistent mental representation is dependent on both top\textendash down and bottom\textendash up processes. Moreover, I argue that this integration is not limited to just sensory inputs, but that internal cognitive processes also shape the resulting mental representation. Studies showing that memory recall is affected by the initial multisensory context in which the stimuli were presented will be discussed, as well as several studies showing that mental imagery can affect multisensory illusions. This empirical evidence will be discussed from a predictive coding perspective, in which a central top\textendash down attentional process is proposed to play a central role in coordinating the integration of all these inputs into a coherent mental representation.},
  language = {en},
  annotation = {ZSCC: 0000110},
  file = {Cognitive Science\\Multisensory Integration\\Talsma, 2015 - Predictive coding and multisensory integration.pdf}
}

@inproceedings{tcha-tokey2016,
  title = {A Questionnaire to Measure the User Experience in Immersive Virtual Environments},
  booktitle = {Proceedings of the 2016 {{Virtual Reality International Conference}} on - {{VRIC}} '16},
  author = {{Tcha-Tokey}, Katy and {Loup-Escande}, Emilie and Christmann, Olivier and Richir, Simon},
  year = {2016},
  pages = {1--5},
  publisher = {{ACM Press}},
  address = {{Laval, France}},
  doi = {10.1145/2927929.2927955},
  isbn = {978-1-4503-4180-6},
  language = {en},
  annotation = {ZSCC: 0000021},
  file = {Human Computer Interaction\\Virtual Reality\\Tcha-Tokey et al., 2016 - A questionnaire to measure the user experience in immersive virtual environments.pdf}
}

@article{tcha-tokey2016a,
  title = {Proposition and {{Validation}} of a {{Questionnaire}} to {{Measure}} the {{User Experience}} in {{Immersive Virtual Environments}}},
  author = {{Tcha-Tokey}, Katy and Christmann, Olivier and {Loup-Escande}, Emilie and Richir, Simon},
  year = {2016},
  month = jan,
  journal = {International Journal of Virtual Reality},
  volume = {16},
  number = {1},
  pages = {33--48},
  issn = {1081-1451},
  doi = {10.20870/IJVR.2016.16.1.2880},
  abstract = {There are increasing new advances in Virtual Reality technologies as well as a rise in Immersive Virtual Environments research and in User eXperience research. Within this framework, we decided to address the overall user experience in Immersive virtual environments. Indeed, in our point of view, this topic is not fully dealt with in the scientific literature, neither in terms of user experience components nor in terms of user experience measurement methods. It is in this context that we conducted a study aiming at proposing and validating a unified questionnaire on User eXperience in Immersive Virtual Environment. Our questionnaire contains 10 scales measuring presence, engagement, immersion, flow, usability, skill, emotion, experience consequence, judgement and technology adoption. Scale construction was based on existing questionnaires. Our questionnaire was tested on 116 participants after they use the edutainment Virtual Environment ``Think and Shoot''. The number of participants allows us to assess the reliability and the sensitivity of our questionnaire. Results show that 9 out of 10 subscales and 68 out of 87 items are reliable as demonstrated by an internal consistency analysis with Cronbach's alpha and an item analysis. Findings also indicate that the scale scores from 6 subscales are considered normal distributed (e.g. presence) whereas the scale scores from 3 subscales are considered negatively skewed (e.g. skill).},
  language = {en},
  file = {Human Computer Interaction\\User Experience Design\\Tcha-Tokey et al., 2016 - Proposition and Validation of a Questionnaire to Measure the User Experience in.pdf}
}

@misc{thiel2011,
  title = {Invisible Istanbul},
  author = {Thiel, Tamiko and Kozar, Cem and {\"U}nal, I{\c s}{\i}l},
  year = {2011},
  month = nov,
  url = {https://web.archive.org/web/20111125101557/http://www.invisibleistanbul.org/},
  urldate = {2021-05-17},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {..\\..\\Zotero\\storage\\C7WGJ42D\\www.invisibleistanbul.org.html}
}

@incollection{thiel2018,
  title = {Critical {{Interventions}} into {{Canonical Spaces}}: {{Augmented Reality}} at the 2011 {{Venice}} and {{Istanbul Biennials}}},
  shorttitle = {Critical {{Interventions}} into {{Canonical Spaces}}},
  booktitle = {Augmented {{Reality Art}}},
  author = {Thiel, Tamiko},
  editor = {Geroimenko, Vladimir},
  year = {2018},
  pages = {41--72},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-69932-5_2},
  isbn = {978-3-319-69931-8 978-3-319-69932-5},
  annotation = {ZSCC: NoCitationData[s0]}
}

@inproceedings{tieben2011,
  title = {Curiosity and {{Interaction}}: Making People Curious through Interactive Systems},
  shorttitle = {Curiosity and {{Interaction}}},
  booktitle = {Proceedings of {{HCI}} 2011 {{The}} 25th {{BCS Conference}} on {{Human Computer Interaction}}},
  author = {Tieben, Rob and Bekker, Tilde and Schouten, Ben},
  year = {2011},
  month = jul,
  doi = {10.14236/ewic/HCI2011.66},
  language = {en},
  file = {Human Computer Interaction\\User Experience Design\\Tieben et al., 2011 - Curiosity and Interaction.pdf}
}

@inproceedings{tikander2008,
  title = {An {{Augmented Reality Audio Headset}}},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Digital Audio Effects}}},
  author = {Tikander, Miikka and Karjalainen, Matti and Riikonen, Ville},
  year = {2008},
  pages = {4},
  address = {{Espoo, Finland}},
  abstract = {Augmented reality audio (ARA) means mixing the natural sound environment with artificially created sound scenes. This requires that the perception of natural environment has to be preserved as well as possible, unless some modification to it is desired. A basic ARA headset consists of binaural microphones, an amplifier/mixer, and earphones feeding sound to the ear canals. All these components more or less change the perceived sound scene. In this paper we describe an ARA headset, equalization of its response, and particularly the results of a usability study. The usability was tested by subjects wearing the headset for relatively long periods in different environments of their everyday-life conditions. The goal was to find out what works well and what are the problems in lengthened use. It was found that acoustically the headset worked fine in most occasions when equalized individually or generically (averaged over several subjects). The main problems of usage were related to handling inconveniences and special environments.},
  language = {en},
  annotation = {ZSCC: 0000048},
  file = {Human Computer Interaction\\Augmented Reality\\Tikander et al., 2008 - An Augmented Reality Audio Headset.pdf}
}

@article{timmers2016,
  title = {Representation of Pitch in Horizontal Space and Its Dependence on Musical and Instrumental Experience.},
  author = {Timmers, Renee and Li, Shen},
  year = {2016},
  journal = {Psychomusicology: Music, Mind, and Brain},
  volume = {26},
  number = {2},
  pages = {139--148},
  issn = {2162-1535, 0275-3987},
  doi = {10.1037/pmu0000146},
  abstract = {Representation of pitch in horizontal space and its relationship to musical and instrumental experience was examined in three behavioral experiments. Each experiment investigated the influence of a task-irrelevant dimension (pitch or location) on the perception of a taskrelevant dimension (location or pitch, respectively). Sine tones with nine different pitches were presented from nine locations, and participants estimated the pitch or location of the stimuli. Experiment 1 showed an influence of the (task irrelevant) pitch of presented stimuli on the perceived location of the stimuli in musically experienced participants only. This influence increased with the degree of musical training of participants. No influence was found of presented location on the perception of pitch. Experiments 2 and 3 investigated the influence of instrumental expertise comparing the responses of a group of flutists with a group of pianists. An interaction with instrumental expertise was found only in Experiment 3, where participants played shortly on their respective instruments before doing the perceptual judgments. The experiments indicate that musical training in general influence the pitchlocation association, and pianistic experience in particular.},
  language = {en},
  file = {Cognitive Science\\Psychophysics\\Timmers and Li, 2016 - Representation of pitch in horizontal space and its dependence on musical and.pdf}
}

@misc{tonn2020,
  title = {Sensory {{Cartographies}}},
  author = {Tonn, Sissel Marie and Reus, Jonathan},
  year = {2020},
  urldate = {2020-10-03},
  file = {..\\..\\Zotero\\storage\\TCGJYW6N\\403184.html},
  url = {https://www.researchcatalogue.net/view/403183/403184}
}

@inproceedings{toppano2019,
  title = {Moving across {{Sonic Atmospheres}}},
  booktitle = {Proceedings of the 14th {{International Audio Mostly Conference}}: {{A Journey}} in {{Sound}}},
  author = {Toppano, Elio and Toppano, Sveva and Basiaco, Alessandro},
  year = {2019},
  month = sep,
  pages = {139--146},
  publisher = {{ACM}},
  address = {{Nottingham United Kingdom}},
  doi = {10.1145/3356590.3356612},
  abstract = {The concept of sonic atmosphere has become the focus of an increasing amount of attention in both academic and public forums, but scholars have developed diverging and overlapping definitions of the concept which threatens to inhibit our progress in understanding atmospheric phenomena. This paper draws on recent developments in the field of New Aesthetics and New Phenomenology. In particular, the research work highlights the role a sonic atmosphere has as a backdrop of the acoustic environment and the soundscape, and explores the relationships existing among these concepts. This provides us with a reference framework for studying movement through and between sonic atmospheres and to understand the possible relationships unfolding between an individual' s mood and the affective tonality and affordances of a sonic space. A case study exemplifies the application of the proposed conceptual framework in the field of urban design.},
  isbn = {978-1-4503-7297-8},
  language = {en},
  annotation = {ZSCC: 0000000},
  file = {Arts & Humanities\\Computational Art\\Toppano et al., 2019 - Moving across Sonic Atmospheres.pdf}
}

@inproceedings{turchet2018,
  title = {Smart {{Mandolin}}: Autobiographical Design, Implementation, Use Cases, and Lessons Learned},
  shorttitle = {Smart {{Mandolin}}},
  booktitle = {Proceedings of the {{Audio Mostly}} 2018 on {{Sound}} in {{Immersion}} and {{Emotion}}  - {{AM}}'18},
  author = {Turchet, Luca},
  year = {2018},
  pages = {1--7},
  publisher = {{ACM Press}},
  address = {{Wrexham, United Kingdom}},
  doi = {10.1145/3243274.3243280},
  abstract = {This paper presents the Smart Mandolin, an exemplar of the family of the so-called smart instruments. Developed according to the paradigms of autobiographical design, it consists of a conventional acoustic mandolin enhanced with different types of sensors, a microphone, a loudspeaker, wireless connectivity to both local networks and the Internet, and a low-latency audio processing board. Various implemented use cases are presented, which leverage the smart qualities of the instrument. These include the programming of the instrument via applications for smartphones and desktop computer, as well as the wireless control of devices enabling multimodal performances such as screen projecting visuals, smartphones, and tactile devices used by the audience. The paper concludes with an evaluation conducted by the author himself after extensive use, which pinpointed pros and cons of the instrument and provided a comparison with the Hyper-Mandolin, an instance of augmented instruments previously developed by the author.},
  isbn = {978-1-4503-6609-0},
  language = {en},
  file = {Arts & Humanities\\Computational Art\\Turchet, 2018 - Smart Mandolin.pdf}
}

@article{turk2014,
  ids = {turkMultimodalInteractionReview2014a},
  title = {Multimodal Interaction: {{A}} Review},
  shorttitle = {Multimodal Interaction},
  author = {Turk, Matthew},
  year = {2014},
  month = jan,
  journal = {Pattern Recognition Letters},
  volume = {36},
  pages = {189--195},
  issn = {01678655},
  doi = {10.1016/j.patrec.2013.07.003},
  abstract = {People naturally interact with the world multimodally, through both parallel and sequential use of multiple perceptual modalities. Multimodal human\textendash computer interaction has sought for decades to endow computers with similar capabilities, in order to provide more natural, powerful, and compelling interactive experiences. With the rapid advance in non-desktop computing generated by powerful mobile devices and affordable sensors in recent years, multimodal research that leverages speech, touch, vision, and gesture is on the rise. This paper provides a brief and personal review of some of the key aspects and issues in multimodal interaction, touching on the history, opportunities, and challenges of the area, especially in the area of multimodal integration. We review the question of early vs. late integration and find inspiration in recent evidence in biological sensory integration. Finally, we list challenges that lie ahead for research in multimodal human\textendash computer interaction.},
  language = {en},
  annotation = {ZSCC: 0000277},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Turk, 2014 - Multimodal interaction.pdf}
}

@incollection{tuters2015,
  title = {Through {{Glass Darkly}}: {{On Google}}'s {{Gnostic Governance}}},
  shorttitle = {Through {{Glass Darkly}}},
  booktitle = {Postdigital {{Aesthetics}}},
  author = {Tuters, Marc},
  editor = {Berry, David M. and Dieter, Michael},
  year = {2015},
  pages = {245--258},
  publisher = {{Palgrave Macmillan UK}},
  address = {{London}},
  doi = {10.1057/9781137437204_19},
  isbn = {978-1-349-49378-4 978-1-137-43720-4},
  language = {en},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {Arts & Humanities\\Media Studies\\Tuters, 2015 - Through Glass Darkly.pdf}
}

@article{ullmer2000,
  title = {Emerging Frameworks for Tangible User Interfaces},
  author = {Ullmer, B. and Ishii, H.},
  year = {2000},
  journal = {IBM Systems Journal},
  volume = {39},
  number = {3.4},
  pages = {915--931},
  issn = {0018-8670},
  doi = {10.1147/sj.393.0915},
  language = {en},
  annotation = {ZSCC: 0001473},
  file = {Human Computer Interaction\\User Interface Design\\Ullmer and Ishii, 2000 - Emerging frameworks for tangible user interfaces.pdf}
}

@misc{ultraleap2020,
  title = {Leap {{Motion Controller}}},
  author = {UltraLeap},
  year = {2020},
  urldate = {2020-05-25},
  abstract = {Small. Fast. Accurate. The LeapMotion Controller is an optical hand tracking module that captures the movements of your hands with unparalleled accuracy.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\QT3Y43VM\\leap-motion-controller.html},
  url = {https://www.ultraleap.com/product/leap-motion-controller/}
}

@misc{ultraleap2020a,
  title = {Touchscreens: {{Business}} as {{Usual Isn}}'t an {{Option}} | {{Ultraleap}}},
  shorttitle = {Touchscreens},
  author = {UltraLeap},
  year = {2020},
  urldate = {2020-11-19},
  abstract = {Covid-19 has made consumers hyper-aware of the hygiene risks of public touchscreens. How will contactless interfaces help retail meet this challenge?},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {..\\..\\Zotero\\storage\\5WMB9HGB\\contactless-technology.html},
  url = {https://www.ultraleap.com/company/news/blog/contactless-technology/}
}

@inproceedings{unander-scharin2014,
  title = {The Vocal Chorder: Empowering Opera Singers with a Large Interactive Instrument},
  shorttitle = {The Vocal Chorder},
  booktitle = {Proceedings of the 32nd Annual {{ACM}} Conference on {{Human}} Factors in Computing Systems - {{CHI}} '14},
  author = {{Unander-Scharin}, Carl and {Unander-Scharin}, {\AA}sa and H{\"o}{\"o}k, Kristina},
  year = {2014},
  pages = {1001--1010},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/2556288.2557050},
  abstract = {With The Vocal Chorder, a large interactive instrument to create accompaniment, opera singers can get more power over the performance. The device allows performers to interactively accompany themselves through pushing, leaning on and bending steel wires. The design was guided by the unique needs of the solo-singer, explored through autobiographical design and material explorations, some on stage, and later tested by other singers. We discuss how designing for opera and for the stage requires extraordinary durability and how opera performances can change with a bodilyoriented instrument such as The Vocal Chorder. Through a designerly exploration, we arrived at a device that offered (1) a tool for singers to take control over the rhythmical pace and overall artistic and aesthetic outcome of their performances, (2) an enriched sense of embodiment between their voice and the overall performance; and (3) a means to empower opera singers on stage.},
  isbn = {978-1-4503-2473-1},
  language = {en},
  file = {Arts & Humanities\\Computational Art\\Unander-Scharin et al., 2014 - The vocal chorder.pdf}
}

@misc{unitytechnologies2020,
  title = {{{Unity3D}}},
  author = {Unity Technologies},
  year = {2020},
  urldate = {2020-05-26},
  abstract = {New address, same Unity3d. Unity real-time development platform. Create 3D, 2D VR \& AR visualizations for Games, Auto, Transportation, Film, Animation, Architecture, Engineering \& more.,},
  language = {en},
  file = {..\\..\\Zotero\\storage\\W7Q2EUD6\\unity.com.html},
  url = {https://www.unity.com/}
}

@inproceedings{vallgarda2007,
  title = {Computational Composites},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}  - {{CHI}} '07},
  author = {Vallg{\aa}rda, Anna and Redstr{\"o}m, Johan},
  year = {2007},
  pages = {513--522},
  publisher = {{ACM Press}},
  address = {{San Jose, California, USA}},
  doi = {10.1145/1240624.1240706},
  abstract = {Computational composite is introduced as a new type of composite material. Arguing that this is not just a metaphorical maneuver, we provide an analysis of computational technology as material in design, which shows how computers share important characteristics with other materials used in design and architecture. We argue that the notion of computational composites provides a precise understanding of the computer as material, and of how computations need to be combined with other materials to come to expression as material. Besides working as an analysis of computers from a designer's point of view, the notion of computational composites may also provide a link for computer science and human-computer interaction to an increasingly rapid development and use of new materials in design and architecture.},
  isbn = {978-1-59593-593-9},
  language = {en},
  file = {Philosophy\\Materiality\\Vallgårda and Redström, 2007 - Computational composites.pdf}
}

@phdthesis{vallino1998,
  title = {Interactive {{Augmented Reality}}},
  author = {Vallino, James R},
  year = {1998},
  address = {{New York}},
  language = {en},
  school = {University of Rochester},
  annotation = {ZSCC: 0000233},
  file = {Human Computer Interaction\\Augmented Reality\\Vallino, 1998 - Interactive Augmented Reality.pdf}
}

@misc{valve2019,
  title = {Valve {{Index}}},
  author = {Valve},
  year = {2019},
  urldate = {2020-07-12},
  abstract = {Upgrade your experience.},
  language = {en},
  file = {..\\..\\Zotero\\storage\\FJ9PCXVP\\valveindex.html},
  url = {https://store.steampowered.com/valveindex}
}

@article{vankrevelen2010,
  title = {A {{Survey}} of {{Augmented Reality Technologies}}, {{Applications}} and {{Limitations}}},
  author = {Van Krevelen, D.W.F. and Poelman, R.},
  year = {2010},
  month = jan,
  journal = {International Journal of Virtual Reality},
  volume = {9},
  number = {2},
  pages = {1--20},
  issn = {1081-1451},
  doi = {10.20870/IJVR.2010.9.2.2767},
  abstract = {A Survey of Augmented Reality Technologies, Applications and Limitations},
  language = {en},
  annotation = {ZSCC: 0001587},
  file = {Human Computer Interaction\\Augmented Reality\\Van Krevelen and Poelman, 2010 - A Survey of Augmented Reality Technologies, Applications and Limitations.pdf}
}

@article{varese23,
  title = {The {{Liberation}} of {{Sound}}},
  author = {Varese, Edgard and {Wen-chung}, Chou},
  year = {23},
  journal = {Perspectives of New Music},
  volume = {5},
  number = {1},
  pages = {11},
  issn = {00316016},
  doi = {10.2307/832385},
  language = {en},
  annotation = {ZSCC: 0000380},
  file = {Arts & Humanities\\Musicology\\Varese and Wen-chung, 1966 - The Liberation of Sound.pdf}
}

@misc{varjo2019,
  title = {Varjo {{XR}}-1},
  author = {{Varjo}},
  year = {2019},
  journal = {Varjo.com},
  urldate = {2020-10-03},
  abstract = {Varjo XR-1 is the world's most advanced mixed reality headset for professionals, with photorealistic visual fidelity and ultra-low latency.},
  language = {en-US},
  file = {..\\..\\Zotero\\storage\\LQV4PK8K\\xr-1.html},
  url = {https://varjo.com/products/xr-1/}
}

@incollection{veal2012,
  title = {Starship {{Africa}}},
  booktitle = {The Sound Studies Reader},
  author = {Veal, Michael},
  editor = {Sterne, Jonathan},
  year = {2012},
  publisher = {{Routledge}},
  address = {{New York}},
  abstract = {"The Sound Studies Reader is a groundbreaking anthology blending recent work that self-consciously describes itself as 'sound studies' with earlier and lesser known scholarship on sound. The collection begins with an introduction to welcome novice readers to the field and acquaint them with key themes and concepts in sound studies. Individual section introductions give readers further background on the essays and an extensive up to date bibliography for further reading in 'sound studies' make this an original and accessible guide to the field"--},
  isbn = {978-0-415-77130-6 978-0-415-77131-3},
  lccn = {TK7881.4 .S684 2012},
  keywords = {Hearing,Listening,MUSIC / Recording \& Reproduction,Recording and reproducing History,Recording and reproducing Social aspects,SOCIAL SCIENCE / Media Studies,Sound},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Musicology\\Veal, 2012 - Starship Africa.pdf}
}

@misc{veenhof2010,
  title = {We {{AR}} in {{MoMA}}},
  author = {Veenhof, Sander and Skwarek, Mark},
  year = {2010},
  url = {http://sndrv.nl/moma/},
  annotation = {ZSCC: NoCitationData[s0]}
}

@inproceedings{vi2017,
  title = {{{TastyFloats}}: {{A Contactless Food Delivery System}}},
  shorttitle = {{{TastyFloats}}},
  booktitle = {Proceedings of the {{Interactive Surfaces}} and {{Spaces}} - {{ISS}} '17},
  author = {Vi, Chi Thanh and Marzo, Asier and Ablart, Damien and Memoli, Gianluca and Subramanian, Sriram and Drinkwater, Bruce and Obrist, Marianna},
  year = {2017},
  pages = {161--170},
  publisher = {{ACM Press}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1145/3132272.3134123},
  abstract = {We present two realizations of TastyFloats, a novel system that uses acoustic levitation to deliver food morsels to the users' tongue. To explore TastyFloats' associated design framework, we first address the technical challenges to successfully levitate and deliver different types of foods on the tongue. We then conduct a user study, assessing the effect of acoustic levitation on users' taste perception, comparing three basic taste stimuli (i.e., sweet, bitter and umami) and three volume sizes of droplets (5\textmu L, 10\textmu L and 20\textmu L). Our results show that users perceive sweet and umami easily, even in minimal quantities, whereas bitter is the least detectable taste, despite its typical association with an unpleasant taste experience. Our results are a first step towards the creation of new culinary experiences and innovative gustatory interfaces.},
  isbn = {978-1-4503-4691-7},
  language = {en},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Vi et al., 2017 - TastyFloats.pdf}
}

@article{vi2017a,
  title = {Not Just Seeing, but Also Feeling Art: {{Mid}}-Air Haptic Experiences Integrated in a Multisensory Art Exhibition},
  shorttitle = {Not Just Seeing, but Also Feeling Art},
  author = {Vi, Chi Thanh and Ablart, Damien and Gatti, Elia and Velasco, Carlos and Obrist, Marianna},
  year = {2017},
  month = dec,
  journal = {International Journal of Human-Computer Studies},
  volume = {108},
  pages = {1--14},
  issn = {10715819},
  doi = {10.1016/j.ijhcs.2017.06.004},
  abstract = {The use of the senses of vision and audition as interactive means has dominated the field of Human-Computer Interaction (HCI) for decades, even though nature has provided us with many more senses for perceiving and interacting with the world around us. That said, it has become attractive for HCI researchers and designers to harness touch, taste, and smell in interactive tasks and experience design. In this paper, we present research and design insights gained throughout an interdisciplinary collaboration on a six-week multisensory display \textendash{} Tate Sensorium \textendash{} exhibited at the Tate Britain art gallery in London, UK. This is a unique and first time case study on how to design art experiences whilst considering all the senses (i.e., vision, sound, touch, smell, and taste), in particular touch, which we exploited by capitalizing on a novel haptic technology, namely, mid-air haptics. We first describe the overall set up of Tate Sensorium and then move on to describing in detail the design process of the mid-air haptic feedback and its integration with sound for the Full Stop painting by John Latham (1961). This was the first time that mid-air haptic technology was used in a museum context over a prolonged period of time and integrated with sound to enhance the experience of visual art. As part of an interdisciplinary team of curators, sensory designers, sound artists, we selected a total of three variations of the mid-air haptic experience (i.e., haptic patterns), which were alternated at dedicated times throughout the six-week exhibition. We collected questionnaire-based feedback from 2500 visitors and conducted 50 interviews to gain quantitative and qualitative insights on visitors' experiences and emotional reactions. Whilst the questionnaire results are generally very positive with only a small variation of the visitors' arousal ratings across the three tactile experiences designed for the Full Stop painting, the interview data shed light on the differences in the visitors' subjective experiences. Our findings suggest multisensory designers and art curators can ensure a balance between surprising experiences versus the possibility of free exploration for visitors. In addition, participants expressed that experiencing art with the combination of mid-air haptic and sound was immersive and provided an up-lifting experience of touching without touch. We are convinced that the insights gained from this large-scale and real-world field exploration of multisensory experience design exploiting a new and emerging technology provide a solid starting point for the HCI community, creative industries, and art curators to think beyond conventional art experiences. Specifically, our work demonstrates how novel mid-air technology can make art more emotionally engaging and stimulating, especially abstract art that is often open to interpretation.},
  language = {en},
  annotation = {ZSCC: 0000049},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Vi et al., 2017 - Not just seeing, but also feeling art.pdf}
}

@inproceedings{vi2017b,
  title = {Gustatory Interface: The Challenges of `How' to Stimulate the Sense of Taste},
  shorttitle = {Gustatory Interface},
  booktitle = {Proceedings of the 2nd {{ACM SIGCHI International Workshop}} on {{Multisensory Approaches}} to {{Human}}-{{Food Interaction}} - {{MHFI}} 2017},
  author = {Vi, Chi Thanh and Ablart, Damien and Arthur, Daniel and Obrist, Marianna},
  year = {2017},
  pages = {29--33},
  publisher = {{ACM Press}},
  address = {{Glasgow, UK}},
  doi = {10.1145/3141788.3141794},
  abstract = {Gustatory interfaces have gained popularity in the field of humancomputer interaction, especially in the context of augmenting gaming and virtual reality experiences, but also in the context of food interaction design enabling the creation of new eating experiences. In this paper, we first review prior works on gustatory interfaces and particularly discuss them based on the use of either a chemical, electrical and/or thermal stimulation approach. We then present two concepts for gustatory interfaces that represent a more traditional delivery approach (using a mouthpiece) versus a novel approach that is based on principles of acoustic levitation (contactless delivery). We discuss the design opportunities around those two concepts in particular to overcome challenges of "how" to stimulate the sense of taste.},
  isbn = {978-1-4503-5556-8},
  language = {en},
  annotation = {ZSCC: 0000011},
  file = {Human Computer Interaction\\Multisensory Interfacing\\Vi et al., 2017 - Gustatory interface.pdf}
}

@inproceedings{vidyarthi2012,
  title = {Sonic {{Cradle}}: Designing for an Immersive Experience of Meditation by Connecting Respiration to Music},
  shorttitle = {{\emph{Sonic }}{{{\emph{Cradle}}}}},
  booktitle = {Proceedings of the {{Designing Interactive Systems Conference}} on - {{DIS}} '12},
  author = {Vidyarthi, Jay and Riecke, Bernhard E. and Gromala, Diane},
  year = {2012},
  pages = {408},
  publisher = {{ACM Press}},
  address = {{Newcastle Upon Tyne, United Kingdom}},
  doi = {10.1145/2317956.2318017},
  abstract = {Sonic Cradle is a chamber of complete darkness where users shape a peaceful soundscape using only their respiration. This interactive system was designed to foster a meditative experience by facilitating users' sense of immersion while following a specific attentional pattern characteristic of mindfulness. The goal of Sonic Cradle is twofold: first, to trigger the proven effects of mindfulness on stress, and second, to help teach and demystify the concept of meditation for users' long-term benefit. This paper presents the design phase of the project, starting by theoretically grounding the initial concept. We then discuss 15 co-design sessions which provided informal conceptual validation and led to several concrete design iterations aimed at balancing users' perceived sense of control. The presented approach to designing an interactive stress management system can be considered research through design, as it also resulted in a novel theoretical framework for the psychology of media immersion which has implications for a wide range of research areas.},
  isbn = {978-1-4503-1210-3},
  language = {en},
  file = {Human Computer Interaction\\Audio Interfacing\\Vidyarthi et al., 2012 - Sonic Cradle.pdf}
}

@article{vines2006,
  title = {Cross-Modal Interactions in the Perception of Musical Performance},
  author = {Vines, B and Krumhansl, C and Wanderley, M and Levitin, D},
  year = {2006},
  month = aug,
  journal = {Cognition},
  volume = {101},
  number = {1},
  pages = {80--113},
  issn = {00100277},
  doi = {10.1016/j.cognition.2005.09.003},
  language = {en},
  annotation = {ZSCC: 0000358},
  file = {Cognitive Science\\Multisensory Integration\\Vines et al., 2006 - Cross-modal interactions in the perception of musical performance.pdf}
}

@misc{vuforia2020,
  title = {Vuforia},
  author = {Vuforia},
  year = {2020},
  urldate = {2020-05-25},
  file = {..\\..\\Zotero\\storage\\J3N5DP6X\\overview.html},
  url = {https://library.vuforia.com/getting-started/overview.html}
}

@inproceedings{walther-hansen2020,
  title = {Don't Extend! {{Reduce}}! {{The}} Sound Approach to Reality},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {{Walther-Hansen}, Mads and {Grimshaw-Aagaard}, Mark},
  year = {2020},
  series = {{{AM}} '20},
  pages = {8--15},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3411111},
  abstract = {In this paper we propose a reduced reality concept of less-is-more that VR designers can use to create technological frameworks that reduce sensory overload and allow for better concentration and focus, less stress, and novel scenarios. We question the approach taken by scholars in the field of XR research, where the focus is typically to design and use technology that adds sensory information to the user's perceptual field and we address some of the confusion related to the typical uses of the term reality. To address the latter terminological muddle, we define reality as our conscious experience of the environment as emergent perception and we use this definition as the basis for a discussion of the role of sound in balancing sensory information and in the construction of a less cluttered and less stressful perceptual environments.},
  isbn = {978-1-4503-7563-4},
  keywords = {augmented reality,cognition,crossmodality,diminished reality,environment,extended reality,listening,presence,reduced reality,sound},
  annotation = {ZSCC: 0000000},
  file = {Human Computer Interaction\\Augmented Reality\\Walther-Hansen and Grimshaw-Aagaard, 2020 - Don't extend.pdf}
}

@article{walzer2017,
  title = {Independent Music Production: How Individuality, Technology and Creative Entrepreneurship Influence Contemporary Music Industry Practices},
  shorttitle = {Independent Music Production},
  author = {Walzer, Daniel A.},
  year = {2017},
  month = jan,
  journal = {Creative Industries Journal},
  volume = {10},
  number = {1},
  pages = {21--39},
  issn = {1751-0694, 1751-0708},
  doi = {10.1080/17510694.2016.1247626},
  language = {en}
}

@article{ward2010,
  title = {Visual Experiences in the Blind Induced by an Auditory Sensory Substitution Device},
  author = {Ward, Jamie and Meijer, Peter},
  year = {2010},
  month = mar,
  journal = {Consciousness and Cognition},
  volume = {19},
  number = {1},
  pages = {492--500},
  issn = {1053-8100},
  doi = {10.1016/j.concog.2009.10.006},
  abstract = {In this report, the phenomenology of two blind users of a sensory substitution device \textendash{} ``The vOICe'' \textendash{} that converts visual images to auditory signals is described. The users both report detailed visual phenomenology that developed within months of immersive use and has continued to evolve over a period of years. This visual phenomenology, although triggered through use of The vOICe, is likely to depend not only on online visualization of the auditory signal but also on the users' previous (albeit distant) experience of veridical vision (e.g. knowledge of shapes and visual perspective). Once established, the sensory substitution mapping between the auditory and visual domains is not confined to when the device is worn and, thus, may constitute an example of acquired synaesthesia.},
  language = {en},
  keywords = {Blind,Mental imagery,Sensory substitution,Synaesthesia/synesthesia,Visual consciousness,vOICe},
  annotation = {ZSCC: 0000202},
  file = {..\\..\\Zotero\\storage\\6I98CSUU\\S1053810009001718.html}
}

@article{ward2016,
  title = {Visual {{Cues}} to {{Reorient Attention}} from {{Head Mounted Displays}}},
  author = {Ward, Matthew and Barde, Amit and Russell, Paul N. and Billinghurst, Mark and Helton, William S.},
  year = {2016},
  month = sep,
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume = {60},
  number = {1},
  pages = {1574--1578},
  issn = {1541-9312},
  doi = {10.1177/1541931213601363},
  language = {en},
  annotation = {ZSCC: 0000005},
  file = {Human Computer Interaction\\Augmented Reality\\Ward et al., 2016 - Visual Cues to Reorient Attention from Head Mounted Displays.pdf}
}

@inproceedings{waters2007,
  title = {Performance {{Ecosystems}}: {{Ecological}} Approaches to Musical Interaction},
  booktitle = {Electroacoustic {{Music Studies Network EMS}}-07 {{Proceedings}}},
  author = {Waters, Simon},
  year = {2007},
  pages = {20},
  address = {{DeMonfort University, Leicester}},
  url = {http://www.ems-network.org/IMG/pdf_WatersEMS07.pdf},
  urldate = {2021-07-13},
  abstract = {Music is understood as a dynamical complex of interacting situated embodied behaviours. These behaviours may be physical or virtual, composed or emergent, or of a time scale such that they figure as constraints or constructs. All interact in the same space by a process of mutual modelling, redescription, and emergent restructuring.},
  file = {Arts & Humanities\\Computational Art\\Waters, 2007 - Performance Ecosystems.pdf}
}

@article{weis2016,
  title = {{{SNARC}} (Spatial\textendash Numerical Association of Response Codes) Meets {{SPARC}} (Spatial\textendash Pitch Association of Response Codes): {{Automaticity}} and Interdependency in Compatibility Effects},
  shorttitle = {{{SNARC}} (Spatial\textendash Numerical Association of Response Codes) Meets {{SPARC}} (Spatial\textendash Pitch Association of Response Codes)},
  author = {Weis, Tina and Estner, Barbara and {van Leeuwen}, Cees and Lachmann, Thomas},
  year = {2016},
  month = jul,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {69},
  number = {7},
  pages = {1366--1383},
  issn = {1747-0218, 1747-0226},
  doi = {10.1080/17470218.2015.1082142},
  language = {en},
  file = {Cognitive Science\\Psychophysics\\Weis et al., 2016 - SNARC (spatial–numerical association of response codes) meets SPARC.pdf}
}

@article{weiser1991,
  title = {The {{Computer}} for the 21st {{Century}}},
  author = {Weiser, Mark},
  year = {1991},
  journal = {SCIENTIFIC AMERICAN},
  pages = {12},
  language = {en},
  annotation = {ZSCC: 0000011},
  file = {Human Computer Interaction\\Miscellaneous\\Weiser, 1991 - The Computer for the 21st Century.pdf}
}

@article{wellner1993,
  title = {Interacting with Paper on the {{DigitalDesk}}},
  author = {Wellner, Pierre},
  year = {1993},
  month = jul,
  journal = {Communications of the ACM},
  volume = {36},
  number = {7},
  pages = {87--96},
  issn = {00010782},
  doi = {10.1145/159544.159630},
  language = {en},
  annotation = {ZSCC: 0001640},
  file = {Human Computer Interaction\\Augmented Reality\\Wellner, 1993 - Interacting with paper on the DigitalDesk.pdf}
}

@article{wendt2017,
  ids = {wendtPerceptionSpatialSound2017,wendtPerceptionSpatialSound2017a},
  title = {Perception of {{Spatial Sound Phenomena Created}} by the {{Icosahedral Loudspeaker}}},
  author = {Wendt, Florian and Sharma, Gerriet K. and Frank, Matthias and Zotter, Franz and H{\"o}ldrich, Robert},
  year = {2017},
  month = mar,
  journal = {Computer Music Journal},
  volume = {41},
  number = {1},
  pages = {76--88},
  issn = {0148-9267, 1531-5169},
  doi = {10.1162/COMJ_a_00396},
  abstract = {The icosahedral loudspeaker (IKO) is able to project strongly focused sound beams into arbitrary directions. Incorporating artistic experience and psychoacoustic research, this article presents three listening experiments that provide evidence for a common, intersubjective perception of spatial sonic phenomena created by the IKO. The experiments are designed on the basis of a hierarchical model of spatiosonic phenomena that exhibit increasing complexity, ranging from a single static sonic object to combinations of multiple, partly moving objects. The results are promising and explore new compositional perspectives in spatial computer music.},
  language = {en},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {Arts & Humanities\\Computational Art\\Wendt et al., 2017 - Perception of Spatial Sound Phenomena Created by the Icosahedral Loudspeaker.pdf}
}

@misc{wikipedia2018,
  title = {Autoethnography},
  author = {Wikipedia},
  year = {2018},
  month = apr,
  journal = {Wikipedia},
  urldate = {2018-04-10},
  abstract = {Autoethnography is a form of qualitative research in which an author uses self-reflection and writing to explore their personal experience and connect this autobiographical story to wider cultural, political, and social meanings and understandings. Autoethnography is a self-reflective form of writing used across various disciplines such as communication studies, performance studies, education, English literature, anthropology, social work, sociology, history, psychology, marketing, business and educational administration, arts education and physiotherapy. According to Mar\'echal (2010), "autoethnography is a form or method of research that involves self-observation and reflexive investigation in the context of ethnographic field work and writing" (p. 43). A well-known autoethnographer, Carolyn Ellis (2004) defines it as "research, writing, story, and method that connect the autobiographical and personal to the cultural, social, and political" (p. xix). However, it is not easy to reach a consensus on the term's definition. For instance, in the 1970s, autoethnography was more narrowly defined as "insider ethnography," referring to studies of the (culture of) a group of which the researcher is a member (Hayano, 1979). Nowadays, however, as Ellingson and Ellis (2008) point out, "the meanings and applications of autoethnography have evolved in a manner that makes precise definition difficult" (p. 449). According to Adams, Jones, and Ellis in Autoethnography: Understanding Qualitative Research, "Autoethnography is a research method that: Uses a researcher's personal experience to describe and critique cultural beliefs, practices, and experiences. Acknowledges and values a researcher's relationships with others. . . . Shows 'people in the process of figuring out what to do, how to live, and the meaning of their struggles'" (Adams, 2015). "Social life is messy, uncertain, and emotional. If our desire to research social life, then we must embrace a research method that, to the best of its/our ability, acknowledges and accommodates mess and chaos, uncertainty and emotion" (Adams, 2015).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {en},
  annotation = {Page Version ID: 833892679},
  url = {https://en.wikipedia.org/w/index.php?title=Autoethnography&oldid=833892679}
}

@misc{wikipedia2020,
  title = {Inertial Measurement Unit},
  author = {Wikipedia},
  year = {2020},
  month = may,
  journal = {Wikipedia},
  urldate = {2020-05-25},
  abstract = {An inertial measurement unit (IMU) is an electronic device that measures and reports a body's specific force, angular rate, and sometimes the orientation of the body, using a combination of accelerometers, gyroscopes, and sometimes magnetometers. IMUs are typically used to maneuver aircraft (an attitude and heading reference system), including unmanned aerial vehicles (UAVs), among many others, and spacecraft, including satellites and landers. Recent developments allow for the production of IMU-enabled GPS devices. An IMU allows a GPS receiver to work when GPS-signals are unavailable, such as in tunnels, inside buildings, or when electronic interference is present.  A wireless IMU is known as a WIMU.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {en},
  annotation = {Page Version ID: 958821399},
  file = {..\\..\\Zotero\\storage\\C8BZWMSF\\index.html},
  url = {https://en.wikipedia.org/w/index.php?title=Inertial_measurement_unit&oldid=958821399}
}

@misc{wikipedia2020a,
  title = {Ambisonics},
  author = {Wikipedia},
  year = {2020},
  month = apr,
  journal = {Wikipedia},
  urldate = {2020-05-25},
  abstract = {Ambisonics is a full-sphere surround sound format: in addition to the horizontal plane, it covers sound sources above and below the listener.Unlike other multichannel surround formats, its transmission channels do not carry speaker signals. Instead, they contain a speaker-independent representation of a sound field called B-format, which is then decoded to the listener's speaker setup. This extra step allows the producer to think in terms of source directions rather than loudspeaker positions, and offers the listener a considerable degree of flexibility as to the layout and number of speakers used for playback. Ambisonics was developed in the UK in the 1970s under the auspices of the British National Research Development Corporation. Despite its solid technical foundation and many advantages, Ambisonics had not until recently been a commercial success, and survived only in niche applications and among recording enthusiasts. With the easy availability of powerful digital signal processing (as opposed to the expensive and error-prone analog circuitry that had to be used during its early years) and the successful market introduction of home theatre surround sound systems since the 1990s, interest in Ambisonics among recording engineers, sound designers, composers, media companies, broadcasters and researchers has returned and continues to increase.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {en},
  annotation = {Page Version ID: 953930554},
  file = {..\\..\\Zotero\\storage\\82WQ9UGX\\index.html},
  url = {https://en.wikipedia.org/w/index.php?title=Ambisonics&oldid=953930554}
}

@book{wikstrom2009,
  title = {The Music Industry: Music in the Cloud},
  shorttitle = {The Music Industry},
  author = {Wikstr{\"o}m, Patrik},
  year = {2009},
  series = {Digital Media and Society Series},
  publisher = {{Polity}},
  address = {{Cambridge ; Malden, MA}},
  isbn = {978-0-7456-4389-2 978-0-7456-4390-8},
  lccn = {ML3790 .W52 2009},
  keywords = {History and criticism,Music,Music trade,Musikwirtschaft,Neue Medien,Popular music,Social aspects},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: ocn320316362},
  file = {Arts & Humanities\\Musicology\\Wikström, 2009 - The music industry.pdf}
}

@misc{winer2016,
  title = {{{ESP32 Arduino Library}}},
  author = {Winer, Kris},
  year = {2016},
  urldate = {2020-05-20},
  abstract = {Arduino sketches for the ESP32. Contribute to kriswiner/ESP32 development by creating an account on GitHub.},
  keywords = {esp32},
  url = {https://github.com/kriswiner/ESP32}
}

@article{xue2019,
  title = {Researcher Introspection for Experience-Driven Design Research},
  author = {Xue, Haian and Desmet, Pieter M.A.},
  year = {2019},
  month = jul,
  journal = {Design Studies},
  volume = {63},
  pages = {37--64},
  issn = {0142694X},
  doi = {10.1016/j.destud.2019.03.001},
  abstract = {We challenge the unquestioning pursuit of the appearance of objectivity and ingrained designer-user dualism in human-centred design research and propose a resurrection of introspection as a valid approach to investigating subjective experiences. Through comparing epistemic perspectives and reviewing the histories of introspection in several disciplines, we liberate the research field of experience-driven design from a long-lasting doubt about and the disguised and unsystematic use of this method. To establish a foundation for the further development of introspective methods, we focus on its most controversial type (i.e. researcher introspection) and discuss its strengths and weaknesses, preconditions of use, diverse ways to practise for different suitable experiencedriven design research purposes, and useful techniques and tools. \'O 2019 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-ncnd/4.0/).},
  language = {en},
  annotation = {ZSCC: 0000004},
  file = {Research Methods\\Autobiographical Design\\Xue and Desmet, 2019 - Researcher introspection for experience-driven design research.pdf}
}

@inproceedings{yang2020,
  title = {Fast Synthesis of Perceptually Adequate Room Impulse Responses from Ultrasonic Measurements},
  booktitle = {Proceedings of the 15th International Conference on Audio Mostly},
  author = {Yang, Jing and Pfreundtner, Felix and Barde, Amit and Heutschi, Kurt and S{\"o}r{\"o}s, G{\'a}bor},
  year = {2020},
  series = {{{AM}} '20},
  pages = {53--60},
  publisher = {{Association for Computing Machinery}},
  address = {{Graz, Austria}},
  doi = {10.1145/3411109.3412300},
  abstract = {Audio augmented reality (AAR) applications need to render virtual sounds with acoustic effects that match the real environment of the user to create an experience with strong sense of presence. This audio rendering process can be formulated as the convolution between the dry sound signal and the room impulse response (IR) that covers the audible frequency spectrum (20Hz - 20kHz). While the IR can be pre-calculated in virtual reality (VR) scenes, AR applications need to continuously estimate it. We propose a method to synthesize room IRs based on the corresponding IR in the ultrasound frequency band (20kHz - 22kHz) and two parameters we propose in this paper: slope factor and RT60 ratio. We assess the synthesized IRs using common acoustic metrics and we conducted a user study to evaluate participants' perceptual similarity between the sounds rendered with the synthesized IR and with the recorded IR in different rooms. The method requires only a small number of pre-measurements in the environment to determine the synthesis parameters and it uses only inaudible signals at runtime for fast IR synthesis, making it well suited for interactive AAR applications.},
  isbn = {978-1-4503-7563-4},
  keywords = {auditory perception,augmented reality,room acoustic effects,room impulse response,ultrasound},
  annotation = {ZSCC: 0000000},
  file = {Human Computer Interaction\\Augmented Reality\\Yang et al., 2020 - Fast synthesis of perceptually adequate room impulse responses from ultrasonic.pdf}
}

@article{zavota2016,
  title = {Expanding the {{Extended Mind}}: {{Merleau}}-{{Ponty}}'s {{Late Ontology}} as {{Radical Enactive Cognition}}},
  shorttitle = {Expanding the {{Extended Mind}}},
  author = {Zavota, Gina},
  year = {2016},
  journal = {Essays in Philosophy},
  volume = {17},
  number = {2},
  pages = {94--124},
  issn = {15260569},
  doi = {10.7710/1526-0569.1558},
  abstract = {In this essay, I argue that the late ontology of Maurice MerleauPonty, in particular the system he began to develop in The Visible and the Invisible, can be conceived of as a form of Radical Enactive Cognition, as described by Hutto and Myin in Radicalizing Enactivism. I will begin by discussing Clark and Chalmers' extended mind hypothesis, as well as the enactive view of consciousness proposed by Varela, Thompson, and Rosch in The Embodied Mind. However, neither Clark and Chalmers' extended mind hypothesis nor the enactive view of consciousness advanced by Varela et al. are radical enough to fully capture Merleau-Ponty's late ontology. Inasmuch as Hutto and Myin's formulation combines features of the extended mind thesis and enactivism, and expresses both in a sufficiently radical fashion, it overcomes the deficits of both theories and can serve as a translation, so to speak, of MerleauPonty's ``ontology of the flesh'' into contemporary terms. In particular, their formulation makes explicit several central aspects of his theory: the intimate, mutually constitutive relationship between perceiver and perceived world, the equal weight given to the contributions of perceiver and world within this relationship, and the displacement of representational content from its central position in the understanding of consciousness. It is thus the ideal vehicle for demonstrating some perhaps unexpected ways in which Merleau-Ponty's thought is compatible with contemporary conversations concerning the nature of mind.},
  language = {en},
  file = {Philosophy\\Extended Cognition\\Zavota, 2016 - Expanding the Extended Mind.pdf}
}

@article{zeltzer1992,
  title = {Autonomy, {{Interaction}}, and {{Presence}}},
  author = {Zeltzer, David},
  year = {1992},
  month = jan,
  journal = {Presence: Teleoperators and Virtual Environments},
  volume = {1},
  number = {1},
  pages = {127--132},
  issn = {1054-7460},
  doi = {10.1162/pres.1992.1.1.127},
  language = {en},
  annotation = {ZSCC: 0000679},
  file = {Human Computer Interaction\\Augmented Reality\\Zeltzer, 1992 - Autonomy, Interaction, and Presence.pdf}
}

@inproceedings{zhou2008,
  title = {Trends in Augmented Reality Tracking, Interaction and Display: {{A}} Review of Ten Years of {{ISMAR}}},
  shorttitle = {Trends in Augmented Reality Tracking, Interaction and Display},
  booktitle = {2008 7th {{IEEE}}/{{ACM International Symposium}} on {{Mixed}} and {{Augmented Reality}}},
  author = {Zhou, Feng and Duh, Henry Been-Lirn and Billinghurst, Mark},
  year = {2008},
  month = sep,
  pages = {193--202},
  publisher = {{IEEE}},
  address = {{Cambridge, UK}},
  doi = {10.1109/ISMAR.2008.4637362},
  abstract = {Although Augmented Reality technology was first developed over forty years ago, there has been little survey work giving an overview of recent research in the field. This paper reviews the tenyear development of the work presented at the ISMAR conference and its predecessors with a particular focus on tracking, interaction and display research. It provides a roadmap for future augmented reality research which will be of great value to this relatively young field, and also for helping researchers decide which topics should be explored when they are beginning their own studies in the area.},
  isbn = {978-1-4244-2840-3},
  language = {en},
  annotation = {ZSCC: 0001155},
  file = {Human Computer Interaction\\Augmented Reality\\Zhou et al., 2008 - Trends in augmented reality tracking, interaction and display.pdf}
}


